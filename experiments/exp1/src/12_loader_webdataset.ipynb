{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 12 - WebDataset DataLoader\n",
    "\n",
    "This notebook implements the PyTorch DataLoader for WebDataset (TAR shards) format.\n",
    "\n",
    "**Format:** TAR-based shards with optional compression\n",
    "- Reads from TAR shards (train-*.tar[.zst], val-*.tar[.zst])\n",
    "- Supports multiple variants (different shard sizes and compression)\n",
    "- Efficient streaming from disk or object storage\n",
    "- Sequential I/O friendly\n",
    "\n",
    "**Usage in other notebooks:**\n",
    "```python\n",
    "%run ./12_loader_webdataset.ipynb\n",
    "loader = make_dataloader('cifar10', 'train', batch_size=64, num_workers=4, variant='shard256_none')\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✓ Common utilities loaded successfully\n",
      "\n",
      "Available functions:\n",
      "  - set_seed(seed)\n",
      "  - get_transforms(augment)\n",
      "  - write_sysinfo(path)\n",
      "  - time_first_batch(dataloader, device)\n",
      "  - start_monitor(log_path, interval)\n",
      "  - stop_monitor(thread, stop_event)\n",
      "  - append_to_summary(path, row_dict)\n",
      "  - compute_metrics_from_logs(log_path)\n",
      "  - get_device()\n",
      "  - format_bytes(bytes)\n",
      "  - count_parameters(model)\n",
      "\n",
      "Constants:\n",
      "  - STANDARD_TRANSFORM\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "from pathlib import Path\n",
    "from typing import Optional\n",
    "import io\n",
    "\n",
    "import torch\n",
    "from torch.utils.data import DataLoader\n",
    "import webdataset as wds\n",
    "from PIL import Image\n",
    "\n",
    "# Load common utilities\n",
    "%run ./10_common_utils.ipynb"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## WebDataset Decoder Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def decode_image(sample):\n",
    "    \"\"\"\n",
    "    Decode image from WebDataset sample.\n",
    "    \n",
    "    Args:\n",
    "        sample: WebDataset sample dict with image bytes\n",
    "    \n",
    "    Returns:\n",
    "        Sample dict with decoded PIL Image\n",
    "    \"\"\"\n",
    "    # Find image key (jpg, jpeg, or png)\n",
    "    img_key = None\n",
    "    for key in ['jpg', 'jpeg', 'png']:\n",
    "        if key in sample:\n",
    "            img_key = key\n",
    "            break\n",
    "    \n",
    "    if img_key is None:\n",
    "        raise ValueError(f\"No image found in sample keys: {list(sample.keys())}\")\n",
    "    \n",
    "    # Decode image bytes to PIL Image\n",
    "    img_bytes = sample[img_key]\n",
    "    image = Image.open(io.BytesIO(img_bytes)).convert('RGB')\n",
    "    \n",
    "    # Replace bytes with PIL Image\n",
    "    sample['image'] = image\n",
    "    \n",
    "    return sample\n",
    "\n",
    "\n",
    "def decode_label(sample):\n",
    "    \"\"\"\n",
    "    Decode label from WebDataset sample.\n",
    "    \n",
    "    Args:\n",
    "        sample: WebDataset sample dict with label\n",
    "    \n",
    "    Returns:\n",
    "        Sample dict with decoded integer label\n",
    "    \"\"\"\n",
    "    # Decode label from bytes to int\n",
    "    if 'cls' in sample:\n",
    "        label_bytes = sample['cls']\n",
    "        if isinstance(label_bytes, bytes):\n",
    "            label = int(label_bytes.decode('utf-8'))\n",
    "        else:\n",
    "            label = int(label_bytes)\n",
    "        sample['label'] = label\n",
    "    \n",
    "    return sample\n",
    "\n",
    "\n",
    "def apply_transform(sample, transform):\n",
    "    \"\"\"\n",
    "    Apply transform to image in sample.\n",
    "    \n",
    "    Args:\n",
    "        sample: Sample dict with PIL Image\n",
    "        transform: Torchvision transform\n",
    "    \n",
    "    Returns:\n",
    "        Tuple of (transformed_image, label)\n",
    "    \"\"\"\n",
    "    image = sample['image']\n",
    "    label = sample['label']\n",
    "    \n",
    "    if transform:\n",
    "        image = transform(image)\n",
    "    \n",
    "    return image, label"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## DataLoader Factory Function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# def make_dataloader(\n",
    "#     dataset: str,\n",
    "#     split: str,\n",
    "#     batch_size: int,\n",
    "#     num_workers: int,\n",
    "#     pin_memory: bool = True,\n",
    "#     variant: str = 'shard256_none',\n",
    "#     shuffle: bool = True,\n",
    "#     transform=None,\n",
    "# ) -> DataLoader:\n",
    "#     \"\"\"\n",
    "#     Create a DataLoader for WebDataset format.\n",
    "    \n",
    "#     Args:\n",
    "#         dataset: Dataset name (e.g., 'cifar10', 'imagenet-mini')\n",
    "#         split: Split name ('train' or 'val')\n",
    "#         batch_size: Batch size\n",
    "#         num_workers: Number of data loading workers\n",
    "#         pin_memory: Whether to pin memory for faster GPU transfer\n",
    "#         variant: WebDataset variant (e.g., 'shard256_none', 'shard64_zstd')\n",
    "#         shuffle: Whether to shuffle data (uses buffer for WebDataset)\n",
    "#         transform: Custom transform (uses STANDARD_TRANSFORM if None)\n",
    "    \n",
    "#     Returns:\n",
    "#         PyTorch DataLoader\n",
    "#     \"\"\"\n",
    "#     # Detect environment\n",
    "#     IS_KAGGLE = \"KAGGLE_KERNEL_RUN_TYPE\" in os.environ\n",
    "#     BASE_DIR = Path('/kaggle/working/format-matters') if IS_KAGGLE else Path('..').resolve()\n",
    "    \n",
    "#     # Build path to WebDataset shards\n",
    "#     wds_dir = BASE_DIR / 'data' / 'built' / dataset / 'webdataset' / variant\n",
    "    \n",
    "#     if not wds_dir.exists():\n",
    "#         raise FileNotFoundError(f\"WebDataset directory not found: {wds_dir}\")\n",
    "    \n",
    "#     # Find shard files for the split\n",
    "#     # Convert to a proper POSIX path for WebDataset\n",
    "#     # Find shard files for the split\n",
    "#     shard_pattern = \"file://\" + (wds_dir / f\"{split}-%06d.tar\").as_posix()\n",
    "\n",
    "    \n",
    "#     shard_files = sorted(wds_dir.glob(f\"{split}-*.tar*\"))\n",
    "#     if not shard_files:\n",
    "#         raise FileNotFoundError(f\"No shards found matching: {shard_pattern}\")\n",
    "    \n",
    "#     print(f\"Found {len(shard_files)} shard(s) for {dataset}/{split} ({variant})\")\n",
    "    \n",
    "#     # Use standard transform if none provided\n",
    "#     if transform is None:\n",
    "#         transform = STANDARD_TRANSFORM\n",
    "    \n",
    "#     dataset_obj = wds.WebDataset(shard_pattern)\n",
    "\n",
    "#     if shuffle:\n",
    "#         dataset_obj = dataset_obj.shuffle(min(5000, batch_size * 50))\n",
    "    \n",
    "#     dataset_obj = (\n",
    "#         dataset_obj\n",
    "#         .map(decode_image)\n",
    "#         .map(decode_label)\n",
    "#         .map(lambda sample: apply_transform(sample, transform))\n",
    "#         .batched(batch_size, partial=False)\n",
    "#     )\n",
    "    \n",
    "#     dataloader = wds.WebLoader(\n",
    "#         dataset_obj,\n",
    "#         num_workers=num_workers,\n",
    "#         pin_memory=pin_memory,\n",
    "#     )\n",
    "    \n",
    "    \n",
    "#     # Unbatch for standard PyTorch interface (returns tensors, not lists)\n",
    "#     dataloader = dataloader.unbatched().batched(batch_size, collation_fn=torch.utils.data.default_collate)\n",
    "    \n",
    "#     return dataloader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_dataloader(\n",
    "    dataset: str,\n",
    "    split: str,\n",
    "    batch_size: int,\n",
    "    num_workers: int,\n",
    "    pin_memory: bool = True,\n",
    "    variant: str = \"shard256_none\",\n",
    "    shuffle: bool = True,\n",
    "    transform=None,\n",
    ") -> DataLoader:\n",
    "    \"\"\"\n",
    "    Create a DataLoader for WebDataset format.\n",
    "\n",
    "    Works on both Windows and Linux. On Windows, uses an explicit list of shard\n",
    "    files (wildcards like *.tar are not supported). On Linux, wildcards work normally.\n",
    "\n",
    "    Args:\n",
    "        dataset: Dataset name (e.g., 'cifar10', 'imagenet-mini')\n",
    "        split: Split name ('train' or 'val')\n",
    "        batch_size: Batch size\n",
    "        num_workers: Number of data loading workers\n",
    "        pin_memory: Whether to pin memory for faster GPU transfer\n",
    "        variant: WebDataset variant (e.g., 'shard256_none', 'shard64_zstd')\n",
    "        shuffle: Whether to shuffle data (uses buffer for WebDataset)\n",
    "        transform: Custom transform (uses STANDARD_TRANSFORM if None)\n",
    "\n",
    "    Returns:\n",
    "        PyTorch DataLoader\n",
    "    \"\"\"\n",
    "    import webdataset as wds\n",
    "    import torch\n",
    "    import os\n",
    "    from pathlib import Path\n",
    "\n",
    "    # Detect environment (Kaggle or local)\n",
    "    IS_KAGGLE = \"KAGGLE_KERNEL_RUN_TYPE\" in os.environ\n",
    "    BASE_DIR = (\n",
    "        Path(\"/kaggle/working/format-matters\")\n",
    "        if IS_KAGGLE\n",
    "        else Path(\"..\").resolve()\n",
    "    )\n",
    "\n",
    "    # Build path to WebDataset shards\n",
    "    wds_dir = BASE_DIR / \"data\" / \"built\" / dataset / \"webdataset\" / variant\n",
    "    if not wds_dir.exists():\n",
    "        raise FileNotFoundError(f\"WebDataset directory not found: {wds_dir}\")\n",
    "\n",
    "    # ✅ Gather all shard files explicitly (Windows-safe)\n",
    "    shard_files = sorted(wds_dir.glob(f\"{split}-*.tar*\"))\n",
    "    if not shard_files:\n",
    "        raise FileNotFoundError(f\"No shards found in {wds_dir}\")\n",
    "\n",
    "    print(f\"Found {len(shard_files)} shard(s) for {dataset}/{split} ({variant})\")\n",
    "    print(f\"Example files: {[f.name for f in shard_files[:5]]}\")\n",
    "\n",
    "    # Convert to POSIX paths\n",
    "    shard_paths = [str(p.as_posix()) for p in shard_files]\n",
    "\n",
    "    # ✅ Use explicit file list for Windows; wildcard for Linux/Kaggle\n",
    "    if os.name == \"nt\":\n",
    "        shard_paths = [\"file://\" + str(p.as_posix()) for p in shard_files]\n",
    "        dataset_obj = wds.WebDataset(shard_paths, handler=wds.handlers.warn_and_continue)\n",
    "\n",
    "    else:\n",
    "        # Linux / Kaggle can use pattern\n",
    "        if \"zstd\" in variant or \"zst\" in variant:\n",
    "            shard_pattern = \"file://\" + (wds_dir / f\"{split}-*.tar.zst\").as_posix()\n",
    "        else:\n",
    "            shard_pattern = \"file://\" + (wds_dir / f\"{split}-*.tar\").as_posix()\n",
    "        dataset_obj = wds.WebDataset(shard_pattern, handler=wds.handlers.warn_and_continue)\n",
    "\n",
    "    # Use standard transform if none provided\n",
    "    if transform is None:\n",
    "        transform = STANDARD_TRANSFORM\n",
    "\n",
    "    # ✅ Optional shuffle (buffer-based)\n",
    "    if shuffle:\n",
    "        shuffle_buffer = 50000  # Enough to hold entire CIFAR-10 train set (50k samples)\n",
    "        dataset_obj = dataset_obj.shuffle(shuffle_buffer)\n",
    "\n",
    "    # ✅ Apply decoding and transforms\n",
    "    dataset_obj = (\n",
    "        dataset_obj\n",
    "        .map(decode_image)\n",
    "        .map(decode_label)\n",
    "        .map(lambda sample: apply_transform(sample, transform))\n",
    "    )\n",
    "\n",
    "    # ✅ Create WebLoader and properly batch samples\n",
    "    # Need to unbatch first, then rebatch with default_collate to get correct shape\n",
    "    dataloader = wds.WebLoader(\n",
    "        dataset_obj,\n",
    "        num_workers=num_workers,\n",
    "        pin_memory=pin_memory,\n",
    "    ).unbatched().batched(batch_size, collation_fn=torch.utils.data.default_collate)\n",
    "\n",
    "    return dataloader"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Smoke Test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Running WebDataset DataLoader smoke test...\n",
      "\n",
      "Testing with dataset: cifar10, variant: shard1024_none\n",
      "\n",
      "Found 1 shard(s) for cifar10/train (shard1024_none)\n",
      "Example files: ['train-000000.tar']\n",
      "\n",
      "DataLoader created:\n",
      "  Batch size: 32\n",
      "  Num workers: 0\n",
      "  Variant: shard1024_none\n",
      "[('cifar10', 'shard1024_none'), ('cifar10', 'shard1024_zstd'), ('cifar10', 'shard256_none'), ('cifar10', 'shard256_zstd'), ('cifar10', 'shard64_none'), ('cifar10', 'shard64_zstd'), ('imagenet-mini', 'shard1024_none'), ('imagenet-mini', 'shard1024_zstd'), ('imagenet-mini', 'shard256_none'), ('imagenet-mini', 'shard256_zstd'), ('imagenet-mini', 'shard64_none'), ('imagenet-mini', 'shard64_zstd')]\n",
      "\n",
      "Loading first batch...\n",
      "First batch took 0.08s\n",
      "\n",
      "Batch shapes:\n",
      "  Images: torch.Size([16, 3, 224, 224]) (torch.float32)\n",
      "  Labels: torch.Size([16]) (torch.int64)\n",
      "  Image range: [-2.118, 2.640]\n",
      "  Label range: [0, 0]\n",
      "\n",
      "Testing throughput (10 batches)...\n",
      "10 batches took 0.28s\n",
      "\n",
      "✓ WebDataset DataLoader smoke test passed!\n"
     ]
    }
   ],
   "source": [
    "if __name__ == \"__main__\":\n",
    "    print(\"Running WebDataset DataLoader smoke test...\\n\")\n",
    "    \n",
    "    # Detect environment\n",
    "    IS_KAGGLE = \"KAGGLE_KERNEL_RUN_TYPE\" in os.environ\n",
    "    BASE_DIR = Path('/kaggle/working/format-matters') if IS_KAGGLE else Path('..').resolve()\n",
    "    BUILT_DIR = BASE_DIR / 'data' / 'built'\n",
    "    \n",
    "    # Find available datasets and variants\n",
    "    available_configs = []\n",
    "    for dataset_name in ['cifar10', 'imagenet-mini']:\n",
    "        wds_base = BUILT_DIR / dataset_name / 'webdataset'\n",
    "        if wds_base.exists():\n",
    "            for variant_dir in wds_base.iterdir():\n",
    "                if variant_dir.is_dir():\n",
    "                    train_shards = list(variant_dir.glob('train-*.tar*'))\n",
    "                    if train_shards:\n",
    "                        available_configs.append((dataset_name, variant_dir.name))\n",
    "    \n",
    "    if not available_configs:\n",
    "        print(\"⚠ No WebDataset datasets found. Run 03_build_webdataset.ipynb first.\")\n",
    "    else:\n",
    "        # Test with first available dataset/variant\n",
    "        test_dataset, test_variant = available_configs[0]\n",
    "        print(f\"Testing with dataset: {test_dataset}, variant: {test_variant}\\n\")\n",
    "        \n",
    "        try:\n",
    "            # Create dataloader\n",
    "            loader = make_dataloader(\n",
    "                dataset=test_dataset,\n",
    "                split='train',\n",
    "                batch_size=16,\n",
    "                num_workers=0,\n",
    "                pin_memory=False,\n",
    "                variant=test_variant,\n",
    "                shuffle=True\n",
    "            )\n",
    "            \n",
    "            print(f\"\\nDataLoader created:\")\n",
    "            print(f\"  Batch size: 32\")\n",
    "            print(f\"  Num workers: 0\")\n",
    "            print(f\"  Variant: {test_variant}\")\n",
    "            print(available_configs)\n",
    "            # Load first batch\n",
    "            print(\"\\nLoading first batch...\")\n",
    "            with Timer(\"First batch\"):\n",
    "                images, labels = next(iter(loader))\n",
    "            \n",
    "            print(f\"\\nBatch shapes:\")\n",
    "            print(f\"  Images: {images.shape} ({images.dtype})\")\n",
    "            print(f\"  Labels: {labels.shape} ({labels.dtype})\")\n",
    "            print(f\"  Image range: [{images.min():.3f}, {images.max():.3f}]\")\n",
    "            print(f\"  Label range: [{labels.min()}, {labels.max()}]\")\n",
    "            \n",
    "            # Load a few more batches to test throughput\n",
    "            print(\"\\nTesting throughput (10 batches)...\")\n",
    "            with Timer(\"10 batches\"):\n",
    "                for i, (images, labels) in enumerate(loader):\n",
    "                    if i >= 9:\n",
    "                        break\n",
    "            \n",
    "            print(\"\\n✓ WebDataset DataLoader smoke test passed!\")\n",
    "            \n",
    "        except Exception as e:\n",
    "            print(f\"\\n✗ Smoke test failed: {e}\")\n",
    "            import traceback\n",
    "            traceback.print_exc()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ✅ WebDataset DataLoader Ready\n",
    "\n",
    "**Usage:**\n",
    "```python\n",
    "# In training notebooks\n",
    "%run ./12_loader_webdataset.ipynb\n",
    "\n",
    "train_loader = make_dataloader(\n",
    "    dataset='cifar10',\n",
    "    split='train',\n",
    "    batch_size=64,\n",
    "    num_workers=4,\n",
    "    pin_memory=True,\n",
    "    variant='shard256_none',  # or 'shard64_zstd', etc.\n",
    "    shuffle=True\n",
    ")\n",
    "\n",
    "val_loader = make_dataloader(\n",
    "    dataset='cifar10',\n",
    "    split='val',\n",
    "    batch_size=64,\n",
    "    num_workers=4,\n",
    "    pin_memory=True,\n",
    "    variant='shard256_none',\n",
    "    shuffle=False\n",
    ")\n",
    "```\n",
    "\n",
    "**Features:**\n",
    "- TAR-based shards with optional compression\n",
    "- Efficient streaming from disk or object storage\n",
    "- Sequential I/O friendly\n",
    "- Multiple variants (different shard sizes and compression)\n",
    "- Buffer-based shuffling for training\n",
    "- Standard PyTorch DataLoader interface\n",
    "\n",
    "**Available variants:**\n",
    "- `shard64_none`: 64MB shards, no compression\n",
    "- `shard64_zstd`: 64MB shards, zstd compression\n",
    "- `shard256_none`: 256MB shards, no compression\n",
    "- `shard256_zstd`: 256MB shards, zstd compression\n",
    "- `shard1024_none`: 1024MB shards, no compression\n",
    "- `shard1024_zstd`: 1024MB shards, zstd compression\n",
    "\n",
    "**Next steps:**\n",
    "1. Create other format loaders (13-14)\n",
    "2. Run training experiments (20-21)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
