{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 13 - TFRecord DataLoader\n",
    "\n",
    "This notebook implements the PyTorch DataLoader for TFRecord format.\n",
    "\n",
    "**Format:** TensorFlow's TFRecord (binary protocol buffer)\n",
    "- Reads from TFRecord shards (train-*.tfrecord[.gz], val-*.tfrecord[.gz])\n",
    "- Supports multiple variants (different shard sizes and compression)\n",
    "- Efficient binary serialization\n",
    "- Sequential I/O friendly\n",
    "\n",
    "**Usage in other notebooks:**\n",
    "```python\n",
    "%run ./13_loader_tfrecord.ipynb\n",
    "loader = make_dataloader('cifar10', 'train', batch_size=64, num_workers=4, variant='shard256_none')\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✓ Common utilities loaded successfully\n",
      "\n",
      "Available functions:\n",
      "  - set_seed(seed)\n",
      "  - get_transforms(augment)\n",
      "  - write_sysinfo(path)\n",
      "  - time_first_batch(dataloader, device)\n",
      "  - start_monitor(log_path, interval)\n",
      "  - stop_monitor(thread, stop_event)\n",
      "  - append_to_summary(path, row_dict)\n",
      "  - compute_metrics_from_logs(log_path)\n",
      "  - get_device()\n",
      "  - format_bytes(bytes)\n",
      "  - count_parameters(model)\n",
      "\n",
      "Constants:\n",
      "  - STANDARD_TRANSFORM\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "from pathlib import Path\n",
    "from typing import Optional\n",
    "import io\n",
    "\n",
    "import torch\n",
    "from torch.utils.data import Dataset, DataLoader, IterableDataset\n",
    "import tensorflow as tf\n",
    "from PIL import Image\n",
    "import numpy as np\n",
    "\n",
    "# Load common utilities\n",
    "%run ./10_common_utils.ipynb"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## TFRecord Dataset Class"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "class TFRecordDataset(IterableDataset):\n",
    "    \"\"\"\n",
    "    PyTorch IterableDataset for TFRecord format.\n",
    "    \n",
    "    Args:\n",
    "        tfrecord_paths: List of paths to TFRecord files\n",
    "        transform: Torchvision transforms to apply\n",
    "        compression: Compression type ('none' or 'gzip')\n",
    "        shuffle_buffer: Buffer size for shuffling (0 = no shuffle)\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(\n",
    "        self,\n",
    "        tfrecord_paths: list,\n",
    "        transform=None,\n",
    "        compression: str = 'none',\n",
    "        shuffle_buffer: int = 0\n",
    "    ):\n",
    "        self.tfrecord_paths = [str(p) for p in tfrecord_paths]\n",
    "        self.transform = transform\n",
    "        self.compression = 'GZIP' if compression == 'gzip' else None\n",
    "        self.shuffle_buffer = shuffle_buffer\n",
    "        \n",
    "        print(f\"Loaded TFRecord dataset: {len(self.tfrecord_paths)} shard(s)\")\n",
    "    \n",
    "    def _parse_example(self, serialized_example):\n",
    "        \"\"\"\n",
    "        Parse a serialized TFRecord example.\n",
    "        \n",
    "        Args:\n",
    "            serialized_example: Serialized tf.train.Example\n",
    "        \n",
    "        Returns:\n",
    "            Tuple of (image_tensor, label)\n",
    "        \"\"\"\n",
    "        # Define feature description\n",
    "        feature_description = {\n",
    "            'image': tf.io.FixedLenFeature([], tf.string),\n",
    "            'label': tf.io.FixedLenFeature([], tf.int64),\n",
    "        }\n",
    "        \n",
    "        # Parse the example\n",
    "        example = tf.io.parse_single_example(serialized_example, feature_description)\n",
    "        \n",
    "        # Decode image\n",
    "        image_bytes = example['image'].numpy()\n",
    "        image = Image.open(io.BytesIO(image_bytes)).convert('RGB')\n",
    "        \n",
    "        # Apply transforms\n",
    "        if self.transform:\n",
    "            image = self.transform(image)\n",
    "        \n",
    "        # Get label\n",
    "        label = int(example['label'].numpy())\n",
    "        \n",
    "        return image, label\n",
    "    \n",
    "    def __iter__(self):\n",
    "        \"\"\"\n",
    "        Iterate over the dataset.\n",
    "        \n",
    "        Yields:\n",
    "            Tuple of (image_tensor, label)\n",
    "        \"\"\"\n",
    "        worker_info = torch.utils.data.get_worker_info()\n",
    "        if worker_info is not None:\n",
    "            paths = self.tfrecord_paths[worker_info.id::worker_info.num_workers]\n",
    "            if not paths:\n",
    "                paths = self.tfrecord_paths\n",
    "        else:\n",
    "            paths = self.tfrecord_paths\n",
    "        \n",
    "        num_parallel_reads = tf.data.AUTOTUNE if len(paths) > 1 else None\n",
    "        dataset = tf.data.TFRecordDataset(\n",
    "            paths,\n",
    "            compression_type=self.compression,\n",
    "            num_parallel_reads=num_parallel_reads\n",
    "        )\n",
    "        \n",
    "        # Add shuffling if requested\n",
    "        if self.shuffle_buffer > 0:\n",
    "            dataset = dataset.shuffle(buffer_size=self.shuffle_buffer)\n",
    "        \n",
    "        # Iterate and parse examples\n",
    "        for serialized_example in dataset:\n",
    "            try:\n",
    "                image, label = self._parse_example(serialized_example)\n",
    "                yield image, label\n",
    "            except Exception as e:\n",
    "                # Skip corrupted examples\n",
    "                print(f\"Warning: Failed to parse example: {e}\")\n",
    "                continue\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## DataLoader Factory Function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_dataloader(\n",
    "    dataset: str,\n",
    "    split: str,\n",
    "    batch_size: int,\n",
    "    num_workers: int,\n",
    "    pin_memory: bool = True,\n",
    "    variant: str = 'shard256_none',\n",
    "    shuffle: bool = True,\n",
    "    transform=None,\n",
    ") -> DataLoader:\n",
    "    \"\"\"\n",
    "    Create a DataLoader for TFRecord format.\n",
    "    \n",
    "    Args:\n",
    "        dataset: Dataset name (e.g., 'cifar10', 'imagenet-mini')\n",
    "        split: Split name ('train' or 'val')\n",
    "        batch_size: Batch size\n",
    "        num_workers: Number of data loading workers\n",
    "        pin_memory: Whether to pin memory for faster GPU transfer\n",
    "        variant: TFRecord variant (e.g., 'shard256_none', 'shard64_gzip')\n",
    "        shuffle: Whether to shuffle data (uses buffer for TFRecord)\n",
    "        transform: Custom transform (uses STANDARD_TRANSFORM if None)\n",
    "    \n",
    "    Returns:\n",
    "        PyTorch DataLoader\n",
    "    \"\"\"\n",
    "    # Detect environment\n",
    "    IS_KAGGLE = \"KAGGLE_KERNEL_RUN_TYPE\" in os.environ\n",
    "    BASE_DIR = Path('/kaggle/working/format-matters') if IS_KAGGLE else Path('..').resolve()\n",
    "    \n",
    "    # Build path to TFRecord shards\n",
    "    tfr_dir = BASE_DIR / 'data' / 'built' / dataset / 'tfrecord' / variant\n",
    "    \n",
    "    if not tfr_dir.exists():\n",
    "        raise FileNotFoundError(f\"TFRecord directory not found: {tfr_dir}\")\n",
    "    \n",
    "    # Find shard files for the split\n",
    "    shard_files = sorted(tfr_dir.glob(f\"{split}-*.tfrecord*\"))\n",
    "    \n",
    "    if not shard_files:\n",
    "        raise FileNotFoundError(f\"No TFRecord shards found in: {tfr_dir}\")\n",
    "    \n",
    "    print(f\"Found {len(shard_files)} shard(s) for {dataset}/{split} ({variant})\")\n",
    "    \n",
    "    # Use standard transform if none provided\n",
    "    if transform is None:\n",
    "        transform = STANDARD_TRANSFORM\n",
    "    \n",
    "    # Determine compression from variant name\n",
    "    compression = 'gzip' if 'gzip' in variant else 'none'\n",
    "    \n",
    "    # Create dataset\n",
    "    shuffle_buffer = 50000 if shuffle else 0  # Enough for entire CIFAR-10 train set\n",
    "    \n",
    "    dataset_obj = TFRecordDataset(\n",
    "        tfrecord_paths=shard_files,\n",
    "        transform=transform,\n",
    "        compression=compression,\n",
    "        shuffle_buffer=shuffle_buffer\n",
    "    )\n",
    "    \n",
    "    # Create dataloader\n",
    "    dataloader = DataLoader(\n",
    "        dataset_obj,\n",
    "        batch_size=batch_size,\n",
    "        num_workers=num_workers,\n",
    "        pin_memory=pin_memory,\n",
    "        drop_last=False,\n",
    "    )\n",
    "    \n",
    "    return dataloader"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Smoke Test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Running TFRecord DataLoader smoke test...\n",
      "\n",
      "Testing with dataset: cifar10, variant: shard1024_gzip\n",
      "\n",
      "Found 1 shard(s) for cifar10/train (shard1024_gzip)\n",
      "Loaded TFRecord dataset: 1 shard(s)\n",
      "\n",
      "DataLoader created:\n",
      "  Batch size: 32\n",
      "  Num workers: 0\n",
      "  Variant: shard1024_gzip\n",
      "\n",
      "Loading first batch...\n",
      "First batch took 1.40s\n",
      "\n",
      "Batch shapes:\n",
      "  Images: torch.Size([32, 3, 224, 224]) (torch.float32)\n",
      "  Labels: torch.Size([32]) (torch.int64)\n",
      "  Image range: [-2.118, 2.640]\n",
      "  Label range: [0, 9]\n",
      "\n",
      "Testing throughput (10 batches)...\n",
      "10 batches took 1.03s\n",
      "\n",
      "✓ TFRecord DataLoader smoke test passed!\n"
     ]
    }
   ],
   "source": [
    "if __name__ == \"__main__\":\n",
    "    print(\"Running TFRecord DataLoader smoke test...\\n\")\n",
    "    \n",
    "    # Detect environment\n",
    "    IS_KAGGLE = \"KAGGLE_KERNEL_RUN_TYPE\" in os.environ\n",
    "    BASE_DIR = Path('/kaggle/working/format-matters') if IS_KAGGLE else Path('..').resolve()\n",
    "    BUILT_DIR = BASE_DIR / 'data' / 'built'\n",
    "    \n",
    "    # Find available datasets and variants\n",
    "    available_configs = []\n",
    "    for dataset_name in ['cifar10', 'imagenet-mini', 'tiny-imagenet-200']:\n",
    "        tfr_base = BUILT_DIR / dataset_name / 'tfrecord'\n",
    "        if tfr_base.exists():\n",
    "            for variant_dir in tfr_base.iterdir():\n",
    "                if variant_dir.is_dir():\n",
    "                    train_shards = list(variant_dir.glob('train-*.tfrecord*'))\n",
    "                    if train_shards:\n",
    "                        available_configs.append((dataset_name, variant_dir.name))\n",
    "    \n",
    "    if not available_configs:\n",
    "        print(\"⚠ No TFRecord datasets found. Run 04_build_tfrecord.ipynb first.\")\n",
    "    else:\n",
    "        # Test with first available dataset/variant\n",
    "        test_dataset, test_variant = available_configs[0]\n",
    "        print(f\"Testing with dataset: {test_dataset}, variant: {test_variant}\\n\")\n",
    "        \n",
    "        try:\n",
    "            # Create dataloader\n",
    "            loader = make_dataloader(\n",
    "                dataset=test_dataset,\n",
    "                split='train',\n",
    "                batch_size=32,\n",
    "                num_workers=0,\n",
    "                pin_memory=False,\n",
    "                variant=test_variant,\n",
    "                shuffle=True\n",
    "            )\n",
    "            \n",
    "            print(f\"\\nDataLoader created:\")\n",
    "            print(f\"  Batch size: 32\")\n",
    "            print(f\"  Num workers: 0\")\n",
    "            print(f\"  Variant: {test_variant}\")\n",
    "            \n",
    "            # Load first batch\n",
    "            print(\"\\nLoading first batch...\")\n",
    "            with Timer(\"First batch\"):\n",
    "                images, labels = next(iter(loader))\n",
    "            \n",
    "            print(f\"\\nBatch shapes:\")\n",
    "            print(f\"  Images: {images.shape} ({images.dtype})\")\n",
    "            print(f\"  Labels: {labels.shape} ({labels.dtype})\")\n",
    "            print(f\"  Image range: [{images.min():.3f}, {images.max():.3f}]\")\n",
    "            print(f\"  Label range: [{labels.min()}, {labels.max()}]\")\n",
    "            \n",
    "            # Load a few more batches to test throughput\n",
    "            print(\"\\nTesting throughput (10 batches)...\")\n",
    "            with Timer(\"10 batches\"):\n",
    "                for i, (images, labels) in enumerate(loader):\n",
    "                    if i >= 9:\n",
    "                        break\n",
    "            \n",
    "            print(\"\\n✓ TFRecord DataLoader smoke test passed!\")\n",
    "            \n",
    "        except Exception as e:\n",
    "            print(f\"\\n✗ Smoke test failed: {e}\")\n",
    "            import traceback\n",
    "            traceback.print_exc()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ✅ TFRecord DataLoader Ready\n",
    "\n",
    "**Usage:**\n",
    "```python\n",
    "# In training notebooks\n",
    "%run ./13_loader_tfrecord.ipynb\n",
    "\n",
    "train_loader = make_dataloader(\n",
    "    dataset='cifar10',\n",
    "    split='train',\n",
    "    batch_size=64,\n",
    "    num_workers=4,\n",
    "    pin_memory=True,\n",
    "    variant='shard256_none',  # or 'shard64_gzip', etc.\n",
    "    shuffle=True\n",
    ")\n",
    "\n",
    "val_loader = make_dataloader(\n",
    "    dataset='cifar10',\n",
    "    split='val',\n",
    "    batch_size=64,\n",
    "    num_workers=4,\n",
    "    pin_memory=True,\n",
    "    variant='shard256_none',\n",
    "    shuffle=False\n",
    ")\n",
    "```\n",
    "\n",
    "**Features:**\n",
    "- TFRecord binary format with optional compression\n",
    "- Efficient serialization and deserialization\n",
    "- Sequential I/O friendly\n",
    "- Multiple variants (different shard sizes and compression)\n",
    "- Buffer-based shuffling for training\n",
    "- Standard PyTorch DataLoader interface\n",
    "- Compatible with TensorFlow ecosystem\n",
    "\n",
    "**Available variants:**\n",
    "- `shard64_none`: 64MB shards, no compression\n",
    "- `shard64_gzip`: 64MB shards, gzip compression\n",
    "- `shard256_none`: 256MB shards, no compression\n",
    "- `shard256_gzip`: 256MB shards, gzip compression\n",
    "- `shard1024_none`: 1024MB shards, no compression\n",
    "- `shard1024_gzip`: 1024MB shards, gzip compression\n",
    "\n",
    "**Next steps:**\n",
    "1. Create other format loaders (14)\n",
    "2. Run training experiments (20-21)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
