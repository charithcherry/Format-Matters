{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 14 - LMDB DataLoader\n",
    "\n",
    "This notebook implements the PyTorch DataLoader for LMDB format.\n",
    "\n",
    "**Format:** LMDB (Lightning Memory-Mapped Database)\n",
    "- Reads from LMDB databases (train.lmdb, val.lmdb)\n",
    "- Supports multiple variants (different compression options)\n",
    "- Memory-mapped for fast random access\n",
    "- Excellent for random access patterns\n",
    "\n",
    "**Usage in other notebooks:**\n",
    "```python\n",
    "%run ./14_loader_lmdb.ipynb\n",
    "loader = make_dataloader('cifar10', 'train', batch_size=64, num_workers=4, variant='compress_none')\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✓ Common utilities loaded successfully\n",
      "\n",
      "Available functions:\n",
      "  - set_seed(seed)\n",
      "  - get_transforms(augment)\n",
      "  - write_sysinfo(path)\n",
      "  - time_first_batch(dataloader, device)\n",
      "  - start_monitor(log_path, interval)\n",
      "  - stop_monitor(thread, stop_event)\n",
      "  - append_to_summary(path, row_dict)\n",
      "  - compute_metrics_from_logs(log_path)\n",
      "  - get_device()\n",
      "  - format_bytes(bytes)\n",
      "  - count_parameters(model)\n",
      "\n",
      "Constants:\n",
      "  - STANDARD_TRANSFORM\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "from pathlib import Path\n",
    "from typing import Optional\n",
    "import io\n",
    "import pickle\n",
    "\n",
    "import torch\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "import lmdb\n",
    "from PIL import Image\n",
    "\n",
    "# Optional compression libraries\n",
    "try:\n",
    "    import zstandard as zstd\n",
    "    HAS_ZSTD = True\n",
    "except ImportError:\n",
    "    HAS_ZSTD = False\n",
    "\n",
    "try:\n",
    "    import lz4.frame\n",
    "    HAS_LZ4 = True\n",
    "except ImportError:\n",
    "    HAS_LZ4 = False\n",
    "\n",
    "# Load common utilities\n",
    "%run ./10_common_utils.ipynb"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## LMDB Dataset Class"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "class LMDBDataset(Dataset):\n",
    "    \"\"\"\n",
    "    PyTorch Dataset for LMDB format.\n",
    "    \n",
    "    Args:\n",
    "        lmdb_path: Path to LMDB database directory\n",
    "        transform: Torchvision transforms to apply\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self, lmdb_path: Path, transform=None):\n",
    "        self.lmdb_path = Path(lmdb_path)\n",
    "        self.transform = transform\n",
    "        \n",
    "        # Open LMDB environment (read-only)\n",
    "        self.env = lmdb.open(\n",
    "            str(self.lmdb_path),\n",
    "            readonly=True,\n",
    "            lock=False,\n",
    "            readahead=False,\n",
    "            meminit=False\n",
    "        )\n",
    "        \n",
    "        # Read metadata\n",
    "        with self.env.begin() as txn:\n",
    "            metadata_bytes = txn.get(b'__metadata__')\n",
    "            if metadata_bytes:\n",
    "                self.metadata = pickle.loads(metadata_bytes)\n",
    "                self.length = self.metadata['num_samples']\n",
    "                self.compression = self.metadata.get('compression', 'none')\n",
    "            else:\n",
    "                raise ValueError(f\"No metadata found in LMDB: {lmdb_path}\")\n",
    "        \n",
    "        print(f\"Loaded LMDB dataset: {self.length:,} samples from {lmdb_path.name}\")\n",
    "        print(f\"  Compression: {self.compression}\")\n",
    "    \n",
    "    def __len__(self):\n",
    "        return self.length\n",
    "    \n",
    "    def _decompress(self, data: bytes, compression: str) -> bytes:\n",
    "        \"\"\"\n",
    "        Decompress data using specified compression algorithm.\n",
    "        \n",
    "        Args:\n",
    "            data: Compressed bytes\n",
    "            compression: Compression type ('none', 'zstd', 'lz4')\n",
    "        \n",
    "        Returns:\n",
    "            Decompressed bytes\n",
    "        \"\"\"\n",
    "        if compression == 'none':\n",
    "            return data\n",
    "        elif compression == 'zstd' and HAS_ZSTD:\n",
    "            decompressor = zstd.ZstdDecompressor()\n",
    "            return decompressor.decompress(data)\n",
    "        elif compression == 'lz4' and HAS_LZ4:\n",
    "            return lz4.frame.decompress(data)\n",
    "        else:\n",
    "            raise ValueError(f\"Unsupported compression: {compression}\")\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        # Create key\n",
    "        key = f\"{idx:08d}\".encode('utf-8')\n",
    "        \n",
    "        # Read from LMDB\n",
    "        with self.env.begin() as txn:\n",
    "            entry_bytes = txn.get(key)\n",
    "            \n",
    "            if entry_bytes is None:\n",
    "                raise IndexError(f\"Index {idx} not found in LMDB\")\n",
    "            \n",
    "            # Deserialize entry\n",
    "            entry = pickle.loads(entry_bytes)\n",
    "        \n",
    "        # Decompress image if needed\n",
    "        img_bytes = self._decompress(entry['image'], entry['compression'])\n",
    "        \n",
    "        # Load image\n",
    "        try:\n",
    "            image = Image.open(io.BytesIO(img_bytes)).convert('RGB')\n",
    "        except Exception as e:\n",
    "            raise RuntimeError(f\"Failed to load image at index {idx}: {e}\")\n",
    "        \n",
    "        # Apply transforms\n",
    "        if self.transform:\n",
    "            image = self.transform(image)\n",
    "        \n",
    "        # Get label\n",
    "        label = entry['label']\n",
    "        \n",
    "        return image, label\n",
    "    \n",
    "    def __del__(self):\n",
    "        \"\"\"Close LMDB environment on deletion.\"\"\"\n",
    "        if hasattr(self, 'env'):\n",
    "            self.env.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## DataLoader Factory Function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_dataloader(\n",
    "    dataset: str,\n",
    "    split: str,\n",
    "    batch_size: int,\n",
    "    num_workers: int,\n",
    "    pin_memory: bool = True,\n",
    "    variant: str = 'compress_none',\n",
    "    shuffle: bool = True,\n",
    "    transform=None,\n",
    ") -> DataLoader:\n",
    "    \"\"\"\n",
    "    Create a DataLoader for LMDB format.\n",
    "    \n",
    "    Args:\n",
    "        dataset: Dataset name (e.g., 'cifar10', 'imagenet-mini')\n",
    "        split: Split name ('train' or 'val')\n",
    "        batch_size: Batch size\n",
    "        num_workers: Number of data loading workers\n",
    "        pin_memory: Whether to pin memory for faster GPU transfer\n",
    "        variant: LMDB variant (e.g., 'compress_none', 'compress_zstd')\n",
    "        shuffle: Whether to shuffle data\n",
    "        transform: Custom transform (uses STANDARD_TRANSFORM if None)\n",
    "    \n",
    "    Returns:\n",
    "        PyTorch DataLoader\n",
    "    \"\"\"\n",
    "    # Detect environment\n",
    "    IS_KAGGLE = \"KAGGLE_KERNEL_RUN_TYPE\" in os.environ\n",
    "    BASE_DIR = Path('/kaggle/working/format-matters') if IS_KAGGLE else Path('..').resolve()\n",
    "    \n",
    "    # Build path to LMDB database\n",
    "    lmdb_path = BASE_DIR / 'data' / 'built' / dataset / 'lmdb' / variant / f'{split}.lmdb'\n",
    "    \n",
    "    if not lmdb_path.exists():\n",
    "        raise FileNotFoundError(f\"LMDB database not found: {lmdb_path}\")\n",
    "    \n",
    "    # Use standard transform if none provided\n",
    "    if transform is None:\n",
    "        transform = STANDARD_TRANSFORM\n",
    "    \n",
    "    # Create dataset\n",
    "    dataset_obj = LMDBDataset(lmdb_path, transform=transform)\n",
    "    \n",
    "    # Create dataloader\n",
    "    dataloader = DataLoader(\n",
    "        dataset_obj,\n",
    "        batch_size=batch_size,\n",
    "        shuffle=shuffle,\n",
    "        num_workers=num_workers,\n",
    "        pin_memory=pin_memory,\n",
    "        drop_last=False,\n",
    "    )\n",
    "    \n",
    "    return dataloader"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Smoke Test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Running LMDB DataLoader smoke test...\n",
      "\n",
      "Testing with dataset: cifar10, variant: compress_lz4\n",
      "\n",
      "Loaded LMDB dataset: 50,000 samples from train.lmdb\n",
      "  Compression: lz4\n",
      "\n",
      "DataLoader created:\n",
      "  Dataset size: 50,000\n",
      "  Batch size: 32\n",
      "  Num batches: 1,563\n",
      "  Num workers: 0\n",
      "  Variant: compress_lz4\n",
      "\n",
      "Loading first batch...\n",
      "First batch took 0.14s\n",
      "\n",
      "Batch shapes:\n",
      "  Images: torch.Size([32, 3, 224, 224]) (torch.float32)\n",
      "  Labels: torch.Size([32]) (torch.int64)\n",
      "  Image range: [-2.118, 2.640]\n",
      "  Label range: [0, 9]\n",
      "\n",
      "Testing throughput (10 batches)...\n",
      "10 batches took 0.52s\n",
      "\n",
      "✓ LMDB DataLoader smoke test passed!\n"
     ]
    }
   ],
   "source": [
    "if __name__ == \"__main__\":\n",
    "    print(\"Running LMDB DataLoader smoke test...\\n\")\n",
    "    \n",
    "    # Detect environment\n",
    "    IS_KAGGLE = \"KAGGLE_KERNEL_RUN_TYPE\" in os.environ\n",
    "    BASE_DIR = Path('/kaggle/working/format-matters') if IS_KAGGLE else Path('..').resolve()\n",
    "    BUILT_DIR = BASE_DIR / 'data' / 'built'\n",
    "    \n",
    "    # Find available datasets and variants\n",
    "    available_configs = []\n",
    "    for dataset_name in ['cifar10', 'imagenet-mini', 'tiny-imagenet-200']:\n",
    "        lmdb_base = BUILT_DIR / dataset_name / 'lmdb'\n",
    "        if lmdb_base.exists():\n",
    "            for variant_dir in lmdb_base.iterdir():\n",
    "                if variant_dir.is_dir():\n",
    "                    train_db = variant_dir / 'train.lmdb'\n",
    "                    if train_db.exists():\n",
    "                        available_configs.append((dataset_name, variant_dir.name))\n",
    "    \n",
    "    if not available_configs:\n",
    "        print(\"⚠ No LMDB datasets found. Run 05_build_lmdb.ipynb first.\")\n",
    "    else:\n",
    "        # Test with first available dataset/variant\n",
    "        test_dataset, test_variant = available_configs[0]\n",
    "        print(f\"Testing with dataset: {test_dataset}, variant: {test_variant}\\n\")\n",
    "        \n",
    "        try:\n",
    "            # Create dataloader\n",
    "            loader = make_dataloader(\n",
    "                dataset=test_dataset,\n",
    "                split='train',\n",
    "                batch_size=32,\n",
    "                num_workers=0,\n",
    "                pin_memory=False,\n",
    "                variant=test_variant,\n",
    "                shuffle=True\n",
    "            )\n",
    "            \n",
    "            print(f\"\\nDataLoader created:\")\n",
    "            print(f\"  Dataset size: {len(loader.dataset):,}\")\n",
    "            print(f\"  Batch size: {loader.batch_size}\")\n",
    "            print(f\"  Num batches: {len(loader):,}\")\n",
    "            print(f\"  Num workers: {loader.num_workers}\")\n",
    "            print(f\"  Variant: {test_variant}\")\n",
    "            \n",
    "            # Load first batch\n",
    "            print(\"\\nLoading first batch...\")\n",
    "            with Timer(\"First batch\"):\n",
    "                images, labels = next(iter(loader))\n",
    "            \n",
    "            print(f\"\\nBatch shapes:\")\n",
    "            print(f\"  Images: {images.shape} ({images.dtype})\")\n",
    "            print(f\"  Labels: {labels.shape} ({labels.dtype})\")\n",
    "            print(f\"  Image range: [{images.min():.3f}, {images.max():.3f}]\")\n",
    "            print(f\"  Label range: [{labels.min()}, {labels.max()}]\")\n",
    "            \n",
    "            # Load a few more batches to test throughput\n",
    "            print(\"\\nTesting throughput (10 batches)...\")\n",
    "            with Timer(\"10 batches\"):\n",
    "                for i, (images, labels) in enumerate(loader):\n",
    "                    if i >= 9:\n",
    "                        break\n",
    "            \n",
    "            print(\"\\n✓ LMDB DataLoader smoke test passed!\")\n",
    "            \n",
    "        except Exception as e:\n",
    "            print(f\"\\n✗ Smoke test failed: {e}\")\n",
    "            import traceback\n",
    "            traceback.print_exc()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ✅ LMDB DataLoader Ready\n",
    "\n",
    "**Usage:**\n",
    "```python\n",
    "# In training notebooks\n",
    "%run ./14_loader_lmdb.ipynb\n",
    "\n",
    "train_loader = make_dataloader(\n",
    "    dataset='cifar10',\n",
    "    split='train',\n",
    "    batch_size=64,\n",
    "    num_workers=4,\n",
    "    pin_memory=True,\n",
    "    variant='compress_none',  # or 'compress_zstd', 'compress_lz4'\n",
    "    shuffle=True\n",
    ")\n",
    "\n",
    "val_loader = make_dataloader(\n",
    "    dataset='cifar10',\n",
    "    split='val',\n",
    "    batch_size=64,\n",
    "    num_workers=4,\n",
    "    pin_memory=True,\n",
    "    variant='compress_none',\n",
    "    shuffle=False\n",
    ")\n",
    "```\n",
    "\n",
    "**Features:**\n",
    "- Memory-mapped database for fast random access\n",
    "- Excellent for random access patterns\n",
    "- Multiple variants (different compression options)\n",
    "- Standard PyTorch DataLoader interface\n",
    "- Efficient for training with shuffling\n",
    "\n",
    "**Available variants:**\n",
    "- `compress_none`: No compression\n",
    "- `compress_zstd`: Zstandard compression (if available)\n",
    "- `compress_lz4`: LZ4 compression (if available)\n",
    "\n",
    "**Next steps:**\n",
    "1. Run training experiments (20-21)\n",
    "2. Run analysis notebooks (30-31, 40)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
