{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 04 - Build TFRecord Format\n",
    "\n",
    "This notebook creates TFRecord format for efficient data loading.\n",
    "\n",
    "**Format:** TensorFlow's TFRecord (binary protocol buffer)\n",
    "- Efficient binary serialization\n",
    "- Sequential I/O friendly\n",
    "- Configurable shard sizes\n",
    "- Optional compression (GZIP, ZLIB)\n",
    "- Compatible with TensorFlow and PyTorch\n",
    "\n",
    "**Variants:**\n",
    "- Shard sizes: 64MB, 256MB, 1024MB\n",
    "- Compression: none, gzip\n",
    "\n",
    "**Output:**\n",
    "- `data/built/<dataset>/tfrecord/<variant>/*.tfrecord[.gz]`\n",
    "- Build statistics logged to `runs/<session>/summary.csv`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: tensorflow in c:\\users\\arjya\\fall 2025\\systems for ml\\project 1\\sml\\format-matters\\.venv\\lib\\site-packages (2.20.0)\n",
      "Requirement already satisfied: absl-py>=1.0.0 in c:\\users\\arjya\\fall 2025\\systems for ml\\project 1\\sml\\format-matters\\.venv\\lib\\site-packages (from tensorflow) (2.3.1)\n",
      "Requirement already satisfied: astunparse>=1.6.0 in c:\\users\\arjya\\fall 2025\\systems for ml\\project 1\\sml\\format-matters\\.venv\\lib\\site-packages (from tensorflow) (1.6.3)\n",
      "Requirement already satisfied: flatbuffers>=24.3.25 in c:\\users\\arjya\\fall 2025\\systems for ml\\project 1\\sml\\format-matters\\.venv\\lib\\site-packages (from tensorflow) (25.9.23)\n",
      "Requirement already satisfied: gast!=0.5.0,!=0.5.1,!=0.5.2,>=0.2.1 in c:\\users\\arjya\\fall 2025\\systems for ml\\project 1\\sml\\format-matters\\.venv\\lib\\site-packages (from tensorflow) (0.6.0)\n",
      "Requirement already satisfied: google_pasta>=0.1.1 in c:\\users\\arjya\\fall 2025\\systems for ml\\project 1\\sml\\format-matters\\.venv\\lib\\site-packages (from tensorflow) (0.2.0)\n",
      "Requirement already satisfied: libclang>=13.0.0 in c:\\users\\arjya\\fall 2025\\systems for ml\\project 1\\sml\\format-matters\\.venv\\lib\\site-packages (from tensorflow) (18.1.1)\n",
      "Requirement already satisfied: opt_einsum>=2.3.2 in c:\\users\\arjya\\fall 2025\\systems for ml\\project 1\\sml\\format-matters\\.venv\\lib\\site-packages (from tensorflow) (3.4.0)\n",
      "Requirement already satisfied: packaging in c:\\users\\arjya\\fall 2025\\systems for ml\\project 1\\sml\\format-matters\\.venv\\lib\\site-packages (from tensorflow) (25.0)\n",
      "Requirement already satisfied: protobuf>=5.28.0 in c:\\users\\arjya\\fall 2025\\systems for ml\\project 1\\sml\\format-matters\\.venv\\lib\\site-packages (from tensorflow) (6.32.1)\n",
      "Requirement already satisfied: requests<3,>=2.21.0 in c:\\users\\arjya\\fall 2025\\systems for ml\\project 1\\sml\\format-matters\\.venv\\lib\\site-packages (from tensorflow) (2.32.5)\n",
      "Requirement already satisfied: setuptools in c:\\users\\arjya\\fall 2025\\systems for ml\\project 1\\sml\\format-matters\\.venv\\lib\\site-packages (from tensorflow) (80.9.0)\n",
      "Requirement already satisfied: six>=1.12.0 in c:\\users\\arjya\\fall 2025\\systems for ml\\project 1\\sml\\format-matters\\.venv\\lib\\site-packages (from tensorflow) (1.17.0)\n",
      "Requirement already satisfied: termcolor>=1.1.0 in c:\\users\\arjya\\fall 2025\\systems for ml\\project 1\\sml\\format-matters\\.venv\\lib\\site-packages (from tensorflow) (3.1.0)\n",
      "Requirement already satisfied: typing_extensions>=3.6.6 in c:\\users\\arjya\\fall 2025\\systems for ml\\project 1\\sml\\format-matters\\.venv\\lib\\site-packages (from tensorflow) (4.15.0)\n",
      "Requirement already satisfied: wrapt>=1.11.0 in c:\\users\\arjya\\fall 2025\\systems for ml\\project 1\\sml\\format-matters\\.venv\\lib\\site-packages (from tensorflow) (1.17.3)\n",
      "Requirement already satisfied: grpcio<2.0,>=1.24.3 in c:\\users\\arjya\\fall 2025\\systems for ml\\project 1\\sml\\format-matters\\.venv\\lib\\site-packages (from tensorflow) (1.75.1)\n",
      "Requirement already satisfied: tensorboard~=2.20.0 in c:\\users\\arjya\\fall 2025\\systems for ml\\project 1\\sml\\format-matters\\.venv\\lib\\site-packages (from tensorflow) (2.20.0)\n",
      "Requirement already satisfied: keras>=3.10.0 in c:\\users\\arjya\\fall 2025\\systems for ml\\project 1\\sml\\format-matters\\.venv\\lib\\site-packages (from tensorflow) (3.11.3)\n",
      "Requirement already satisfied: numpy>=1.26.0 in c:\\users\\arjya\\fall 2025\\systems for ml\\project 1\\sml\\format-matters\\.venv\\lib\\site-packages (from tensorflow) (1.26.4)\n",
      "Requirement already satisfied: h5py>=3.11.0 in c:\\users\\arjya\\fall 2025\\systems for ml\\project 1\\sml\\format-matters\\.venv\\lib\\site-packages (from tensorflow) (3.14.0)\n",
      "Requirement already satisfied: ml_dtypes<1.0.0,>=0.5.1 in c:\\users\\arjya\\fall 2025\\systems for ml\\project 1\\sml\\format-matters\\.venv\\lib\\site-packages (from tensorflow) (0.5.3)\n",
      "Requirement already satisfied: wheel<1.0,>=0.23.0 in c:\\users\\arjya\\fall 2025\\systems for ml\\project 1\\sml\\format-matters\\.venv\\lib\\site-packages (from astunparse>=1.6.0->tensorflow) (0.45.1)\n",
      "Requirement already satisfied: rich in c:\\users\\arjya\\fall 2025\\systems for ml\\project 1\\sml\\format-matters\\.venv\\lib\\site-packages (from keras>=3.10.0->tensorflow) (14.2.0)\n",
      "Requirement already satisfied: namex in c:\\users\\arjya\\fall 2025\\systems for ml\\project 1\\sml\\format-matters\\.venv\\lib\\site-packages (from keras>=3.10.0->tensorflow) (0.1.0)\n",
      "Requirement already satisfied: optree in c:\\users\\arjya\\fall 2025\\systems for ml\\project 1\\sml\\format-matters\\.venv\\lib\\site-packages (from keras>=3.10.0->tensorflow) (0.17.0)\n",
      "Requirement already satisfied: charset_normalizer<4,>=2 in c:\\users\\arjya\\fall 2025\\systems for ml\\project 1\\sml\\format-matters\\.venv\\lib\\site-packages (from requests<3,>=2.21.0->tensorflow) (3.4.3)\n",
      "Requirement already satisfied: idna<4,>=2.5 in c:\\users\\arjya\\fall 2025\\systems for ml\\project 1\\sml\\format-matters\\.venv\\lib\\site-packages (from requests<3,>=2.21.0->tensorflow) (3.10)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in c:\\users\\arjya\\fall 2025\\systems for ml\\project 1\\sml\\format-matters\\.venv\\lib\\site-packages (from requests<3,>=2.21.0->tensorflow) (2.5.0)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in c:\\users\\arjya\\fall 2025\\systems for ml\\project 1\\sml\\format-matters\\.venv\\lib\\site-packages (from requests<3,>=2.21.0->tensorflow) (2025.10.5)\n",
      "Requirement already satisfied: markdown>=2.6.8 in c:\\users\\arjya\\fall 2025\\systems for ml\\project 1\\sml\\format-matters\\.venv\\lib\\site-packages (from tensorboard~=2.20.0->tensorflow) (3.9)\n",
      "Requirement already satisfied: pillow in c:\\users\\arjya\\fall 2025\\systems for ml\\project 1\\sml\\format-matters\\.venv\\lib\\site-packages (from tensorboard~=2.20.0->tensorflow) (10.3.0)\n",
      "Requirement already satisfied: tensorboard-data-server<0.8.0,>=0.7.0 in c:\\users\\arjya\\fall 2025\\systems for ml\\project 1\\sml\\format-matters\\.venv\\lib\\site-packages (from tensorboard~=2.20.0->tensorflow) (0.7.2)\n",
      "Requirement already satisfied: werkzeug>=1.0.1 in c:\\users\\arjya\\fall 2025\\systems for ml\\project 1\\sml\\format-matters\\.venv\\lib\\site-packages (from tensorboard~=2.20.0->tensorflow) (3.1.3)\n",
      "Requirement already satisfied: MarkupSafe>=2.1.1 in c:\\users\\arjya\\fall 2025\\systems for ml\\project 1\\sml\\format-matters\\.venv\\lib\\site-packages (from werkzeug>=1.0.1->tensorboard~=2.20.0->tensorflow) (3.0.3)\n",
      "Requirement already satisfied: markdown-it-py>=2.2.0 in c:\\users\\arjya\\fall 2025\\systems for ml\\project 1\\sml\\format-matters\\.venv\\lib\\site-packages (from rich->keras>=3.10.0->tensorflow) (4.0.0)\n",
      "Requirement already satisfied: pygments<3.0.0,>=2.13.0 in c:\\users\\arjya\\fall 2025\\systems for ml\\project 1\\sml\\format-matters\\.venv\\lib\\site-packages (from rich->keras>=3.10.0->tensorflow) (2.19.2)\n",
      "Requirement already satisfied: mdurl~=0.1 in c:\\users\\arjya\\fall 2025\\systems for ml\\project 1\\sml\\format-matters\\.venv\\lib\\site-packages (from markdown-it-py>=2.2.0->rich->keras>=3.10.0->tensorflow) (0.1.2)\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "[notice] A new release of pip is available: 24.2 -> 25.3\n",
      "[notice] To update, run: python.exe -m pip install --upgrade pip\n"
     ]
    }
   ],
   "source": [
    "pip install tensorflow"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✓ Common utilities loaded successfully\n",
      "\n",
      "Available functions:\n",
      "  - set_seed(seed)\n",
      "  - get_transforms(augment)\n",
      "  - write_sysinfo(path)\n",
      "  - time_first_batch(dataloader, device)\n",
      "  - start_monitor(log_path, interval)\n",
      "  - stop_monitor(thread, stop_event)\n",
      "  - append_to_summary(path, row_dict)\n",
      "  - compute_metrics_from_logs(log_path)\n",
      "  - get_device()\n",
      "  - format_bytes(bytes)\n",
      "  - count_parameters(model)\n",
      "\n",
      "Constants:\n",
      "  - STANDARD_TRANSFORM\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import sys\n",
    "import time\n",
    "import json\n",
    "from pathlib import Path\n",
    "from collections import defaultdict\n",
    "\n",
    "import tensorflow as tf\n",
    "from PIL import Image\n",
    "from tqdm.auto import tqdm\n",
    "import numpy as np\n",
    "\n",
    "# Load common utilities\n",
    "%run ./10_common_utils.ipynb"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Configuration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Environment: Local\n",
      "Base directory: C:\\Users\\arjya\\Fall 2025\\Systems for ML\\Project 1\\SML\\format-matters\n",
      "Run directory: C:\\Users\\arjya\\Fall 2025\\Systems for ML\\Project 1\\SML\\format-matters\\runs\\20251127-135410\\builds\n",
      "Summary log: C:\\Users\\arjya\\Fall 2025\\Systems for ML\\Project 1\\SML\\format-matters\\runs\\20251127-135410\\builds\\summary.csv\n"
     ]
    }
   ],
   "source": [
    "# Detect environment\n",
    "IS_KAGGLE = \"KAGGLE_KERNEL_RUN_TYPE\" in os.environ\n",
    "BASE_DIR = Path('/kaggle/working/format-matters') if IS_KAGGLE else Path('..').resolve()\n",
    "\n",
    "RAW_DIR = BASE_DIR / 'data/raw'\n",
    "BUILT_DIR = BASE_DIR / 'data/built'\n",
    "\n",
    "# Create run directory for this session\n",
    "RUN_DIR = BASE_DIR / 'runs' / time.strftime('%Y%m%d-%H%M%S') / 'builds'\n",
    "RUN_DIR.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "SUMMARY_CSV = RUN_DIR / 'summary.csv'\n",
    "SUMMARY_CSV.touch(exist_ok=True)\n",
    "\n",
    "print(f\"Environment: {'Kaggle' if IS_KAGGLE else 'Local'}\")\n",
    "print(f\"Base directory: {BASE_DIR}\")\n",
    "print(f\"Run directory: {RUN_DIR}\")\n",
    "print(f\"Summary log: {SUMMARY_CSV}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Build Configuration\n",
    "\n",
    "Configure which variants to build:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Will build 6 variants:\n",
      "  - shard64_none\n",
      "  - shard64_gzip\n",
      "  - shard256_none\n",
      "  - shard256_gzip\n",
      "  - shard1024_none\n",
      "  - shard1024_gzip\n"
     ]
    }
   ],
   "source": [
    "# Shard sizes in MB\n",
    "SHARD_SIZES = [64, 256, 1024]\n",
    "\n",
    "# Compression options\n",
    "COMPRESSIONS = ['none', 'gzip']\n",
    "\n",
    "# Generate all variants\n",
    "VARIANTS = []\n",
    "for shard_mb in SHARD_SIZES:\n",
    "    for compression in COMPRESSIONS:\n",
    "        VARIANTS.append({\n",
    "            'shard_mb': shard_mb,\n",
    "            'compression': compression,\n",
    "            'name': f\"shard{shard_mb}_{compression}\"\n",
    "        })\n",
    "\n",
    "print(f\"Will build {len(VARIANTS)} variants:\")\n",
    "for v in VARIANTS:\n",
    "    print(f\"  - {v['name']}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## TFRecord Helper Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def _bytes_feature(value):\n",
    "    \"\"\"Returns a bytes_list from a string / byte.\"\"\"\n",
    "    if isinstance(value, type(tf.constant(0))):\n",
    "        value = value.numpy()\n",
    "    return tf.train.Feature(bytes_list=tf.train.BytesList(value=[value]))\n",
    "\n",
    "\n",
    "def _int64_feature(value):\n",
    "    \"\"\"Returns an int64_list from a bool / enum / int / uint.\"\"\"\n",
    "    return tf.train.Feature(int64_list=tf.train.Int64List(value=[value]))\n",
    "\n",
    "\n",
    "def create_example(image_bytes, label):\n",
    "    \"\"\"\n",
    "    Create a TFRecord example from image bytes and label.\n",
    "    \n",
    "    Args:\n",
    "        image_bytes: Raw image bytes (JPEG/PNG encoded)\n",
    "        label: Integer label\n",
    "    \n",
    "    Returns:\n",
    "        tf.train.Example\n",
    "    \"\"\"\n",
    "    feature = {\n",
    "        'image': _bytes_feature(image_bytes),\n",
    "        'label': _int64_feature(label),\n",
    "    }\n",
    "    \n",
    "    return tf.train.Example(features=tf.train.Features(feature=feature))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## TFRecord Builder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_tfrecord_split(\n",
    "    dataset_name: str,\n",
    "    split: str,\n",
    "    raw_path: Path,\n",
    "    output_path: Path,\n",
    "    shard_mb: int,\n",
    "    compression: str,\n",
    "    class_to_label: dict,\n",
    "    image_extensions: list\n",
    "):\n",
    "    \"\"\"\n",
    "    Build TFRecord shards for a single split.\n",
    "    \n",
    "    Args:\n",
    "        dataset_name: Name of dataset\n",
    "        split: Split name ('train' or 'val')\n",
    "        raw_path: Path to raw dataset split directory\n",
    "        output_path: Path to output directory\n",
    "        shard_mb: Target shard size in MB\n",
    "        compression: Compression type ('none' or 'gzip')\n",
    "        class_to_label: Mapping from class name to label index\n",
    "        image_extensions: List of image file extensions to include\n",
    "    \n",
    "    Returns:\n",
    "        Dictionary with build statistics\n",
    "    \"\"\"\n",
    "    output_path.mkdir(parents=True, exist_ok=True)\n",
    "    \n",
    "    # Find all image files\n",
    "    image_files = []\n",
    "    for ext in image_extensions:\n",
    "        image_files.extend(raw_path.rglob(f'*{ext}'))\n",
    "    \n",
    "    # Sort for reproducibility\n",
    "    image_files = sorted(image_files)\n",
    "    split_class_count = len({f.parent.name for f in image_files})\n",
    "    print(f\"    Found {len(image_files):,} images in {split_class_count} classes\")\n",
    "    \n",
    "    # Set compression options\n",
    "    if compression == 'gzip':\n",
    "        if hasattr(tf.io, 'TFRecordCompressionType'):\n",
    "            compression_type = tf.io.TFRecordCompressionType.GZIP\n",
    "        else:\n",
    "            compression_type = 'GZIP'\n",
    "        file_ext = '.tfrecord.gz'\n",
    "    else:\n",
    "        if hasattr(tf.io, 'TFRecordCompressionType'):\n",
    "            compression_type = tf.io.TFRecordCompressionType.NONE\n",
    "        else:\n",
    "            compression_type = None\n",
    "        file_ext = '.tfrecord'\n",
    "    \n",
    "    # Target shard size in bytes\n",
    "    maxsize = shard_mb * 1024 * 1024\n",
    "    \n",
    "    # Write TFRecords\n",
    "    shard_idx = 0\n",
    "    current_size = 0\n",
    "    writer = None\n",
    "    shard_files = []\n",
    "    \n",
    "    options = tf.io.TFRecordOptions(compression_type=compression_type)\n",
    "    \n",
    "    for idx, img_path in enumerate(tqdm(image_files, desc=f\"    Writing {split}\")):\n",
    "        # Read image bytes\n",
    "        with open(img_path, 'rb') as f:\n",
    "            img_bytes = f.read()\n",
    "        \n",
    "        # Get label\n",
    "        class_name = img_path.parent.name\n",
    "        if class_name not in class_to_label:\n",
    "            raise KeyError(f\"Class {class_name} missing from mapping for split {split}\")\n",
    "        label = class_to_label[class_name]\n",
    "        \n",
    "        # Create example\n",
    "        example = create_example(img_bytes, label)\n",
    "        serialized = example.SerializeToString()\n",
    "        \n",
    "        # Check if we need a new shard\n",
    "        if writer is None or current_size >= maxsize:\n",
    "            if writer is not None:\n",
    "                writer.close()\n",
    "            \n",
    "            shard_path = output_path / f\"{split}-{shard_idx:06d}{file_ext}\"\n",
    "            shard_files.append(shard_path)\n",
    "            writer = tf.io.TFRecordWriter(str(shard_path), options=options)\n",
    "            current_size = 0\n",
    "            shard_idx += 1\n",
    "        \n",
    "        # Write example\n",
    "        writer.write(serialized)\n",
    "        current_size += len(serialized)\n",
    "    \n",
    "    # Close final writer\n",
    "    if writer is not None:\n",
    "        writer.close()\n",
    "    \n",
    "    # Calculate statistics\n",
    "    total_bytes = sum(f.stat().st_size for f in shard_files)\n",
    "    \n",
    "    return {\n",
    "        'items': len(image_files),\n",
    "        'bytes_on_disk': total_bytes,\n",
    "        'num_files': len(shard_files),\n",
    "        'avg_file_size': total_bytes // len(shard_files) if shard_files else 0,\n",
    "    }\n",
    "\n",
    "\n",
    "def build_tfrecord(\n",
    "    dataset_name: str,\n",
    "    raw_path: Path,\n",
    "    output_path: Path,\n",
    "    shard_mb: int,\n",
    "    compression: str,\n",
    "    variant_name: str\n",
    "):\n",
    "    \"\"\"\n",
    "    Build TFRecord for a dataset with specific configuration.\n",
    "    \n",
    "    Args:\n",
    "        dataset_name: Name of dataset\n",
    "        raw_path: Path to raw dataset directory\n",
    "        output_path: Path to output directory\n",
    "        shard_mb: Target shard size in MB\n",
    "        compression: Compression type ('none' or 'gzip')\n",
    "        variant_name: Variant identifier\n",
    "    \n",
    "    Returns:\n",
    "        Dictionary with build statistics\n",
    "    \"\"\"\n",
    "    print(f\"\\nBuilding TFRecord for {dataset_name} ({variant_name})...\")\n",
    "    print(f\"  Source: {raw_path}\")\n",
    "    print(f\"  Output: {output_path}\")\n",
    "    print(f\"  Shard size: {shard_mb}MB, Compression: {compression}\")\n",
    "    \n",
    "    start_time = time.time()\n",
    "    # FIXED: Use case-insensitive matching\n",
    "    image_extensions = {'.jpg', '.jpeg', '.png'}\n",
    "    all_class_names = set()\n",
    "    for split in ['train', 'val']:\n",
    "        split_dir = raw_path / split\n",
    "        if not split_dir.exists():\n",
    "            continue\n",
    "        for ext in image_extensions:\n",
    "            for img_path in split_dir.rglob(f'*{ext}'):\n",
    "                all_class_names.add(img_path.parent.name)\n",
    "    class_names = sorted(all_class_names)\n",
    "    class_to_label = {name: idx for idx, name in enumerate(class_names)}\n",
    "    print(f\"  Total classes across splits: {len(class_names)}\")\n",
    "    \n",
    "    # Process each split\n",
    "    for split in ['train', 'val']:\n",
    "        split_dir = raw_path / split\n",
    "        if not split_dir.exists():\n",
    "            print(f\"  ⚠ {split} split not found, skipping\")\n",
    "            continue\n",
    "        \n",
    "        print(f\"\\n  Processing {split} split...\")\n",
    "        \n",
    "        split_stats = build_tfrecord_split(\n",
    "            dataset_name, split, split_dir, output_path,\n",
    "            shard_mb, compression, class_to_label, image_extensions\n",
    "        )\n",
    "        \n",
    "        print(f\"    ✓ {split}: {split_stats['num_files']} shards, \"\n",
    "              f\"{format_bytes(split_stats['bytes_on_disk'])}\")\n",
    "        \n",
    "        # Log to summary\n",
    "        build_time = time.time() - start_time\n",
    "        row = {\n",
    "            'stage': 'build',\n",
    "            'dataset': dataset_name,\n",
    "            'format': 'tfrecord',\n",
    "            'variant': variant_name,\n",
    "            'split': split,\n",
    "            'items': split_stats['items'],\n",
    "            'bytes_on_disk': split_stats['bytes_on_disk'],\n",
    "            'num_files': split_stats['num_files'],\n",
    "            'avg_file_size': split_stats['avg_file_size'],\n",
    "            'build_wall_s': build_time,\n",
    "        }\n",
    "        append_to_summary(SUMMARY_CSV, row)\n",
    "    \n",
    "    build_time = time.time() - start_time\n",
    "    print(f\"\\n  ✓ Build completed in {build_time:.2f}s\")\n",
    "    \n",
    "    return {'dataset': dataset_name, 'variant': variant_name, 'build_time': build_time}\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Build TFRecords for All Datasets and Variants"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 2 dataset(s): cifar10, imagenet-mini\n",
      "\n",
      "============================================================\n"
     ]
    }
   ],
   "source": [
    "# Find available datasets\n",
    "available_datasets = []\n",
    "for dataset_name in ['cifar10', 'imagenet-mini', 'tiny-imagenet-200']:\n",
    "    dataset_path = RAW_DIR / dataset_name\n",
    "    if dataset_path.exists() and (dataset_path / 'train').exists():\n",
    "        available_datasets.append(dataset_name)\n",
    "\n",
    "print(f\"Found {len(available_datasets)} dataset(s): {', '.join(available_datasets)}\")\n",
    "print(\"\\n\" + \"=\"*60)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Building TFRecord for cifar10 (shard64_none)...\n",
      "  Source: C:\\Users\\arjya\\Fall 2025\\Systems for ML\\Project 1\\SML\\format-matters\\data\\raw\\cifar10\n",
      "  Output: C:\\Users\\arjya\\Fall 2025\\Systems for ML\\Project 1\\SML\\format-matters\\data\\built\\cifar10\\tfrecord\\shard64_none\n",
      "  Shard size: 64MB, Compression: none\n",
      "  Total classes across splits: 10\n",
      "\n",
      "  Processing train split...\n",
      "    Found 50,000 images in 10 classes\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "181053513eac4928bb8324fd7fd26ae5",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "    Writing train:   0%|          | 0/50000 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "    ✓ train: 2 shards, 110.4 MB\n",
      "\n",
      "  Processing val split...\n",
      "    Found 10,000 images in 10 classes\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "03b032481ebe4d2599ead5f5a2254b0f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "    Writing val:   0%|          | 0/10000 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "    ✓ val: 1 shards, 22.1 MB\n",
      "\n",
      "  ✓ Build completed in 48.08s\n",
      "============================================================\n",
      "\n",
      "Building TFRecord for cifar10 (shard64_gzip)...\n",
      "  Source: C:\\Users\\arjya\\Fall 2025\\Systems for ML\\Project 1\\SML\\format-matters\\data\\raw\\cifar10\n",
      "  Output: C:\\Users\\arjya\\Fall 2025\\Systems for ML\\Project 1\\SML\\format-matters\\data\\built\\cifar10\\tfrecord\\shard64_gzip\n",
      "  Shard size: 64MB, Compression: gzip\n",
      "  Total classes across splits: 10\n",
      "\n",
      "  Processing train split...\n",
      "    Found 50,000 images in 10 classes\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "2239b70eeb404301b7dbf1a9790f6f50",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "    Writing train:   0%|          | 0/50000 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "    ✓ train: 2 shards, 107.2 MB\n",
      "\n",
      "  Processing val split...\n",
      "    Found 10,000 images in 10 classes\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "91e06d0f4f4d4c9f94d3e99d0d8ccf0b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "    Writing val:   0%|          | 0/10000 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "    ✓ val: 1 shards, 21.4 MB\n",
      "\n",
      "  ✓ Build completed in 19.01s\n",
      "============================================================\n",
      "\n",
      "Building TFRecord for cifar10 (shard256_none)...\n",
      "  Source: C:\\Users\\arjya\\Fall 2025\\Systems for ML\\Project 1\\SML\\format-matters\\data\\raw\\cifar10\n",
      "  Output: C:\\Users\\arjya\\Fall 2025\\Systems for ML\\Project 1\\SML\\format-matters\\data\\built\\cifar10\\tfrecord\\shard256_none\n",
      "  Shard size: 256MB, Compression: none\n",
      "  Total classes across splits: 10\n",
      "\n",
      "  Processing train split...\n",
      "    Found 50,000 images in 10 classes\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "2f66256bed5446e69015b2933c218c95",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "    Writing train:   0%|          | 0/50000 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "    ✓ train: 1 shards, 110.4 MB\n",
      "\n",
      "  Processing val split...\n",
      "    Found 10,000 images in 10 classes\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "4141510574614ce1a75c4298e6e81bf6",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "    Writing val:   0%|          | 0/10000 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "    ✓ val: 1 shards, 22.1 MB\n",
      "\n",
      "  ✓ Build completed in 10.06s\n",
      "============================================================\n",
      "\n",
      "Building TFRecord for cifar10 (shard256_gzip)...\n",
      "  Source: C:\\Users\\arjya\\Fall 2025\\Systems for ML\\Project 1\\SML\\format-matters\\data\\raw\\cifar10\n",
      "  Output: C:\\Users\\arjya\\Fall 2025\\Systems for ML\\Project 1\\SML\\format-matters\\data\\built\\cifar10\\tfrecord\\shard256_gzip\n",
      "  Shard size: 256MB, Compression: gzip\n",
      "  Total classes across splits: 10\n",
      "\n",
      "  Processing train split...\n",
      "    Found 50,000 images in 10 classes\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c54206f4ae6c41ae9817456e7bc777f3",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "    Writing train:   0%|          | 0/50000 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "    ✓ train: 1 shards, 107.2 MB\n",
      "\n",
      "  Processing val split...\n",
      "    Found 10,000 images in 10 classes\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a880b7cad90a431d9572d32c5c70af88",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "    Writing val:   0%|          | 0/10000 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "    ✓ val: 1 shards, 21.4 MB\n",
      "\n",
      "  ✓ Build completed in 12.08s\n",
      "============================================================\n",
      "\n",
      "Building TFRecord for cifar10 (shard1024_none)...\n",
      "  Source: C:\\Users\\arjya\\Fall 2025\\Systems for ML\\Project 1\\SML\\format-matters\\data\\raw\\cifar10\n",
      "  Output: C:\\Users\\arjya\\Fall 2025\\Systems for ML\\Project 1\\SML\\format-matters\\data\\built\\cifar10\\tfrecord\\shard1024_none\n",
      "  Shard size: 1024MB, Compression: none\n",
      "  Total classes across splits: 10\n",
      "\n",
      "  Processing train split...\n",
      "    Found 50,000 images in 10 classes\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "6f4a15322dd5429b87e14509e068f6a8",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "    Writing train:   0%|          | 0/50000 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "    ✓ train: 1 shards, 110.4 MB\n",
      "\n",
      "  Processing val split...\n",
      "    Found 10,000 images in 10 classes\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a5d966fb048c4aadbb5af350cd2d0ad8",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "    Writing val:   0%|          | 0/10000 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "    ✓ val: 1 shards, 22.1 MB\n",
      "\n",
      "  ✓ Build completed in 9.21s\n",
      "============================================================\n",
      "\n",
      "Building TFRecord for cifar10 (shard1024_gzip)...\n",
      "  Source: C:\\Users\\arjya\\Fall 2025\\Systems for ML\\Project 1\\SML\\format-matters\\data\\raw\\cifar10\n",
      "  Output: C:\\Users\\arjya\\Fall 2025\\Systems for ML\\Project 1\\SML\\format-matters\\data\\built\\cifar10\\tfrecord\\shard1024_gzip\n",
      "  Shard size: 1024MB, Compression: gzip\n",
      "  Total classes across splits: 10\n",
      "\n",
      "  Processing train split...\n",
      "    Found 50,000 images in 10 classes\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "26636373a7ba47b89c9ff625e981cfce",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "    Writing train:   0%|          | 0/50000 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "    ✓ train: 1 shards, 107.2 MB\n",
      "\n",
      "  Processing val split...\n",
      "    Found 10,000 images in 10 classes\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ad1f912510ac417190076e3cbec377ab",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "    Writing val:   0%|          | 0/10000 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "    ✓ val: 1 shards, 21.4 MB\n",
      "\n",
      "  ✓ Build completed in 19.75s\n",
      "============================================================\n",
      "\n",
      "Building TFRecord for imagenet-mini (shard64_none)...\n",
      "  Source: C:\\Users\\arjya\\Fall 2025\\Systems for ML\\Project 1\\SML\\format-matters\\data\\raw\\imagenet-mini\n",
      "  Output: C:\\Users\\arjya\\Fall 2025\\Systems for ML\\Project 1\\SML\\format-matters\\data\\built\\imagenet-mini\\tfrecord\\shard64_none\n",
      "  Shard size: 64MB, Compression: none\n",
      "  Total classes across splits: 1000\n",
      "\n",
      "  Processing train split...\n",
      "    Found 34,745 images in 1000 classes\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "cf67de0c7a4c49b69c8ad0788bfd4f06",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "    Writing train:   0%|          | 0/34745 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "    ✓ train: 56 shards, 3.5 GB\n",
      "\n",
      "  Processing val split...\n",
      "    Found 3,923 images in 1000 classes\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "498a4249cf1640fea865a3c89c6e8518",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "    Writing val:   0%|          | 0/3923 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "    ✓ val: 8 shards, 480.3 MB\n",
      "\n",
      "  ✓ Build completed in 73.26s\n",
      "============================================================\n",
      "\n",
      "Building TFRecord for imagenet-mini (shard64_gzip)...\n",
      "  Source: C:\\Users\\arjya\\Fall 2025\\Systems for ML\\Project 1\\SML\\format-matters\\data\\raw\\imagenet-mini\n",
      "  Output: C:\\Users\\arjya\\Fall 2025\\Systems for ML\\Project 1\\SML\\format-matters\\data\\built\\imagenet-mini\\tfrecord\\shard64_gzip\n",
      "  Shard size: 64MB, Compression: gzip\n",
      "  Total classes across splits: 1000\n",
      "\n",
      "  Processing train split...\n",
      "    Found 34,745 images in 1000 classes\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a24932f47e9249b5a4ceb51a6f610f7f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "    Writing train:   0%|          | 0/34745 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "    ✓ train: 56 shards, 3.4 GB\n",
      "\n",
      "  Processing val split...\n",
      "    Found 3,923 images in 1000 classes\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "6e2253bb66744610949d9aa3ec990026",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "    Writing val:   0%|          | 0/3923 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "    ✓ val: 8 shards, 476.1 MB\n",
      "\n",
      "  ✓ Build completed in 230.10s\n",
      "============================================================\n",
      "\n",
      "Building TFRecord for imagenet-mini (shard256_none)...\n",
      "  Source: C:\\Users\\arjya\\Fall 2025\\Systems for ML\\Project 1\\SML\\format-matters\\data\\raw\\imagenet-mini\n",
      "  Output: C:\\Users\\arjya\\Fall 2025\\Systems for ML\\Project 1\\SML\\format-matters\\data\\built\\imagenet-mini\\tfrecord\\shard256_none\n",
      "  Shard size: 256MB, Compression: none\n",
      "  Total classes across splits: 1000\n",
      "\n",
      "  Processing train split...\n",
      "    Found 34,745 images in 1000 classes\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "bd0b1d4a7af44dd8b0aad4f3214efa04",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "    Writing train:   0%|          | 0/34745 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "    ✓ train: 14 shards, 3.5 GB\n",
      "\n",
      "  Processing val split...\n",
      "    Found 3,923 images in 1000 classes\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "9d9087909b7a4aa8870ecd7542b29e92",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "    Writing val:   0%|          | 0/3923 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "    ✓ val: 2 shards, 480.3 MB\n",
      "\n",
      "  ✓ Build completed in 70.25s\n",
      "============================================================\n",
      "\n",
      "Building TFRecord for imagenet-mini (shard256_gzip)...\n",
      "  Source: C:\\Users\\arjya\\Fall 2025\\Systems for ML\\Project 1\\SML\\format-matters\\data\\raw\\imagenet-mini\n",
      "  Output: C:\\Users\\arjya\\Fall 2025\\Systems for ML\\Project 1\\SML\\format-matters\\data\\built\\imagenet-mini\\tfrecord\\shard256_gzip\n",
      "  Shard size: 256MB, Compression: gzip\n",
      "  Total classes across splits: 1000\n",
      "\n",
      "  Processing train split...\n",
      "    Found 34,745 images in 1000 classes\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "cac087007fd746c18e44e02e3cc2ad77",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "    Writing train:   0%|          | 0/34745 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "    ✓ train: 14 shards, 3.4 GB\n",
      "\n",
      "  Processing val split...\n",
      "    Found 3,923 images in 1000 classes\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a09990cab7e04b4a99bffc57e6f79c5b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "    Writing val:   0%|          | 0/3923 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "    ✓ val: 2 shards, 476.1 MB\n",
      "\n",
      "  ✓ Build completed in 248.99s\n",
      "============================================================\n",
      "\n",
      "Building TFRecord for imagenet-mini (shard1024_none)...\n",
      "  Source: C:\\Users\\arjya\\Fall 2025\\Systems for ML\\Project 1\\SML\\format-matters\\data\\raw\\imagenet-mini\n",
      "  Output: C:\\Users\\arjya\\Fall 2025\\Systems for ML\\Project 1\\SML\\format-matters\\data\\built\\imagenet-mini\\tfrecord\\shard1024_none\n",
      "  Shard size: 1024MB, Compression: none\n",
      "  Total classes across splits: 1000\n",
      "\n",
      "  Processing train split...\n",
      "    Found 34,745 images in 1000 classes\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "fe825a404f97413989542fadca0fa51a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "    Writing train:   0%|          | 0/34745 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "    ✓ train: 4 shards, 3.5 GB\n",
      "\n",
      "  Processing val split...\n",
      "    Found 3,923 images in 1000 classes\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f0ec87790c3545eb9d93868a0698ef69",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "    Writing val:   0%|          | 0/3923 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "    ✓ val: 1 shards, 480.3 MB\n",
      "\n",
      "  ✓ Build completed in 84.91s\n",
      "============================================================\n",
      "\n",
      "Building TFRecord for imagenet-mini (shard1024_gzip)...\n",
      "  Source: C:\\Users\\arjya\\Fall 2025\\Systems for ML\\Project 1\\SML\\format-matters\\data\\raw\\imagenet-mini\n",
      "  Output: C:\\Users\\arjya\\Fall 2025\\Systems for ML\\Project 1\\SML\\format-matters\\data\\built\\imagenet-mini\\tfrecord\\shard1024_gzip\n",
      "  Shard size: 1024MB, Compression: gzip\n",
      "  Total classes across splits: 1000\n",
      "\n",
      "  Processing train split...\n",
      "    Found 34,745 images in 1000 classes\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "4eb62bf32443429b8dad6c3a21799f3b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "    Writing train:   0%|          | 0/34745 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "    ✓ train: 4 shards, 3.4 GB\n",
      "\n",
      "  Processing val split...\n",
      "    Found 3,923 images in 1000 classes\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "55776747c9e4476e889c07e511f2af9d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "    Writing val:   0%|          | 0/3923 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "    ✓ val: 1 shards, 476.1 MB\n",
      "\n",
      "  ✓ Build completed in 226.68s\n",
      "============================================================\n",
      "\n",
      "✓ Built 12 TFRecord variant(s)\n"
     ]
    }
   ],
   "source": [
    "# Build all variants for all datasets\n",
    "build_results = []\n",
    "\n",
    "for dataset_name in available_datasets:\n",
    "    raw_path = RAW_DIR / dataset_name\n",
    "    \n",
    "    for variant in VARIANTS:\n",
    "        output_path = BUILT_DIR / dataset_name / 'tfrecord' / variant['name']\n",
    "        \n",
    "        result = build_tfrecord(\n",
    "            dataset_name=dataset_name,\n",
    "            raw_path=raw_path,\n",
    "            output_path=output_path,\n",
    "            shard_mb=variant['shard_mb'],\n",
    "            compression=variant['compression'],\n",
    "            variant_name=variant['name']\n",
    "        )\n",
    "        build_results.append(result)\n",
    "        \n",
    "        print(\"=\"*60)\n",
    "\n",
    "print(f\"\\n✓ Built {len(build_results)} TFRecord variant(s)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Verification"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Verifying TFRecord shards...\n",
      "\n",
      "cifar10:\n",
      "  ✓ shard64_none:\n",
      "      Train: 2 shards, 110.4 MB\n",
      "      Val: 1 shards, 22.1 MB\n",
      "  ✓ shard64_gzip:\n",
      "      Train: 2 shards, 107.2 MB\n",
      "      Val: 1 shards, 21.4 MB\n",
      "  ✓ shard256_none:\n",
      "      Train: 1 shards, 110.4 MB\n",
      "      Val: 1 shards, 22.1 MB\n",
      "  ✓ shard256_gzip:\n",
      "      Train: 1 shards, 107.2 MB\n",
      "      Val: 1 shards, 21.4 MB\n",
      "  ✓ shard1024_none:\n",
      "      Train: 1 shards, 110.4 MB\n",
      "      Val: 1 shards, 22.1 MB\n",
      "  ✓ shard1024_gzip:\n",
      "      Train: 1 shards, 107.2 MB\n",
      "      Val: 1 shards, 21.4 MB\n",
      "\n",
      "imagenet-mini:\n",
      "  ✓ shard64_none:\n",
      "      Train: 56 shards, 3.5 GB\n",
      "      Val: 8 shards, 480.3 MB\n",
      "  ✓ shard64_gzip:\n",
      "      Train: 56 shards, 3.4 GB\n",
      "      Val: 8 shards, 476.1 MB\n",
      "  ✓ shard256_none:\n",
      "      Train: 14 shards, 3.5 GB\n",
      "      Val: 2 shards, 480.3 MB\n",
      "  ✓ shard256_gzip:\n",
      "      Train: 14 shards, 3.4 GB\n",
      "      Val: 2 shards, 476.1 MB\n",
      "  ✓ shard1024_none:\n",
      "      Train: 4 shards, 3.5 GB\n",
      "      Val: 1 shards, 480.3 MB\n",
      "  ✓ shard1024_gzip:\n",
      "      Train: 4 shards, 3.4 GB\n",
      "      Val: 1 shards, 476.1 MB\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(\"\\nVerifying TFRecord shards...\\n\")\n",
    "\n",
    "for dataset_name in available_datasets:\n",
    "    print(f\"{dataset_name}:\")\n",
    "    \n",
    "    for variant in VARIANTS:\n",
    "        tfr_dir = BUILT_DIR / dataset_name / 'tfrecord' / variant['name']\n",
    "        \n",
    "        if not tfr_dir.exists():\n",
    "            print(f\"  ✗ {variant['name']}: directory not found\")\n",
    "            continue\n",
    "        \n",
    "        # Count shards\n",
    "        train_shards = list(tfr_dir.glob('train-*.tfrecord*'))\n",
    "        val_shards = list(tfr_dir.glob('val-*.tfrecord*'))\n",
    "        \n",
    "        if not train_shards:\n",
    "            print(f\"  ✗ {variant['name']}: no train shards found\")\n",
    "            continue\n",
    "        \n",
    "        # Calculate sizes\n",
    "        train_size = sum(f.stat().st_size for f in train_shards)\n",
    "        val_size = sum(f.stat().st_size for f in val_shards)\n",
    "        \n",
    "        print(f\"  ✓ {variant['name']}:\")\n",
    "        print(f\"      Train: {len(train_shards)} shards, {format_bytes(train_size)}\")\n",
    "        print(f\"      Val: {len(val_shards)} shards, {format_bytes(val_size)}\")\n",
    "    \n",
    "    print()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Sample Data Inspection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Inspecting first shard of cifar10 (shard64_none):\n",
      "\n",
      "Shard: train-000000.tfrecord\n",
      "Size: 64.5 MB\n",
      "\n",
      "First 5 samples:\n",
      "\n",
      "Sample 0:\n",
      "  Image bytes: 2063\n",
      "  Label: 0\n",
      "\n",
      "Sample 1:\n",
      "  Image bytes: 2169\n",
      "  Label: 0\n",
      "\n",
      "Sample 2:\n",
      "  Image bytes: 2237\n",
      "  Label: 0\n",
      "\n",
      "Sample 3:\n",
      "  Image bytes: 2104\n",
      "  Label: 0\n",
      "\n",
      "Sample 4:\n",
      "  Image bytes: 2181\n",
      "  Label: 0\n"
     ]
    }
   ],
   "source": [
    "# Inspect first shard of first dataset/variant\n",
    "if available_datasets and VARIANTS:\n",
    "    dataset_name = available_datasets[0]\n",
    "    variant = VARIANTS[0]\n",
    "    tfr_dir = BUILT_DIR / dataset_name / 'tfrecord' / variant['name']\n",
    "    \n",
    "    train_shards = sorted(tfr_dir.glob('train-*.tfrecord*'))\n",
    "    if train_shards:\n",
    "        print(f\"Inspecting first shard of {dataset_name} ({variant['name']}):\\n\")\n",
    "        print(f\"Shard: {train_shards[0].name}\")\n",
    "        print(f\"Size: {format_bytes(train_shards[0].stat().st_size)}\")\n",
    "        \n",
    "        # Read first few samples\n",
    "        print(\"\\nFirst 5 samples:\")\n",
    "        \n",
    "        # Determine compression\n",
    "        compression = 'GZIP' if train_shards[0].suffix == '.gz' else ''\n",
    "        \n",
    "        dataset = tf.data.TFRecordDataset(\n",
    "            str(train_shards[0]),\n",
    "            compression_type=compression\n",
    "        )\n",
    "        \n",
    "        for i, raw_record in enumerate(dataset.take(5)):\n",
    "            example = tf.train.Example()\n",
    "            example.ParseFromString(raw_record.numpy())\n",
    "            \n",
    "            # Extract features\n",
    "            image_bytes = example.features.feature['image'].bytes_list.value[0]\n",
    "            label = example.features.feature['label'].int64_list.value[0]\n",
    "            \n",
    "            print(f\"\\nSample {i}:\")\n",
    "            print(f\"  Image bytes: {len(image_bytes)}\")\n",
    "            print(f\"  Label: {label}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Build Summary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Read and display summary\n",
    "# if SUMMARY_CSV.exists() and SUMMARY_CSV.stat().st_size > 0:\n",
    "#     import pandas as pd\n",
    "#     summary_df = pd.read_csv(SUMMARY_CSV)\n",
    "    \n",
    "#     print(\"\\nBuild Summary:\")\n",
    "#     print(\"=\"*80)\n",
    "    \n",
    "#     for dataset in summary_df['dataset'].unique():\n",
    "#         print(f\"\\n{dataset}:\")\n",
    "#         dataset_df = summary_df[summary_df['dataset'] == dataset]\n",
    "        \n",
    "#         for variant in dataset_df['variant'].unique():\n",
    "#             variant_df = dataset_df[dataset_df['variant'] == variant]\n",
    "#             print(f\"\\n  {variant}:\")\n",
    "            \n",
    "#             for _, row in variant_df.iterrows():\n",
    "#                 print(f\"    {row['split']}:\")\n",
    "#                 print(f\"      Items: {row['items']:,}\")\n",
    "#                 print(f\"      Shards: {row['num_files']}\")\n",
    "#                 print(f\"      Size: {format_bytes(row['bytes_on_disk'])}\")\n",
    "#                 print(f\"      Avg shard: {format_bytes(row['avg_file_size'])}\")\n",
    "    \n",
    "#     print(\"\\n\" + \"=\"*80)\n",
    "#     print(f\"\\nSummary saved to: {SUMMARY_CSV}\")\n",
    "# else:\n",
    "#     print(\"No summary data available\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Build Summary:\n",
      "================================================================================\n",
      "\n",
      "cifar10:\n",
      "\n",
      "  shard64_none:\n",
      "    train:\n",
      "      Items: 50,000\n",
      "      Shards: 2\n",
      "      Size: 110.4 MB\n",
      "      Avg shard: 55.2 MB\n",
      "    val:\n",
      "      Items: 10,000\n",
      "      Shards: 1\n",
      "      Size: 22.1 MB\n",
      "      Avg shard: 22.1 MB\n",
      "\n",
      "  shard64_gzip:\n",
      "    train:\n",
      "      Items: 50,000\n",
      "      Shards: 2\n",
      "      Size: 107.2 MB\n",
      "      Avg shard: 53.6 MB\n",
      "    val:\n",
      "      Items: 10,000\n",
      "      Shards: 1\n",
      "      Size: 21.4 MB\n",
      "      Avg shard: 21.4 MB\n",
      "\n",
      "  shard256_none:\n",
      "    train:\n",
      "      Items: 50,000\n",
      "      Shards: 1\n",
      "      Size: 110.4 MB\n",
      "      Avg shard: 110.4 MB\n",
      "    val:\n",
      "      Items: 10,000\n",
      "      Shards: 1\n",
      "      Size: 22.1 MB\n",
      "      Avg shard: 22.1 MB\n",
      "\n",
      "  shard256_gzip:\n",
      "    train:\n",
      "      Items: 50,000\n",
      "      Shards: 1\n",
      "      Size: 107.2 MB\n",
      "      Avg shard: 107.2 MB\n",
      "    val:\n",
      "      Items: 10,000\n",
      "      Shards: 1\n",
      "      Size: 21.4 MB\n",
      "      Avg shard: 21.4 MB\n",
      "\n",
      "  shard1024_none:\n",
      "    train:\n",
      "      Items: 50,000\n",
      "      Shards: 1\n",
      "      Size: 110.4 MB\n",
      "      Avg shard: 110.4 MB\n",
      "    val:\n",
      "      Items: 10,000\n",
      "      Shards: 1\n",
      "      Size: 22.1 MB\n",
      "      Avg shard: 22.1 MB\n",
      "\n",
      "  shard1024_gzip:\n",
      "    train:\n",
      "      Items: 50,000\n",
      "      Shards: 1\n",
      "      Size: 107.2 MB\n",
      "      Avg shard: 107.2 MB\n",
      "    val:\n",
      "      Items: 10,000\n",
      "      Shards: 1\n",
      "      Size: 21.4 MB\n",
      "      Avg shard: 21.4 MB\n",
      "\n",
      "imagenet-mini:\n",
      "\n",
      "  shard64_none:\n",
      "    train:\n",
      "      Items: 34,745\n",
      "      Shards: 56\n",
      "      Size: 3.5 GB\n",
      "      Avg shard: 63.7 MB\n",
      "    val:\n",
      "      Items: 3,923\n",
      "      Shards: 8\n",
      "      Size: 480.3 MB\n",
      "      Avg shard: 60.0 MB\n",
      "\n",
      "  shard64_gzip:\n",
      "    train:\n",
      "      Items: 34,745\n",
      "      Shards: 56\n",
      "      Size: 3.4 GB\n",
      "      Avg shard: 63.1 MB\n",
      "    val:\n",
      "      Items: 3,923\n",
      "      Shards: 8\n",
      "      Size: 476.1 MB\n",
      "      Avg shard: 59.5 MB\n",
      "\n",
      "  shard256_none:\n",
      "    train:\n",
      "      Items: 34,745\n",
      "      Shards: 14\n",
      "      Size: 3.5 GB\n",
      "      Avg shard: 254.8 MB\n",
      "    val:\n",
      "      Items: 3,923\n",
      "      Shards: 2\n",
      "      Size: 480.3 MB\n",
      "      Avg shard: 240.1 MB\n",
      "\n",
      "  shard256_gzip:\n",
      "    train:\n",
      "      Items: 34,745\n",
      "      Shards: 14\n",
      "      Size: 3.4 GB\n",
      "      Avg shard: 252.3 MB\n",
      "    val:\n",
      "      Items: 3,923\n",
      "      Shards: 2\n",
      "      Size: 476.1 MB\n",
      "      Avg shard: 238.0 MB\n",
      "\n",
      "  shard1024_none:\n",
      "    train:\n",
      "      Items: 34,745\n",
      "      Shards: 4\n",
      "      Size: 3.5 GB\n",
      "      Avg shard: 891.7 MB\n",
      "    val:\n",
      "      Items: 3,923\n",
      "      Shards: 1\n",
      "      Size: 480.3 MB\n",
      "      Avg shard: 480.3 MB\n",
      "\n",
      "  shard1024_gzip:\n",
      "    train:\n",
      "      Items: 34,745\n",
      "      Shards: 4\n",
      "      Size: 3.4 GB\n",
      "      Avg shard: 883.2 MB\n",
      "    val:\n",
      "      Items: 3,923\n",
      "      Shards: 1\n",
      "      Size: 476.1 MB\n",
      "      Avg shard: 476.1 MB\n",
      "\n",
      "================================================================================\n",
      "\n",
      "Summary saved to: C:\\Users\\arjya\\Fall 2025\\Systems for ML\\Project 1\\SML\\format-matters\\runs\\20251127-135410\\builds\\summary.csv\n"
     ]
    }
   ],
   "source": [
    "# Read and display summary\n",
    "import pandas as pd\n",
    "import os\n",
    "\n",
    "# Ensure column headers exist\n",
    "expected_cols = [\n",
    "    \"stage\", \"dataset\", \"format\", \"variant\", \"split\",\n",
    "    \"items\", \"bytes_on_disk\", \"num_files\", \"avg_file_size\",\n",
    "    \"build_wall_s\", \"timestamp\"\n",
    "]\n",
    "\n",
    "# If file exists but lacks headers, rewrite with them\n",
    "if SUMMARY_CSV.exists() and SUMMARY_CSV.stat().st_size > 0:\n",
    "    # Try reading with headers first\n",
    "    try:\n",
    "        summary_df = pd.read_csv(SUMMARY_CSV)\n",
    "        # If headers are missing (numeric columns instead of named ones)\n",
    "        if list(summary_df.columns) != expected_cols:\n",
    "            summary_df = pd.read_csv(SUMMARY_CSV, names=expected_cols, header=None)\n",
    "            summary_df.to_csv(SUMMARY_CSV, index=False)\n",
    "    except pd.errors.ParserError:\n",
    "        # In case of malformed header, force rewrite\n",
    "        summary_df = pd.read_csv(SUMMARY_CSV, names=expected_cols, header=None)\n",
    "        summary_df.to_csv(SUMMARY_CSV, index=False)\n",
    "else:\n",
    "    print(\"No summary data available\")\n",
    "    summary_df = None\n",
    "\n",
    "if summary_df is not None and not summary_df.empty:\n",
    "    print(\"\\nBuild Summary:\")\n",
    "    print(\"=\" * 80)\n",
    "\n",
    "    for dataset in summary_df['dataset'].unique():\n",
    "        print(f\"\\n{dataset}:\")\n",
    "        dataset_df = summary_df[summary_df['dataset'] == dataset]\n",
    "\n",
    "        for variant in dataset_df['variant'].unique():\n",
    "            variant_df = dataset_df[dataset_df['variant'] == variant]\n",
    "            print(f\"\\n  {variant}:\")\n",
    "\n",
    "            for _, row in variant_df.iterrows():\n",
    "                print(f\"    {row['split']}:\")\n",
    "                print(f\"      Items: {int(row['items']):,}\")\n",
    "                print(f\"      Shards: {int(row['num_files'])}\")\n",
    "                print(f\"      Size: {format_bytes(row['bytes_on_disk'])}\")\n",
    "                print(f\"      Avg shard: {format_bytes(row['avg_file_size'])}\")\n",
    "\n",
    "    print(\"\\n\" + \"=\" * 80)\n",
    "    print(f\"\\nSummary saved to: {SUMMARY_CSV}\")\n",
    "else:\n",
    "    print(\"No summary data available\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ✅ TFRecord Build Complete\n",
    "\n",
    "**What was created:**\n",
    "- TFRecord shards with configurable sizes\n",
    "- Multiple variants with different shard sizes and compression\n",
    "- Sequential I/O friendly format\n",
    "- Compatible with TensorFlow and PyTorch\n",
    "\n",
    "**Variants built:**\n",
    "- `shard64_none`: 64MB shards, no compression\n",
    "- `shard64_gzip`: 64MB shards, gzip compression\n",
    "- `shard256_none`: 256MB shards, no compression\n",
    "- `shard256_gzip`: 256MB shards, gzip compression\n",
    "- `shard1024_none`: 1024MB shards, no compression\n",
    "- `shard1024_gzip`: 1024MB shards, gzip compression\n",
    "\n",
    "**Output locations:**\n",
    "- `data/built/<dataset>/tfrecord/<variant>/*.tfrecord[.gz]`\n",
    "\n",
    "**Next steps:**\n",
    "1. Run `13_loader_tfrecord.ipynb` to create the dataloader\n",
    "2. Or continue with other format builders (05)\n",
    "3. Then run training experiments (20-21)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
