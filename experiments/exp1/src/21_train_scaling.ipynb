{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 21 - Scaling Experiments\n",
    "\n",
    "This notebook tests how data formats scale with different configurations:\n",
    "- **Batch size scaling**: 16, 32, 64, 128, 256\n",
    "- **Worker scaling**: 0, 2, 4, 8, 16\n",
    "\n",
    "**Goal:** Understand which formats scale better with:\n",
    "- Larger batch sizes (GPU utilization)\n",
    "- More workers (parallel data loading)\n",
    "\n",
    "**Experiment Design:**\n",
    "- Single epoch per configuration\n",
    "- Measure throughput (samples/second)\n",
    "- Track resource utilization\n",
    "- Compare across all formats\n",
    "\n",
    "**Output:**\n",
    "- Scaling metrics logged to `runs/<session>/summary.csv`\n",
    "- Resource monitoring logs per configuration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import sys\n",
    "import time\n",
    "import json\n",
    "from pathlib import Path\n",
    "from collections import defaultdict\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torchvision import models\n",
    "from tqdm.auto import tqdm\n",
    "\n",
    "# Load common utilities\n",
    "%run ./10_common_utils.ipynb"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Configuration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Detect environment\n",
    "IS_KAGGLE = \"KAGGLE_KERNEL_RUN_TYPE\" in os.environ\n",
    "BASE_DIR = Path('/kaggle/working/format-matters') if IS_KAGGLE else Path('..').resolve()\n",
    "\n",
    "# Create run directory for this session\n",
    "RUN_DIR = BASE_DIR / 'runs' / time.strftime('%Y%m%d-%H%M%S') / 'train_scaling'\n",
    "RUN_DIR.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "SUMMARY_CSV = RUN_DIR / 'summary.csv'\n",
    "SUMMARY_CSV.touch(exist_ok=True)\n",
    "\n",
    "print(f\"Environment: {'Kaggle' if IS_KAGGLE else 'Local'}\")\n",
    "print(f\"Base directory: {BASE_DIR}\")\n",
    "print(f\"Run directory: {RUN_DIR}\")\n",
    "print(f\"Summary log: {SUMMARY_CSV}\")\n",
    "\n",
    "# Write system info\n",
    "write_sysinfo(RUN_DIR / 'sysinfo.json')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Scaling Configuration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Scaling parameters\n",
    "BATCH_SIZES = [16, 32, 64, 128, 256]\n",
    "NUM_WORKERS = [0, 2, 4, 8]\n",
    "\n",
    "# Base configuration\n",
    "BASE_CONFIG = {\n",
    "    'num_batches': 100,  # Run for 100 batches per test\n",
    "    'seed': 42,\n",
    "}\n",
    "\n",
    "# Formats to test\n",
    "FORMATS = [\n",
    "    ('csv', '11_loader_csv.ipynb', 'default'),\n",
    "    ('webdataset', '12_loader_webdataset.ipynb', 'shard256_none'),\n",
    "    ('tfrecord', '13_loader_tfrecord.ipynb', 'shard256_none'),\n",
    "    ('lmdb', '14_loader_lmdb.ipynb', 'compress_none'),\n",
    "]\n",
    "\n",
    "# Dataset to use\n",
    "BUILT_DIR = BASE_DIR / 'data' / 'built'\n",
    "DATASET = None\n",
    "for ds in ['cifar10', 'imagenet-mini', 'tiny-imagenet-200']:\n",
    "    if (BUILT_DIR / ds).exists():\n",
    "        DATASET = ds\n",
    "        break\n",
    "\n",
    "if DATASET is None:\n",
    "    raise RuntimeError(\"No datasets found. Run dataset preparation notebooks first.\")\n",
    "\n",
    "print(f\"\\nScaling Configuration:\")\n",
    "print(f\"  Dataset: {DATASET}\")\n",
    "print(f\"  Batch sizes: {BATCH_SIZES}\")\n",
    "print(f\"  Worker counts: {NUM_WORKERS}\")\n",
    "print(f\"  Batches per test: {BASE_CONFIG['num_batches']}\")\n",
    "print(f\"\\nFormats to test: {len(FORMATS)}\")\n",
    "for fmt_name, _, variant in FORMATS:\n",
    "    print(f\"  - {fmt_name} ({variant})\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Throughput Measurement"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def measure_throughput(dataloader, device, num_batches):\n",
    "    \"\"\"\n",
    "    Measure data loading throughput.\n",
    "    \n",
    "    Args:\n",
    "        dataloader: PyTorch DataLoader\n",
    "        device: Device to transfer data to\n",
    "        num_batches: Number of batches to measure\n",
    "    \n",
    "    Returns:\n",
    "        Dictionary with throughput metrics\n",
    "    \"\"\"\n",
    "    total_samples = 0\n",
    "    start_time = time.time()\n",
    "    \n",
    "    # Warmup (first batch is often slower)\n",
    "    try:\n",
    "        images, labels = next(iter(dataloader))\n",
    "        images = images.to(device)\n",
    "        if device.type == 'cuda':\n",
    "            torch.cuda.synchronize()\n",
    "    except StopIteration:\n",
    "        return {'error': 'Empty dataloader'}\n",
    "    \n",
    "    # Measure throughput\n",
    "    start_time = time.time()\n",
    "    \n",
    "    for i, (images, labels) in enumerate(dataloader):\n",
    "        if i >= num_batches:\n",
    "            break\n",
    "        \n",
    "        # Transfer to device\n",
    "        images = images.to(device)\n",
    "        labels = labels.to(device)\n",
    "        \n",
    "        # Synchronize if using CUDA\n",
    "        if device.type == 'cuda':\n",
    "            torch.cuda.synchronize()\n",
    "        \n",
    "        total_samples += images.size(0)\n",
    "    \n",
    "    elapsed_time = time.time() - start_time\n",
    "    \n",
    "    return {\n",
    "        'total_samples': total_samples,\n",
    "        'elapsed_time': elapsed_time,\n",
    "        'samples_per_sec': total_samples / elapsed_time if elapsed_time > 0 else 0,\n",
    "        'batches_per_sec': num_batches / elapsed_time if elapsed_time > 0 else 0,\n",
    "    }"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Scaling Experiment Runner"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def run_scaling_experiment(\n",
    "    format_name,\n",
    "    loader_notebook,\n",
    "    variant,\n",
    "    dataset,\n",
    "    batch_size,\n",
    "    num_workers,\n",
    "    num_batches,\n",
    "    device\n",
    "):\n",
    "    \"\"\"\n",
    "    Run scaling experiment for a single configuration.\n",
    "    \n",
    "    Args:\n",
    "        format_name: Name of format\n",
    "        loader_notebook: Path to loader notebook\n",
    "        variant: Format variant\n",
    "        dataset: Dataset name\n",
    "        batch_size: Batch size to test\n",
    "        num_workers: Number of workers to test\n",
    "        num_batches: Number of batches to measure\n",
    "        device: Device to use\n",
    "    \n",
    "    Returns:\n",
    "        Dictionary with results\n",
    "    \"\"\"\n",
    "    print(f\"\\nTesting {format_name} - batch_size={batch_size}, workers={num_workers}\")\n",
    "    \n",
    "    try:\n",
    "        # Load dataloader\n",
    "        %run ./{loader_notebook}\n",
    "        \n",
    "        # Create dataloader\n",
    "        dataloader = make_dataloader(\n",
    "            dataset=dataset,\n",
    "            split='train',\n",
    "            batch_size=batch_size,\n",
    "            num_workers=num_workers,\n",
    "            variant=variant,\n",
    "            shuffle=True,\n",
    "            pin_memory=True\n",
    "        )\n",
    "        \n",
    "        # Start resource monitoring\n",
    "        log_path = RUN_DIR / f\"{format_name}_bs{batch_size}_w{num_workers}_metrics.csv\"\n",
    "        monitor_thread, stop_event = start_monitor(log_path, interval=0.5)\n",
    "        \n",
    "        # Measure throughput\n",
    "        metrics = measure_throughput(dataloader, device, num_batches)\n",
    "        \n",
    "        # Stop monitoring\n",
    "        stop_monitor(monitor_thread, stop_event)\n",
    "        \n",
    "        # Compute resource metrics\n",
    "        resource_metrics = compute_metrics_from_logs(log_path)\n",
    "        \n",
    "        # Combine results\n",
    "        results = {\n",
    "            'format': format_name,\n",
    "            'variant': variant,\n",
    "            'dataset': dataset,\n",
    "            'batch_size': batch_size,\n",
    "            'num_workers': num_workers,\n",
    "            **metrics,\n",
    "            **resource_metrics,\n",
    "        }\n",
    "        \n",
    "        # Log to summary\n",
    "        append_to_summary(SUMMARY_CSV, results)\n",
    "        \n",
    "        print(f\"  ✓ Throughput: {metrics['samples_per_sec']:.1f} samples/s\")\n",
    "        \n",
    "        return results\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"  ✗ Failed: {e}\")\n",
    "        return {\n",
    "            'format': format_name,\n",
    "            'variant': variant,\n",
    "            'dataset': dataset,\n",
    "            'batch_size': batch_size,\n",
    "            'num_workers': num_workers,\n",
    "            'error': str(e),\n",
    "        }"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Run Batch Size Scaling Experiments"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get device\n",
    "device = get_device()\n",
    "print(f\"Using device: {device}\\n\")\n",
    "\n",
    "print(\"=\"*80)\n",
    "print(\"BATCH SIZE SCALING EXPERIMENTS\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "# Fixed number of workers for batch size scaling\n",
    "FIXED_WORKERS = 4\n",
    "\n",
    "batch_size_results = []\n",
    "\n",
    "for format_name, loader_notebook, variant in FORMATS:\n",
    "    print(f\"\\n{'='*80}\")\n",
    "    print(f\"Format: {format_name} ({variant})\")\n",
    "    print(f\"{'='*80}\")\n",
    "    \n",
    "    for batch_size in BATCH_SIZES:\n",
    "        result = run_scaling_experiment(\n",
    "            format_name=format_name,\n",
    "            loader_notebook=loader_notebook,\n",
    "            variant=variant,\n",
    "            dataset=DATASET,\n",
    "            batch_size=batch_size,\n",
    "            num_workers=FIXED_WORKERS,\n",
    "            num_batches=BASE_CONFIG['num_batches'],\n",
    "            device=device\n",
    "        )\n",
    "        batch_size_results.append(result)\n",
    "        \n",
    "        # Small delay\n",
    "        time.sleep(2)\n",
    "\n",
    "print(f\"\\n{'='*80}\")\n",
    "print(\"Batch size scaling experiments completed!\")\n",
    "print(f\"{'='*80}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Run Worker Scaling Experiments"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"WORKER SCALING EXPERIMENTS\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "# Fixed batch size for worker scaling\n",
    "FIXED_BATCH_SIZE = 64\n",
    "\n",
    "worker_results = []\n",
    "\n",
    "for format_name, loader_notebook, variant in FORMATS:\n",
    "    print(f\"\\n{'='*80}\")\n",
    "    print(f\"Format: {format_name} ({variant})\")\n",
    "    print(f\"{'='*80}\")\n",
    "    \n",
    "    for num_workers in NUM_WORKERS:\n",
    "        result = run_scaling_experiment(\n",
    "            format_name=format_name,\n",
    "            loader_notebook=loader_notebook,\n",
    "            variant=variant,\n",
    "            dataset=DATASET,\n",
    "            batch_size=FIXED_BATCH_SIZE,\n",
    "            num_workers=num_workers,\n",
    "            num_batches=BASE_CONFIG['num_batches'],\n",
    "            device=device\n",
    "        )\n",
    "        worker_results.append(result)\n",
    "        \n",
    "        # Small delay\n",
    "        time.sleep(2)\n",
    "\n",
    "print(f\"\\n{'='*80}\")\n",
    "print(\"Worker scaling experiments completed!\")\n",
    "print(f\"{'='*80}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Results Summary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "if SUMMARY_CSV.exists() and SUMMARY_CSV.stat().st_size > 0:\n",
    "    df = pd.read_csv(SUMMARY_CSV)\n",
    "    \n",
    "    print(\"\\n\" + \"=\"*80)\n",
    "    print(\"SCALING EXPERIMENTS SUMMARY\")\n",
    "    print(\"=\"*80)\n",
    "    \n",
    "    # Batch Size Scaling Results\n",
    "    print(f\"\\n\\nBatch Size Scaling (workers={FIXED_WORKERS}):\\n\")\n",
    "    print(f\"{'Format':<15} {'Batch Size':<12} {'Throughput (samp/s)':<20} {'GPU Util %':<12}\")\n",
    "    print(\"-\" * 70)\n",
    "    \n",
    "    batch_df = df[df['num_workers'] == FIXED_WORKERS].sort_values(['format', 'batch_size'])\n",
    "    for _, row in batch_df.iterrows():\n",
    "        gpu_util = row.get('gpu_util_mean', 0) or 0\n",
    "        print(f\"{row['format']:<15} {row['batch_size']:<12} \"\n",
    "              f\"{row['samples_per_sec']:>18.1f} {gpu_util:>10.1f}%\")\n",
    "    \n",
    "    # Worker Scaling Results\n",
    "    print(f\"\\n\\nWorker Scaling (batch_size={FIXED_BATCH_SIZE}):\\n\")\n",
    "    print(f\"{'Format':<15} {'Workers':<10} {'Throughput (samp/s)':<20} {'CPU Util %':<12}\")\n",
    "    print(\"-\" * 70)\n",
    "    \n",
    "    worker_df = df[df['batch_size'] == FIXED_BATCH_SIZE].sort_values(['format', 'num_workers'])\n",
    "    for _, row in worker_df.iterrows():\n",
    "        cpu_util = row.get('cpu_util_mean', 0) or 0\n",
    "        print(f\"{row['format']:<15} {row['num_workers']:<10} \"\n",
    "              f\"{row['samples_per_sec']:>18.1f} {cpu_util:>10.1f}%\")\n",
    "    \n",
    "    # Best configurations\n",
    "    print(f\"\\n\\nBest Configurations (by throughput):\\n\")\n",
    "    print(f\"{'Format':<15} {'Batch Size':<12} {'Workers':<10} {'Throughput (samp/s)':<20}\")\n",
    "    print(\"-\" * 70)\n",
    "    \n",
    "    for format_name in df['format'].unique():\n",
    "        format_df = df[df['format'] == format_name]\n",
    "        best = format_df.loc[format_df['samples_per_sec'].idxmax()]\n",
    "        print(f\"{best['format']:<15} {best['batch_size']:<12} \"\n",
    "              f\"{best['num_workers']:<10} {best['samples_per_sec']:>18.1f}\")\n",
    "    \n",
    "    print(\"\\n\" + \"=\"*80)\n",
    "    print(f\"\\nResults saved to: {SUMMARY_CSV}\")\n",
    "    print(f\"Resource logs saved to: {RUN_DIR}\")\n",
    "else:\n",
    "    print(\"No results available\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ✅ Scaling Experiments Complete\n",
    "\n",
    "**What was measured:**\n",
    "- Throughput scaling with batch size\n",
    "- Throughput scaling with number of workers\n",
    "- Resource utilization at different scales\n",
    "- Optimal configurations per format\n",
    "\n",
    "**Key Insights:**\n",
    "- Which formats scale better with larger batches?\n",
    "- Which formats benefit most from parallel workers?\n",
    "- What are the optimal configurations for each format?\n",
    "- Where are the bottlenecks (GPU, CPU, I/O)?\n",
    "\n",
    "**Next steps:**\n",
    "1. Aggregate all results (30_analysis_summary.ipynb)\n",
    "2. Create visualizations (31_analysis_plots.ipynb)\n",
    "3. Generate decision guide (40_decision_guide.ipynb)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
