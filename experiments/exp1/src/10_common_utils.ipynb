{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 10 - Common Utilities\n",
    "\n",
    "This notebook contains shared utility functions used across the project.\n",
    "\n",
    "**Usage in other notebooks:**\n",
    "```python\n",
    "%run ../notebooks/10_common_utils.ipynb\n",
    "```\n",
    "\n",
    "**Provides:**\n",
    "- Seed setting for reproducibility\n",
    "- Standard image transforms\n",
    "- System information writer\n",
    "- Timing utilities\n",
    "- Resource monitoring thread\n",
    "- Logging helpers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import sys\n",
    "import json\n",
    "import time\n",
    "import random\n",
    "import platform\n",
    "import subprocess\n",
    "import threading\n",
    "from pathlib import Path\n",
    "from datetime import datetime\n",
    "from typing import Optional, Dict, Any\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import psutil\n",
    "import torch\n",
    "import torchvision.transforms as transforms\n",
    "from PIL import Image"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Seed Setting for Reproducibility"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def set_seed(seed: int = 42):\n",
    "    \"\"\"\n",
    "    Set random seeds for reproducibility across Python, NumPy, and PyTorch.\n",
    "    \n",
    "    Args:\n",
    "        seed: Random seed value\n",
    "    \"\"\"\n",
    "    random.seed(seed)\n",
    "    np.random.seed(seed)\n",
    "    torch.manual_seed(seed)\n",
    "    torch.cuda.manual_seed_all(seed)\n",
    "    \n",
    "    # Make PyTorch deterministic (may impact performance)\n",
    "    torch.backends.cudnn.deterministic = True\n",
    "    torch.backends.cudnn.benchmark = False\n",
    "    \n",
    "    # Set environment variable for Python hash seed\n",
    "    os.environ['PYTHONHASHSEED'] = str(seed)\n",
    "    \n",
    "    print(f\"\u2713 Random seed set to {seed}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Standard Image Transforms"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_transforms(augment: bool = False):\n",
    "    \"\"\"\n",
    "    Get standard image transforms for the project.\n",
    "    \n",
    "    Args:\n",
    "        augment: Whether to include data augmentation (for training)\n",
    "    \n",
    "    Returns:\n",
    "        torchvision.transforms.Compose object\n",
    "    \"\"\"\n",
    "    # ImageNet normalization stats\n",
    "    normalize = transforms.Normalize(\n",
    "        mean=[0.485, 0.456, 0.406],\n",
    "        std=[0.229, 0.224, 0.225]\n",
    "    )\n",
    "    \n",
    "    if augment:\n",
    "        # Training transforms with augmentation\n",
    "        transform = transforms.Compose([\n",
    "            transforms.RandomResizedCrop(224),\n",
    "            transforms.RandomHorizontalFlip(),\n",
    "            transforms.ToTensor(),\n",
    "            normalize,\n",
    "        ])\n",
    "    else:\n",
    "        # Standard transforms (no augmentation for fair comparison)\n",
    "        transform = transforms.Compose([\n",
    "            transforms.Resize(256),\n",
    "            transforms.CenterCrop(224),\n",
    "            transforms.ToTensor(),\n",
    "            normalize,\n",
    "        ])\n",
    "    \n",
    "    return transform\n",
    "\n",
    "\n",
    "# Create standard transform instance\n",
    "STANDARD_TRANSFORM = get_transforms(augment=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. System Information Writer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_gpu_info():\n",
    "    \"\"\"\n",
    "    Get GPU information using nvidia-smi.\n",
    "    \n",
    "    Returns:\n",
    "        Dict with GPU info or None if not available\n",
    "    \"\"\"\n",
    "    if not torch.cuda.is_available():\n",
    "        return None\n",
    "    \n",
    "    try:\n",
    "        # Try to get driver version\n",
    "        result = subprocess.run(\n",
    "            ['nvidia-smi', '--query-gpu=driver_version', '--format=csv,noheader'],\n",
    "            capture_output=True, text=True, timeout=5\n",
    "        )\n",
    "        driver_version = result.stdout.strip() if result.returncode == 0 else 'unknown'\n",
    "    except:\n",
    "        driver_version = 'unknown'\n",
    "    \n",
    "    return {\n",
    "        'name': torch.cuda.get_device_name(0),\n",
    "        'count': torch.cuda.device_count(),\n",
    "        'memory_gb': round(torch.cuda.get_device_properties(0).total_memory / (1024**3), 2),\n",
    "        'cuda_version': torch.version.cuda,\n",
    "        'cudnn_version': torch.backends.cudnn.version(),\n",
    "        'driver_version': driver_version,\n",
    "    }\n",
    "\n",
    "\n",
    "def write_sysinfo(output_path: Path):\n",
    "    \"\"\"\n",
    "    Write system information to JSON file.\n",
    "    \n",
    "    Args:\n",
    "        output_path: Path to output JSON file\n",
    "    \"\"\"\n",
    "    sysinfo = {\n",
    "        'timestamp': datetime.now().isoformat(),\n",
    "        'hostname': platform.node(),\n",
    "        'os': {\n",
    "            'system': platform.system(),\n",
    "            'release': platform.release(),\n",
    "            'version': platform.version(),\n",
    "            'platform': platform.platform(),\n",
    "        },\n",
    "        'cpu': {\n",
    "            'processor': platform.processor(),\n",
    "            'physical_cores': psutil.cpu_count(logical=False),\n",
    "            'logical_cores': psutil.cpu_count(logical=True),\n",
    "            'frequency_mhz': psutil.cpu_freq().current if psutil.cpu_freq() else None,\n",
    "        },\n",
    "        'memory': {\n",
    "            'total_gb': round(psutil.virtual_memory().total / (1024**3), 2),\n",
    "            'available_gb': round(psutil.virtual_memory().available / (1024**3), 2),\n",
    "        },\n",
    "        'disk': {\n",
    "            'total_gb': round(psutil.disk_usage('/').total / (1024**3), 2),\n",
    "            'free_gb': round(psutil.disk_usage('/').free / (1024**3), 2),\n",
    "        },\n",
    "        'python': {\n",
    "            'version': platform.python_version(),\n",
    "            'implementation': platform.python_implementation(),\n",
    "        },\n",
    "        'pytorch': {\n",
    "            'version': torch.__version__,\n",
    "            'cuda_available': torch.cuda.is_available(),\n",
    "        },\n",
    "        'gpu': get_gpu_info(),\n",
    "    }\n",
    "    \n",
    "    output_path = Path(output_path)\n",
    "    output_path.parent.mkdir(parents=True, exist_ok=True)\n",
    "    output_path.write_text(json.dumps(sysinfo, indent=2))\n",
    "    print(f\"\u2713 System info written to {output_path}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Timing Utilities"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def time_first_batch(dataloader, device='cuda'):\n",
    "    \"\"\"\n",
    "    Measure time to load and transfer first batch to device.\n",
    "    \n",
    "    Args:\n",
    "        dataloader: PyTorch DataLoader\n",
    "        device: Target device ('cuda' or 'cpu')\n",
    "    \n",
    "    Returns:\n",
    "        Time in seconds\n",
    "    \"\"\"\n",
    "    device = torch.device(device if torch.cuda.is_available() else 'cpu')\n",
    "    \n",
    "    start = time.time()\n",
    "    batch = next(iter(dataloader))\n",
    "    \n",
    "    # Transfer to device if needed\n",
    "    if isinstance(batch, (list, tuple)):\n",
    "        batch = [b.to(device) if isinstance(b, torch.Tensor) else b for b in batch]\n",
    "    elif isinstance(batch, torch.Tensor):\n",
    "        batch = batch.to(device)\n",
    "    \n",
    "    # Synchronize if using CUDA\n",
    "    if device.type == 'cuda':\n",
    "        torch.cuda.synchronize()\n",
    "    \n",
    "    elapsed = time.time() - start\n",
    "    return elapsed\n",
    "\n",
    "\n",
    "class Timer:\n",
    "    \"\"\"\n",
    "    Simple context manager for timing code blocks.\n",
    "    \n",
    "    Usage:\n",
    "        with Timer(\"operation\") as t:\n",
    "            # code to time\n",
    "            pass\n",
    "        print(f\"Took {t.elapsed:.2f}s\")\n",
    "    \"\"\"\n",
    "    def __init__(self, name: str = \"Operation\", verbose: bool = True):\n",
    "        self.name = name\n",
    "        self.verbose = verbose\n",
    "        self.elapsed = 0\n",
    "    \n",
    "    def __enter__(self):\n",
    "        self.start = time.time()\n",
    "        return self\n",
    "    \n",
    "    def __exit__(self, *args):\n",
    "        self.elapsed = time.time() - self.start\n",
    "        if self.verbose:\n",
    "            print(f\"{self.name} took {self.elapsed:.2f}s\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Resource Monitoring Thread"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_gpu_utilization():\n",
    "    \"\"\"\n",
    "    Get current GPU utilization and memory using nvidia-smi.\n",
    "    \n",
    "    Returns:\n",
    "        Tuple of (utilization_percent, memory_used_mb) or (None, None)\n",
    "    \"\"\"\n",
    "    if not torch.cuda.is_available():\n",
    "        return None, None\n",
    "    \n",
    "    try:\n",
    "        result = subprocess.run(\n",
    "            ['nvidia-smi', '--query-gpu=utilization.gpu,memory.used',\n",
    "             '--format=csv,noheader,nounits'],\n",
    "            capture_output=True, text=True, timeout=2\n",
    "        )\n",
    "        if result.returncode == 0:\n",
    "            util, mem = result.stdout.strip().split(',')\n",
    "            return float(util), float(mem)\n",
    "    except:\n",
    "        pass\n",
    "    \n",
    "    return None, None\n",
    "\n",
    "\n",
    "def monitor_resources(log_path: Path, interval: float, stop_event: threading.Event):\n",
    "    \"\"\"\n",
    "    Monitor system resources in a background thread.\n",
    "    \n",
    "    Args:\n",
    "        log_path: Path to CSV log file\n",
    "        interval: Sampling interval in seconds\n",
    "        stop_event: Threading event to signal stop\n",
    "    \"\"\"\n",
    "    log_path = Path(log_path)\n",
    "    log_path.parent.mkdir(parents=True, exist_ok=True)\n",
    "    \n",
    "    # Initialize CSV with header\n",
    "    with open(log_path, 'w') as f:\n",
    "        f.write('ts,gpu_util,gpu_mem_mb,cpu_pct,rss_mb,read_mb,write_mb\\n')\n",
    "    \n",
    "    process = psutil.Process()\n",
    "    io_start = process.io_counters() if hasattr(process, 'io_counters') else None\n",
    "    \n",
    "    while not stop_event.is_set():\n",
    "        timestamp = time.time()\n",
    "        \n",
    "        # GPU metrics\n",
    "        gpu_util, gpu_mem = get_gpu_utilization()\n",
    "        \n",
    "        # CPU metrics\n",
    "        cpu_pct = process.cpu_percent()\n",
    "        \n",
    "        # Memory metrics\n",
    "        rss_mb = process.memory_info().rss / (1024 * 1024)\n",
    "        \n",
    "        # Disk I/O metrics\n",
    "        if io_start and hasattr(process, 'io_counters'):\n",
    "            io_now = process.io_counters()\n",
    "            read_mb = (io_now.read_bytes - io_start.read_bytes) / (1024 * 1024)\n",
    "            write_mb = (io_now.write_bytes - io_start.write_bytes) / (1024 * 1024)\n",
    "        else:\n",
    "            read_mb, write_mb = 0, 0\n",
    "        \n",
    "        # Write to log\n",
    "        with open(log_path, 'a') as f:\n",
    "            f.write(f'{timestamp},{gpu_util},{gpu_mem},{cpu_pct},{rss_mb:.1f},{read_mb:.2f},{write_mb:.2f}\\n')\n",
    "        \n",
    "        time.sleep(interval)\n",
    "\n",
    "\n",
    "def start_monitor(log_path: Path, interval: float = 0.5) -> tuple:\n",
    "    \"\"\"\n",
    "    Start resource monitoring in background thread.\n",
    "    \n",
    "    Args:\n",
    "        log_path: Path to CSV log file\n",
    "        interval: Sampling interval in seconds\n",
    "    \n",
    "    Returns:\n",
    "        Tuple of (thread, stop_event) for later cleanup\n",
    "    \"\"\"\n",
    "    stop_event = threading.Event()\n",
    "    thread = threading.Thread(\n",
    "        target=monitor_resources,\n",
    "        args=(log_path, interval, stop_event),\n",
    "        daemon=True\n",
    "    )\n",
    "    thread.start()\n",
    "    print(f\"\u2713 Resource monitoring started (logging to {log_path})\")\n",
    "    return thread, stop_event\n",
    "\n",
    "\n",
    "def stop_monitor(thread: threading.Thread, stop_event: threading.Event):\n",
    "    \"\"\"\n",
    "    Stop resource monitoring thread.\n",
    "    \n",
    "    Args:\n",
    "        thread: Monitoring thread\n",
    "        stop_event: Stop event to signal\n",
    "    \"\"\"\n",
    "    stop_event.set()\n",
    "    thread.join(timeout=2)\n",
    "    print(\"\u2713 Resource monitoring stopped\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. Logging Helpers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def append_to_summary(summary_path: Path, row_dict: Dict[str, Any]):\n",
    "    \"\"\"\n",
    "    Append a row to summary CSV file.\n",
    "    \n",
    "    Args:\n",
    "        summary_path: Path to summary CSV\n",
    "        row_dict: Dictionary of column:value pairs\n",
    "    \"\"\"\n",
    "    summary_path = Path(summary_path)\n",
    "    summary_path.parent.mkdir(parents=True, exist_ok=True)\n",
    "    \n",
    "    # Add timestamp if not present\n",
    "    if 'timestamp' not in row_dict:\n",
    "        row_dict['timestamp'] = datetime.now().isoformat()\n",
    "    \n",
    "    # Create DataFrame\n",
    "    df = pd.DataFrame([row_dict])\n",
    "    \n",
    "    # Append to file\n",
    "    if summary_path.exists() and summary_path.stat().st_size > 0:\n",
    "        df.to_csv(summary_path, mode='a', header=False, index=False)\n",
    "    else:\n",
    "        df.to_csv(summary_path, mode='w', header=True, index=False)\n",
    "\n",
    "\n",
    "def compute_metrics_from_logs(log_path: Path) -> Dict[str, float]:\n",
    "    \"\"\"\n",
    "    Compute summary metrics from resource monitoring logs.\n",
    "    \n",
    "    Args:\n",
    "        log_path: Path to logs_metrics.csv\n",
    "    \n",
    "    Returns:\n",
    "        Dictionary of computed metrics\n",
    "    \"\"\"\n",
    "    df = pd.read_csv(log_path)\n",
    "    \n",
    "    metrics = {}\n",
    "    \n",
    "    # GPU metrics\n",
    "    if 'gpu_util' in df.columns and df['gpu_util'].notna().any():\n",
    "        metrics['gpu_util_mean'] = df['gpu_util'].mean()\n",
    "        metrics['gpu_idle_pct'] = (df['gpu_util'] < 10).mean() * 100\n",
    "        metrics['gpu_mem_mb_peak'] = df['gpu_mem_mb'].max()\n",
    "    else:\n",
    "        metrics['gpu_util_mean'] = None\n",
    "        metrics['gpu_idle_pct'] = None\n",
    "        metrics['gpu_mem_mb_peak'] = None\n",
    "    \n",
    "    # CPU metrics\n",
    "    metrics['cpu_util_mean'] = df['cpu_pct'].mean()\n",
    "    \n",
    "    # Memory metrics\n",
    "    metrics['rss_mb_peak'] = df['rss_mb'].max()\n",
    "    \n",
    "    # Disk I/O metrics (rate per second)\n",
    "    if len(df) > 1:\n",
    "        duration = df['ts'].iloc[-1] - df['ts'].iloc[0]\n",
    "        metrics['disk_read_mb_s_mean'] = df['read_mb'].iloc[-1] / duration if duration > 0 else 0\n",
    "        metrics['disk_write_mb_s_mean'] = df['write_mb'].iloc[-1] / duration if duration > 0 else 0\n",
    "    else:\n",
    "        metrics['disk_read_mb_s_mean'] = 0\n",
    "        metrics['disk_write_mb_s_mean'] = 0\n",
    "    \n",
    "    return metrics"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7. Utility Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_device():\n",
    "    \"\"\"\n",
    "    Get the best available device (CUDA > MPS > CPU).\n",
    "    \n",
    "    Returns:\n",
    "        torch.device\n",
    "    \"\"\"\n",
    "    if torch.cuda.is_available():\n",
    "        return torch.device('cuda')\n",
    "    elif torch.backends.mps.is_available():\n",
    "        return torch.device('mps')\n",
    "    else:\n",
    "        return torch.device('cpu')\n",
    "\n",
    "\n",
    "def format_bytes(bytes_val: int) -> str:\n",
    "    \"\"\"\n",
    "    Format bytes as human-readable string.\n",
    "    \n",
    "    Args:\n",
    "        bytes_val: Number of bytes\n",
    "    \n",
    "    Returns:\n",
    "        Formatted string (e.g., \"1.5 GB\")\n",
    "    \"\"\"\n",
    "    for unit in ['B', 'KB', 'MB', 'GB', 'TB']:\n",
    "        if bytes_val < 1024.0:\n",
    "            return f\"{bytes_val:.1f} {unit}\"\n",
    "        bytes_val /= 1024.0\n",
    "    return f\"{bytes_val:.1f} PB\"\n",
    "\n",
    "\n",
    "def count_parameters(model: torch.nn.Module) -> int:\n",
    "    \"\"\"\n",
    "    Count trainable parameters in a model.\n",
    "    \n",
    "    Args:\n",
    "        model: PyTorch model\n",
    "    \n",
    "    Returns:\n",
    "        Number of trainable parameters\n",
    "    \"\"\"\n",
    "    return sum(p.numel() for p in model.parameters() if p.requires_grad)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## \u2705 Utilities Loaded\n",
    "\n",
    "All utility functions are now available. Use `%run ../notebooks/10_common_utils.ipynb` in other notebooks to import these functions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test that everything loaded correctly\n",
    "if __name__ == \"__main__\":\n",
    "    print(\"\u2713 Common utilities loaded successfully\")\n",
    "    print(\"\\nAvailable functions:\")\n",
    "    print(\"  - set_seed(seed)\")\n",
    "    print(\"  - get_transforms(augment)\")\n",
    "    print(\"  - write_sysinfo(path)\")\n",
    "    print(\"  - time_first_batch(dataloader, device)\")\n",
    "    print(\"  - start_monitor(log_path, interval)\")\n",
    "    print(\"  - stop_monitor(thread, stop_event)\")\n",
    "    print(\"  - append_to_summary(path, row_dict)\")\n",
    "    print(\"  - compute_metrics_from_logs(log_path)\")\n",
    "    print(\"  - get_device()\")\n",
    "    print(\"  - format_bytes(bytes)\")\n",
    "    print(\"  - count_parameters(model)\")\n",
    "    print(\"\\nConstants:\")\n",
    "    print(\"  - STANDARD_TRANSFORM\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}