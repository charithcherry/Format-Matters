{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Debug Validation Data\n",
    "\n",
    "This notebook inspects the validation data across all formats to identify label issues."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import sys\n",
    "from pathlib import Path\n",
    "import pandas as pd\n",
    "import webdataset as wds\n",
    "import tensorflow as tf\n",
    "import lmdb\n",
    "import pickle\n",
    "from collections import Counter\n",
    "\n",
    "# Detect environment\n",
    "IS_KAGGLE = \"KAGGLE_KERNEL_RUN_TYPE\" in os.environ\n",
    "BASE_DIR = Path('/kaggle/working/format-matters') if IS_KAGGLE else Path('..').resolve()\n",
    "BUILT_DIR = BASE_DIR / 'data' / 'built'\n",
    "DATASET = 'cifar10'\n",
    "\n",
    "print(\"=\" * 80)\n",
    "print(\"VALIDATION DATA INSPECTION\")\n",
    "print(\"=\" * 80)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. CSV Format"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"\\n1. CSV FORMAT (val.csv)\")\n",
    "csv_path = BUILT_DIR / DATASET / 'csv' / 'default' / 'val.csv'\n",
    "if csv_path.exists():\n",
    "    df = pd.read_csv(csv_path)\n",
    "    print(f\"   Total samples: {len(df)}\")\n",
    "    print(f\"   Unique labels: {df['label'].nunique()}\")\n",
    "    print(f\"   Label distribution:\")\n",
    "    label_counts = df['label'].value_counts().sort_index()\n",
    "    for label, count in label_counts.items():\n",
    "        print(f\"      Label {label}: {count} samples\")\n",
    "    print(f\"\\n   First 20 samples:\")\n",
    "    print(df[['path', 'label']].head(20))\n",
    "else:\n",
    "    print(\"   NOT FOUND\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. WebDataset Format"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"\\n2. WEBDATASET FORMAT (val-*.tar)\")\n",
    "wds_dir = BUILT_DIR / DATASET / 'webdataset' / 'shard256_none'\n",
    "val_shards = sorted(wds_dir.glob('val-*.tar'))\n",
    "if val_shards:\n",
    "    print(f\"   Found {len(val_shards)} shard(s): {[s.name for s in val_shards]}\")\n",
    "    labels = []\n",
    "    keys = []\n",
    "    \n",
    "    for shard in val_shards:\n",
    "        shard_path = \"file://\" + shard.as_posix()\n",
    "        dataset = wds.WebDataset(shard_path)\n",
    "        \n",
    "        for i, sample in enumerate(dataset):\n",
    "            if '__key__' in sample:\n",
    "                keys.append(sample['__key__'])\n",
    "            if 'cls' in sample:\n",
    "                label_bytes = sample['cls']\n",
    "                label = int(label_bytes.decode('utf-8') if isinstance(label_bytes, bytes) else label_bytes)\n",
    "                labels.append(label)\n",
    "            if len(labels) >= 20000:  # Get all val samples\n",
    "                break\n",
    "        if len(labels) >= 20000:\n",
    "            break\n",
    "    \n",
    "    print(f\"   Total samples inspected: {len(labels)}\")\n",
    "    print(f\"   Unique labels: {len(set(labels))}\")\n",
    "    print(f\"   Label distribution:\")\n",
    "    label_counts = Counter(labels)\n",
    "    for label in sorted(label_counts.keys()):\n",
    "        print(f\"      Label {label}: {label_counts[label]} samples\")\n",
    "    print(f\"\\n   First 20 samples:\")\n",
    "    for i in range(min(20, len(labels))):\n",
    "        print(f\"      {keys[i]}: label={labels[i]}\")\n",
    "else:\n",
    "    print(\"   NOT FOUND\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. TFRecord Format"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"\\n3. TFRECORD FORMAT (val-*.tfrecord)\")\n",
    "tfr_dir = BUILT_DIR / DATASET / 'tfrecord' / 'shard256_none'\n",
    "val_shards = sorted(tfr_dir.glob('val-*.tfrecord'))\n",
    "if val_shards:\n",
    "    print(f\"   Found {len(val_shards)} shard(s): {[s.name for s in val_shards]}\")\n",
    "    labels = []\n",
    "    \n",
    "    for shard in val_shards:\n",
    "        dataset = tf.data.TFRecordDataset(str(shard))\n",
    "        \n",
    "        for i, raw_record in enumerate(dataset):\n",
    "            example = tf.train.Example()\n",
    "            example.ParseFromString(raw_record.numpy())\n",
    "            label = example.features.feature['label'].int64_list.value[0]\n",
    "            labels.append(int(label))\n",
    "            if len(labels) >= 20000:\n",
    "                break\n",
    "        if len(labels) >= 20000:\n",
    "            break\n",
    "    \n",
    "    print(f\"   Total samples inspected: {len(labels)}\")\n",
    "    print(f\"   Unique labels: {len(set(labels))}\")\n",
    "    print(f\"   Label distribution:\")\n",
    "    label_counts = Counter(labels)\n",
    "    for label in sorted(label_counts.keys()):\n",
    "        print(f\"      Label {label}: {label_counts[label]} samples\")\n",
    "    print(f\"\\n   First 20 labels: {labels[:20]}\")\n",
    "else:\n",
    "    print(\"   NOT FOUND\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. LMDB Format"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"\\n4. LMDB FORMAT (val.lmdb)\")\n",
    "lmdb_path = BUILT_DIR / DATASET / 'lmdb' / 'compress_none' / 'val.lmdb'\n",
    "if lmdb_path.exists():\n",
    "    env = lmdb.open(str(lmdb_path), readonly=True, lock=False)\n",
    "    labels = []\n",
    "    \n",
    "    with env.begin() as txn:\n",
    "        metadata_bytes = txn.get(b'__metadata__')\n",
    "        if metadata_bytes:\n",
    "            metadata = pickle.loads(metadata_bytes)\n",
    "            num_samples = metadata['num_samples']\n",
    "            print(f\"   Total samples: {num_samples}\")\n",
    "            \n",
    "            # Read all samples\n",
    "            for idx in range(min(20000, num_samples)):\n",
    "                key = f\"{idx:08d}\".encode('utf-8')\n",
    "                entry_bytes = txn.get(key)\n",
    "                if entry_bytes:\n",
    "                    entry = pickle.loads(entry_bytes)\n",
    "                    labels.append(entry['label'])\n",
    "    \n",
    "    env.close()\n",
    "    \n",
    "    print(f\"   Total samples inspected: {len(labels)}\")\n",
    "    print(f\"   Unique labels: {len(set(labels))}\")\n",
    "    print(f\"   Label distribution:\")\n",
    "    label_counts = Counter(labels)\n",
    "    for label in sorted(label_counts.keys()):\n",
    "        print(f\"      Label {label}: {label_counts[label]} samples\")\n",
    "    print(f\"\\n   First 20 labels: {labels[:20]}\")\n",
    "else:\n",
    "    print(\"   NOT FOUND\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Summary\n",
    "\n",
    "Compare the label distributions across all formats to identify discrepancies."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"\\n\" + \"=\" * 80)\n",
    "print(\"INSPECTION COMPLETE\")\n",
    "print(\"=\" * 80)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.9.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
