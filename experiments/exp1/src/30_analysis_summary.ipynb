{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 30 - Analysis Summary\n",
    "\n",
    "This notebook aggregates and summarizes all experimental results:\n",
    "- Build statistics (disk usage, build time)\n",
    "- Baseline training results (throughput, accuracy)\n",
    "- Scaling experiments (batch size, workers)\n",
    "- Resource utilization (GPU, CPU, disk I/O)\n",
    "\n",
    "**Output:**\n",
    "- Comprehensive summary tables\n",
    "- Statistical comparisons\n",
    "- Performance rankings\n",
    "- Key findings and insights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "âœ“ Common utilities loaded successfully\n",
      "\n",
      "Available functions:\n",
      "  - set_seed(seed)\n",
      "  - get_transforms(augment)\n",
      "  - write_sysinfo(path)\n",
      "  - time_first_batch(dataloader, device)\n",
      "  - start_monitor(log_path, interval)\n",
      "  - stop_monitor(thread, stop_event)\n",
      "  - append_to_summary(path, row_dict)\n",
      "  - compute_metrics_from_logs(log_path)\n",
      "  - get_device()\n",
      "  - format_bytes(bytes)\n",
      "  - count_parameters(model)\n",
      "\n",
      "Constants:\n",
      "  - STANDARD_TRANSFORM\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import sys\n",
    "from pathlib import Path\n",
    "from collections import defaultdict\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "# Load common utilities\n",
    "%run ./10_common_utils.ipynb"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Configuration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Environment: Local\n",
      "Base directory: C:\\Users\\arjya\\Fall 2025\\Systems for ML\\Project 1\\SML\\format-matters\n",
      "Runs directory: C:\\Users\\arjya\\Fall 2025\\Systems for ML\\Project 1\\SML\\format-matters\\runs\n"
     ]
    }
   ],
   "source": [
    "# Detect environment\n",
    "IS_KAGGLE = \"KAGGLE_KERNEL_RUN_TYPE\" in os.environ\n",
    "BASE_DIR = Path('/kaggle/working/format-matters') if IS_KAGGLE else Path('..').resolve()\n",
    "\n",
    "RUNS_DIR = BASE_DIR / 'runs'\n",
    "\n",
    "print(f\"Environment: {'Kaggle' if IS_KAGGLE else 'Local'}\")\n",
    "print(f\"Base directory: {BASE_DIR}\")\n",
    "print(f\"Runs directory: {RUNS_DIR}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load All Results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "âœ“ Loaded 68 rows from builds\n",
      "âœ“ Loaded 12 rows from train_baselines\n",
      "âš  No data found for train_scaling\n"
     ]
    }
   ],
   "source": [
    "def load_all_summaries(runs_dir):\n",
    "    \"\"\"\n",
    "    Load all summary.csv files from runs directory.\n",
    "    \n",
    "    Args:\n",
    "        runs_dir: Path to runs directory\n",
    "    \n",
    "    Returns:\n",
    "        Dictionary of DataFrames by experiment type\n",
    "    \"\"\"\n",
    "    summaries = {\n",
    "        'builds': [],\n",
    "        'train_baselines': [],\n",
    "        'train_scaling': [],\n",
    "    }\n",
    "    \n",
    "    if not runs_dir.exists():\n",
    "        print(f\"âš  Runs directory not found: {runs_dir}\")\n",
    "        return summaries\n",
    "    \n",
    "    # Find all summary.csv files\n",
    "    for summary_file in runs_dir.rglob('summary.csv'):\n",
    "        if summary_file.stat().st_size == 0:\n",
    "            continue\n",
    "        \n",
    "        try:\n",
    "            df = pd.read_csv(summary_file)\n",
    "            \n",
    "            # Determine experiment type from path\n",
    "            if 'builds' in str(summary_file):\n",
    "                summaries['builds'].append(df)\n",
    "            elif 'train_baselines' in str(summary_file):\n",
    "                summaries['train_baselines'].append(df)\n",
    "            elif 'train_scaling' in str(summary_file):\n",
    "                summaries['train_scaling'].append(df)\n",
    "        except Exception as e:\n",
    "            print(f\"âš  Failed to load {summary_file}: {e}\")\n",
    "    \n",
    "    # Concatenate DataFrames\n",
    "    for key in summaries:\n",
    "        if summaries[key]:\n",
    "            summaries[key] = pd.concat(summaries[key], ignore_index=True)\n",
    "            print(f\"âœ“ Loaded {len(summaries[key])} rows from {key}\")\n",
    "        else:\n",
    "            summaries[key] = pd.DataFrame()\n",
    "            print(f\"âš  No data found for {key}\")\n",
    "    \n",
    "    return summaries\n",
    "\n",
    "# Load all results\n",
    "results = load_all_summaries(RUNS_DIR)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Build Statistics Summary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "================================================================================\n",
      "BUILD STATISTICS SUMMARY\n",
      "================================================================================\n",
      "\n",
      "Disk Usage by Format:\n",
      "\n",
      "Format          Variant              Items      Size            Files    Build Time  \n",
      "------------------------------------------------------------------------------------------\n",
      "csv             default              296004     35.4 MB         8        29.39s\n",
      "lmdb            compress_lz4         98668      19.1 GB         4        29.19s\n",
      "lmdb            compress_none        98668      19.1 GB         4        46.42s\n",
      "lmdb            compress_zstd        98668      19.1 GB         4        40.66s\n",
      "tfrecord        shard1024_gzip       98668      4.0 GB          7        116.29s\n",
      "tfrecord        shard1024_none       98668      4.1 GB          7        44.95s\n",
      "tfrecord        shard256_gzip        98668      4.0 GB          18       122.70s\n",
      "tfrecord        shard256_none        98668      4.1 GB          18       37.93s\n",
      "tfrecord        shard64_gzip         98668      4.0 GB          67       117.32s\n",
      "tfrecord        shard64_none         98668      4.1 GB          67       57.13s\n",
      "webdataset      shard1024_none       98668      4.4 GB          7        49.22s\n",
      "webdataset      shard1024_zstd       98668      4.4 GB          7        54.18s\n",
      "webdataset      shard256_none        98668      4.4 GB          18       42.06s\n",
      "webdataset      shard256_zstd        98668      4.4 GB          18       42.64s\n",
      "webdataset      shard64_none         98668      4.4 GB          67       54.53s\n",
      "webdataset      shard64_zstd         98668      4.4 GB          67       41.91s\n",
      "\n",
      "\n",
      "Compression Analysis:\n",
      "\n",
      "\n",
      "LMDB:\n",
      "  compress_lz4              19.1 GB         (+0.0% vs baseline)\n",
      "  compress_none             19.1 GB         (+0.0% vs baseline)\n",
      "  compress_zstd             19.1 GB         (+0.0% vs baseline)\n",
      "\n",
      "TFRECORD:\n",
      "  shard1024_gzip            4.0 GB          (+0.0% vs baseline)\n",
      "  shard1024_none            4.1 GB          (-1.0% vs baseline)\n",
      "  shard256_gzip             4.0 GB          (+0.0% vs baseline)\n",
      "  shard256_none             4.1 GB          (-1.0% vs baseline)\n",
      "  shard64_gzip              4.0 GB          (-0.0% vs baseline)\n",
      "  shard64_none              4.1 GB          (-1.0% vs baseline)\n",
      "\n",
      "WEBDATASET:\n",
      "  shard1024_none            4.4 GB          (+0.0% vs baseline)\n",
      "  shard1024_zstd            4.4 GB          (+0.0% vs baseline)\n",
      "  shard256_none             4.4 GB          (-0.0% vs baseline)\n",
      "  shard256_zstd             4.4 GB          (-0.0% vs baseline)\n",
      "  shard64_none              4.4 GB          (-0.0% vs baseline)\n",
      "  shard64_zstd              4.4 GB          (-0.0% vs baseline)\n"
     ]
    }
   ],
   "source": [
    "if not results['builds'].empty:\n",
    "    print(\"\\n\" + \"=\"*80)\n",
    "    print(\"BUILD STATISTICS SUMMARY\")\n",
    "    print(\"=\"*80)\n",
    "    \n",
    "    builds_df = results['builds']\n",
    "    \n",
    "    # Group by format and variant\n",
    "    summary = builds_df.groupby(['format', 'variant']).agg({\n",
    "        'items': 'sum',\n",
    "        'bytes_on_disk': 'sum',\n",
    "        'num_files': 'sum',\n",
    "        'build_wall_s': 'mean',\n",
    "    }).reset_index()\n",
    "    \n",
    "    print(\"\\nDisk Usage by Format:\\n\")\n",
    "    print(f\"{'Format':<15} {'Variant':<20} {'Items':<10} {'Size':<15} {'Files':<8} {'Build Time':<12}\")\n",
    "    print(\"-\" * 90)\n",
    "    \n",
    "    for _, row in summary.iterrows():\n",
    "        print(f\"{row['format']:<15} {row['variant']:<20} \"\n",
    "              f\"{int(row['items']):<10} {format_bytes(row['bytes_on_disk']):<15} \"\n",
    "              f\"{int(row['num_files']):<8} {row['build_wall_s']:.2f}s\")\n",
    "    \n",
    "    # Compression ratios\n",
    "    print(\"\\n\\nCompression Analysis:\\n\")\n",
    "    \n",
    "    # Compare compressed vs uncompressed variants\n",
    "    for format_name in summary['format'].unique():\n",
    "        format_data = summary[summary['format'] == format_name]\n",
    "        \n",
    "        if len(format_data) > 1:\n",
    "            print(f\"\\n{format_name.upper()}:\")\n",
    "            baseline = format_data.iloc[0]\n",
    "            \n",
    "            for _, variant in format_data.iterrows():\n",
    "                ratio = variant['bytes_on_disk'] / baseline['bytes_on_disk']\n",
    "                savings = (1 - ratio) * 100\n",
    "                print(f\"  {variant['variant']:<25} {format_bytes(variant['bytes_on_disk']):<15} \"\n",
    "                      f\"({savings:+.1f}% vs baseline)\")\n",
    "else:\n",
    "    print(\"\\nâš  No build statistics available\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Training Performance Summary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "================================================================================\n",
      "TRAINING PERFORMANCE SUMMARY\n",
      "================================================================================\n",
      "\n",
      "Final Epoch Performance:\n",
      "\n",
      "Format          Variant              Train Acc    Val Acc      Throughput     \n",
      "--------------------------------------------------------------------------------\n",
      "webdataset      shard256_none             61.74%      43.45%         20.5 samp/s\n",
      "csv             default                   59.87%      61.41%         21.3 samp/s\n",
      "tfrecord        shard256_none             59.04%      58.42%         21.2 samp/s\n",
      "lmdb            compress_none             59.36%      59.79%         21.5 samp/s\n",
      "\n",
      "\n",
      "Throughput Ranking:\n",
      "\n",
      "Rank   Format          Variant              Throughput      vs Baseline    \n",
      "--------------------------------------------------------------------------------\n",
      "1      lmdb            compress_none                21.5 samp/s         1.05x\n",
      "2      csv             default                      21.3 samp/s         1.04x\n",
      "3      tfrecord        shard256_none                21.2 samp/s         1.04x\n",
      "4      webdataset      shard256_none                20.5 samp/s         1.00x\n",
      "\n",
      "\n",
      "Resource Utilization:\n",
      "\n",
      "Format          Variant              GPU %      CPU %      Disk R       Disk W      \n",
      "-------------------------------------------------------------------------------------\n",
      "webdataset      shard256_none             nan%    792.8%      0.13 MB/s      0.00 MB/s\n",
      "csv             default                   nan%    792.5%      0.05 MB/s      0.00 MB/s\n",
      "tfrecord        shard256_none             nan%    795.2%      0.05 MB/s      0.00 MB/s\n",
      "lmdb            compress_none             nan%    795.2%      0.00 MB/s      0.00 MB/s\n"
     ]
    }
   ],
   "source": [
    "if not results['train_baselines'].empty:\n",
    "    print(\"\\n\" + \"=\"*80)\n",
    "    print(\"TRAINING PERFORMANCE SUMMARY\")\n",
    "    print(\"=\"*80)\n",
    "    \n",
    "    train_df = results['train_baselines']\n",
    "    \n",
    "    # Get final epoch results\n",
    "    final_epoch = train_df.groupby(['format', 'variant'])['epoch'].max().reset_index()\n",
    "    final_results = train_df.merge(final_epoch, on=['format', 'variant', 'epoch'])\n",
    "    \n",
    "    print(\"\\nFinal Epoch Performance:\\n\")\n",
    "    print(f\"{'Format':<15} {'Variant':<20} {'Train Acc':<12} {'Val Acc':<12} {'Throughput':<15}\")\n",
    "    print(\"-\" * 80)\n",
    "    \n",
    "    for _, row in final_results.iterrows():\n",
    "        print(f\"{row['format']:<15} {row['variant']:<20} \"\n",
    "              f\"{row['train_acc']:>10.2f}% {row['val_acc']:>10.2f}% \"\n",
    "              f\"{row['train_samples_per_sec']:>12.1f} samp/s\")\n",
    "    \n",
    "    # Throughput comparison\n",
    "    print(\"\\n\\nThroughput Ranking:\\n\")\n",
    "    ranked = final_results.sort_values('train_samples_per_sec', ascending=False)\n",
    "    \n",
    "    baseline_throughput = ranked.iloc[-1]['train_samples_per_sec']\n",
    "    \n",
    "    print(f\"{'Rank':<6} {'Format':<15} {'Variant':<20} {'Throughput':<15} {'vs Baseline':<15}\")\n",
    "    print(\"-\" * 80)\n",
    "    \n",
    "    for rank, (_, row) in enumerate(ranked.iterrows(), 1):\n",
    "        speedup = row['train_samples_per_sec'] / baseline_throughput\n",
    "        print(f\"{rank:<6} {row['format']:<15} {row['variant']:<20} \"\n",
    "              f\"{row['train_samples_per_sec']:>12.1f} samp/s {speedup:>12.2f}x\")\n",
    "    \n",
    "    # Resource utilization\n",
    "    print(\"\\n\\nResource Utilization:\\n\")\n",
    "    print(f\"{'Format':<15} {'Variant':<20} {'GPU %':<10} {'CPU %':<10} {'Disk R':<12} {'Disk W':<12}\")\n",
    "    print(\"-\" * 85)\n",
    "    \n",
    "    for _, row in final_results.iterrows():\n",
    "        gpu = row.get('gpu_util_mean', 0) or 0\n",
    "        cpu = row.get('cpu_util_mean', 0) or 0\n",
    "        disk_r = row.get('disk_read_mb_s_mean', 0) or 0\n",
    "        disk_w = row.get('disk_write_mb_s_mean', 0) or 0\n",
    "        \n",
    "        print(f\"{row['format']:<15} {row['variant']:<20} \"\n",
    "              f\"{gpu:>8.1f}% {cpu:>8.1f}% \"\n",
    "              f\"{disk_r:>9.2f} MB/s {disk_w:>9.2f} MB/s\")\n",
    "else:\n",
    "    print(\"\\nâš  No training baseline results available\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Scaling Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "âš  No scaling results available\n"
     ]
    }
   ],
   "source": [
    "if not results['train_scaling'].empty:\n",
    "    print(\"\\n\" + \"=\"*80)\n",
    "    print(\"SCALING ANALYSIS\")\n",
    "    print(\"=\"*80)\n",
    "    \n",
    "    scaling_df = results['train_scaling']\n",
    "    \n",
    "    # Batch size scaling\n",
    "    print(\"\\nBatch Size Scaling:\\n\")\n",
    "    \n",
    "    for format_name in scaling_df['format'].unique():\n",
    "        format_data = scaling_df[\n",
    "            (scaling_df['format'] == format_name) & \n",
    "            (scaling_df['num_workers'] == 4)  # Fixed workers\n",
    "        ].sort_values('batch_size')\n",
    "        \n",
    "        if not format_data.empty:\n",
    "            print(f\"\\n{format_name.upper()}:\")\n",
    "            print(f\"  {'Batch Size':<12} {'Throughput':<20} {'GPU Util %':<12}\")\n",
    "            print(\"  \" + \"-\" * 50)\n",
    "            \n",
    "            for _, row in format_data.iterrows():\n",
    "                gpu = row.get('gpu_util_mean', 0) or 0\n",
    "                print(f\"  {row['batch_size']:<12} {row['samples_per_sec']:>17.1f} samp/s {gpu:>10.1f}%\")\n",
    "    \n",
    "    # Worker scaling\n",
    "    print(\"\\n\\nWorker Scaling:\\n\")\n",
    "    \n",
    "    for format_name in scaling_df['format'].unique():\n",
    "        format_data = scaling_df[\n",
    "            (scaling_df['format'] == format_name) & \n",
    "            (scaling_df['batch_size'] == 64)  # Fixed batch size\n",
    "        ].sort_values('num_workers')\n",
    "        \n",
    "        if not format_data.empty:\n",
    "            print(f\"\\n{format_name.upper()}:\")\n",
    "            print(f\"  {'Workers':<10} {'Throughput':<20} {'CPU Util %':<12}\")\n",
    "            print(\"  \" + \"-\" * 50)\n",
    "            \n",
    "            for _, row in format_data.iterrows():\n",
    "                cpu = row.get('cpu_util_mean', 0) or 0\n",
    "                print(f\"  {row['num_workers']:<10} {row['samples_per_sec']:>17.1f} samp/s {cpu:>10.1f}%\")\n",
    "    \n",
    "    # Best configurations\n",
    "    print(\"\\n\\nOptimal Configurations:\\n\")\n",
    "    print(f\"{'Format':<15} {'Batch Size':<12} {'Workers':<10} {'Throughput':<20}\")\n",
    "    print(\"-\" * 65)\n",
    "    \n",
    "    for format_name in scaling_df['format'].unique():\n",
    "        format_data = scaling_df[scaling_df['format'] == format_name]\n",
    "        best = format_data.loc[format_data['samples_per_sec'].idxmax()]\n",
    "        print(f\"{best['format']:<15} {best['batch_size']:<12} \"\n",
    "              f\"{best['num_workers']:<10} {best['samples_per_sec']:>17.1f} samp/s\")\n",
    "else:\n",
    "    print(\"\\nâš  No scaling results available\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Key Findings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "================================================================================\n",
      "KEY FINDINGS\n",
      "================================================================================\n",
      "\n",
      "ðŸ“Š DISK USAGE:\n",
      "  â€¢ Most compact format: CSV (35.4 MB)\n",
      "  â€¢ Largest format: LMDB (57.2 GB)\n",
      "  â€¢ Size difference: 1653.86x\n",
      "\n",
      "âš¡ TRAINING THROUGHPUT:\n",
      "  â€¢ Fastest format: LMDB (21.5 samples/s)\n",
      "  â€¢ Slowest format: WEBDATASET (20.5 samples/s)\n",
      "  â€¢ Performance difference: 1.05x\n",
      "\n",
      "ðŸŽ® GPU UTILIZATION:\n",
      "  â€¢ No GPU detected (CPU-only training)\n",
      "\n",
      "================================================================================\n"
     ]
    }
   ],
   "source": [
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"KEY FINDINGS\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "findings = []\n",
    "\n",
    "# Disk usage findings\n",
    "if not results['builds'].empty:\n",
    "    builds_df = results['builds']\n",
    "    total_by_format = builds_df.groupby('format')['bytes_on_disk'].sum()\n",
    "    most_compact = total_by_format.idxmin()\n",
    "    least_compact = total_by_format.idxmax()\n",
    "    \n",
    "    findings.append(f\"\\nðŸ“Š DISK USAGE:\")\n",
    "    findings.append(f\"  â€¢ Most compact format: {most_compact.upper()} ({format_bytes(total_by_format[most_compact])})\")\n",
    "    findings.append(f\"  â€¢ Largest format: {least_compact.upper()} ({format_bytes(total_by_format[least_compact])})\")\n",
    "    \n",
    "    ratio = total_by_format[least_compact] / total_by_format[most_compact]\n",
    "    findings.append(f\"  â€¢ Size difference: {ratio:.2f}x\")\n",
    "\n",
    "# Training performance findings\n",
    "if not results['train_baselines'].empty:\n",
    "    train_df = results['train_baselines']\n",
    "    final_epoch = train_df.groupby(['format', 'variant'])['epoch'].max().reset_index()\n",
    "    final_results = train_df.merge(final_epoch, on=['format', 'variant', 'epoch'])\n",
    "    \n",
    "    fastest = final_results.loc[final_results['train_samples_per_sec'].idxmax()]\n",
    "    slowest = final_results.loc[final_results['train_samples_per_sec'].idxmin()]\n",
    "    \n",
    "    findings.append(f\"\\nâš¡ TRAINING THROUGHPUT:\")\n",
    "    findings.append(f\"  â€¢ Fastest format: {fastest['format'].upper()} ({fastest['train_samples_per_sec']:.1f} samples/s)\")\n",
    "    findings.append(f\"  â€¢ Slowest format: {slowest['format'].upper()} ({slowest['train_samples_per_sec']:.1f} samples/s)\")\n",
    "    \n",
    "    speedup = fastest['train_samples_per_sec'] / slowest['train_samples_per_sec']\n",
    "    findings.append(f\"  â€¢ Performance difference: {speedup:.2f}x\")\n",
    "    \n",
    "    # GPU utilization\n",
    "    if 'gpu_util_mean' in final_results.columns:\n",
    "        # Check if there are any non-NaN GPU values\n",
    "        gpu_values = final_results['gpu_util_mean'].dropna()\n",
    "        if len(gpu_values) > 0:\n",
    "            avg_gpu = gpu_values.mean()\n",
    "            findings.append(f\"\\nðŸŽ® GPU UTILIZATION:\")\n",
    "            findings.append(f\"  â€¢ Average GPU utilization: {avg_gpu:.1f}%\")\n",
    "            \n",
    "            best_gpu = final_results.loc[final_results['gpu_util_mean'].idxmax()]\n",
    "            findings.append(f\"  â€¢ Best GPU utilization: {best_gpu['format'].upper()} ({best_gpu['gpu_util_mean']:.1f}%)\")\n",
    "        else:\n",
    "            findings.append(f\"\\nðŸŽ® GPU UTILIZATION:\")\n",
    "            findings.append(f\"  â€¢ No GPU detected (CPU-only training)\")\n",
    "\n",
    "# Scaling findings\n",
    "if not results['train_scaling'].empty:\n",
    "    scaling_df = results['train_scaling']\n",
    "    \n",
    "    findings.append(f\"\\nðŸ“ˆ SCALING CHARACTERISTICS:\")\n",
    "    \n",
    "    # Best batch size scaling\n",
    "    for format_name in scaling_df['format'].unique():\n",
    "        format_data = scaling_df[\n",
    "            (scaling_df['format'] == format_name) & \n",
    "            (scaling_df['num_workers'] == 4)\n",
    "        ].sort_values('batch_size')\n",
    "        \n",
    "        if len(format_data) >= 2:\n",
    "            first = format_data.iloc[0]\n",
    "            last = format_data.iloc[-1]\n",
    "            scaling_factor = last['samples_per_sec'] / first['samples_per_sec']\n",
    "            findings.append(f\"  â€¢ {format_name.upper()} batch scaling: {scaling_factor:.2f}x improvement\")\n",
    "\n",
    "# Print all findings\n",
    "for finding in findings:\n",
    "    print(finding)\n",
    "\n",
    "print(\"\\n\" + \"=\"*80)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Export Summary Report"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "âœ“ Exported builds summary to C:\\Users\\arjya\\Fall 2025\\Systems for ML\\Project 1\\SML\\format-matters\\reports\\builds_summary.csv\n",
      "âœ“ Exported training summary to C:\\Users\\arjya\\Fall 2025\\Systems for ML\\Project 1\\SML\\format-matters\\reports\\training_summary.csv\n",
      "\n",
      "âœ“ All summaries exported to C:\\Users\\arjya\\Fall 2025\\Systems for ML\\Project 1\\SML\\format-matters\\reports\n"
     ]
    }
   ],
   "source": [
    "# Create summary report directory\n",
    "REPORT_DIR = BASE_DIR / 'reports'\n",
    "REPORT_DIR.mkdir(exist_ok=True)\n",
    "\n",
    "# Export summary tables\n",
    "if not results['builds'].empty:\n",
    "    builds_summary = results['builds'].groupby(['format', 'variant']).agg({\n",
    "        'items': 'sum',\n",
    "        'bytes_on_disk': 'sum',\n",
    "        'num_files': 'sum',\n",
    "        'build_wall_s': 'mean',\n",
    "    }).reset_index()\n",
    "    builds_summary.to_csv(REPORT_DIR / 'builds_summary.csv', index=False)\n",
    "    print(f\"âœ“ Exported builds summary to {REPORT_DIR / 'builds_summary.csv'}\")\n",
    "\n",
    "if not results['train_baselines'].empty:\n",
    "    final_epoch = results['train_baselines'].groupby(['format', 'variant'])['epoch'].max().reset_index()\n",
    "    train_summary = results['train_baselines'].merge(final_epoch, on=['format', 'variant', 'epoch'])\n",
    "    train_summary.to_csv(REPORT_DIR / 'training_summary.csv', index=False)\n",
    "    print(f\"âœ“ Exported training summary to {REPORT_DIR / 'training_summary.csv'}\")\n",
    "\n",
    "if not results['train_scaling'].empty:\n",
    "    results['train_scaling'].to_csv(REPORT_DIR / 'scaling_summary.csv', index=False)\n",
    "    print(f\"âœ“ Exported scaling summary to {REPORT_DIR / 'scaling_summary.csv'}\")\n",
    "\n",
    "print(f\"\\nâœ“ All summaries exported to {REPORT_DIR}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## âœ… Analysis Summary Complete\n",
    "\n",
    "**What was analyzed:**\n",
    "- Build statistics (disk usage, compression ratios)\n",
    "- Training performance (throughput, accuracy)\n",
    "- Resource utilization (GPU, CPU, disk I/O)\n",
    "- Scaling characteristics (batch size, workers)\n",
    "- Optimal configurations per format\n",
    "\n",
    "**Key Outputs:**\n",
    "- Comprehensive summary tables\n",
    "- Performance rankings\n",
    "- Key findings and insights\n",
    "- Exported CSV reports\n",
    "\n",
    "**Next steps:**\n",
    "1. Create visualizations (31_analysis_plots.ipynb)\n",
    "2. Generate decision guide (40_decision_guide.ipynb)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
