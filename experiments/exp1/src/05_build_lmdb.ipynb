{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 05 - Build LMDB Format\n",
    "\n",
    "This notebook creates LMDB (Lightning Memory-Mapped Database) format for efficient data loading.\n",
    "\n",
    "**Format:** LMDB key-value store\n",
    "- Memory-mapped database for fast random access\n",
    "- Single database file per split\n",
    "- Excellent for random access patterns\n",
    "- Optional compression (zstd, lz4)\n",
    "- Used by many computer vision frameworks\n",
    "\n",
    "**Variants:**\n",
    "- Compression: none, zstd, lz4\n",
    "\n",
    "**Output:**\n",
    "- `data/built/<dataset>/lmdb/<variant>/<split>.lmdb/`\n",
    "- Build statistics logged to `runs/<session>/summary.csv`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# pip install zstandard"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# pip install lz4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✓ Common utilities loaded successfully\n",
      "\n",
      "Available functions:\n",
      "  - set_seed(seed)\n",
      "  - get_transforms(augment)\n",
      "  - write_sysinfo(path)\n",
      "  - time_first_batch(dataloader, device)\n",
      "  - start_monitor(log_path, interval)\n",
      "  - stop_monitor(thread, stop_event)\n",
      "  - append_to_summary(path, row_dict)\n",
      "  - compute_metrics_from_logs(log_path)\n",
      "  - get_device()\n",
      "  - format_bytes(bytes)\n",
      "  - count_parameters(model)\n",
      "\n",
      "Constants:\n",
      "  - STANDARD_TRANSFORM\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import sys\n",
    "import time\n",
    "import json\n",
    "import pickle\n",
    "from pathlib import Path\n",
    "from collections import defaultdict\n",
    "\n",
    "import lmdb\n",
    "from PIL import Image\n",
    "from tqdm.auto import tqdm\n",
    "import numpy as np\n",
    "\n",
    "# Optional compression libraries\n",
    "try:\n",
    "    import zstandard as zstd\n",
    "    HAS_ZSTD = True\n",
    "except ImportError:\n",
    "    HAS_ZSTD = False\n",
    "    print(\"⚠ zstandard not available, zstd compression will be skipped\")\n",
    "\n",
    "try:\n",
    "    import lz4.frame\n",
    "    HAS_LZ4 = True\n",
    "except ImportError:\n",
    "    HAS_LZ4 = False\n",
    "    print(\"⚠ lz4 not available, lz4 compression will be skipped\")\n",
    "\n",
    "# Load common utilities\n",
    "%run ./10_common_utils.ipynb"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Configuration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Environment: Local\n",
      "Base directory: C:\\Users\\arjya\\Fall 2025\\Systems for ML\\Project 1\\SML\\format-matters\n",
      "Run directory: C:\\Users\\arjya\\Fall 2025\\Systems for ML\\Project 1\\SML\\format-matters\\runs\\20251127-141322\\builds\n",
      "Summary log: C:\\Users\\arjya\\Fall 2025\\Systems for ML\\Project 1\\SML\\format-matters\\runs\\20251127-141322\\builds\\summary.csv\n"
     ]
    }
   ],
   "source": [
    "# Detect environment\n",
    "IS_KAGGLE = \"KAGGLE_KERNEL_RUN_TYPE\" in os.environ\n",
    "BASE_DIR = Path('/kaggle/working/format-matters') if IS_KAGGLE else Path('..').resolve()\n",
    "\n",
    "RAW_DIR = BASE_DIR / 'data/raw'\n",
    "BUILT_DIR = BASE_DIR / 'data/built'\n",
    "\n",
    "# Create run directory for this session\n",
    "RUN_DIR = BASE_DIR / 'runs' / time.strftime('%Y%m%d-%H%M%S') / 'builds'\n",
    "RUN_DIR.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "SUMMARY_CSV = RUN_DIR / 'summary.csv'\n",
    "SUMMARY_CSV.touch(exist_ok=True)\n",
    "\n",
    "print(f\"Environment: {'Kaggle' if IS_KAGGLE else 'Local'}\")\n",
    "print(f\"Base directory: {BASE_DIR}\")\n",
    "print(f\"Run directory: {RUN_DIR}\")\n",
    "print(f\"Summary log: {SUMMARY_CSV}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Build Configuration\n",
    "\n",
    "Configure which variants to build:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Will build 3 variants:\n",
      "  - compress_none\n",
      "  - compress_zstd\n",
      "  - compress_lz4\n"
     ]
    }
   ],
   "source": [
    "# Compression options (only include available ones)\n",
    "COMPRESSIONS = ['none']\n",
    "if HAS_ZSTD:\n",
    "    COMPRESSIONS.append('zstd')\n",
    "if HAS_LZ4:\n",
    "    COMPRESSIONS.append('lz4')\n",
    "\n",
    "# Generate all variants\n",
    "VARIANTS = []\n",
    "for compression in COMPRESSIONS:\n",
    "    VARIANTS.append({\n",
    "        'compression': compression,\n",
    "        'name': f\"compress_{compression}\"\n",
    "    })\n",
    "\n",
    "print(f\"Will build {len(VARIANTS)} variants:\")\n",
    "for v in VARIANTS:\n",
    "    print(f\"  - {v['name']}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## LMDB Helper Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compress_data(data: bytes, compression: str) -> bytes:\n",
    "    \"\"\"\n",
    "    Compress data using specified compression algorithm.\n",
    "    \n",
    "    Args:\n",
    "        data: Raw bytes to compress\n",
    "        compression: Compression type ('none', 'zstd', 'lz4')\n",
    "    \n",
    "    Returns:\n",
    "        Compressed bytes (or original if compression='none')\n",
    "    \"\"\"\n",
    "    if compression == 'none':\n",
    "        return data\n",
    "    elif compression == 'zstd' and HAS_ZSTD:\n",
    "        compressor = zstd.ZstdCompressor(level=3)\n",
    "        return compressor.compress(data)\n",
    "    elif compression == 'lz4' and HAS_LZ4:\n",
    "        return lz4.frame.compress(data)\n",
    "    else:\n",
    "        raise ValueError(f\"Unsupported compression: {compression}\")\n",
    "\n",
    "\n",
    "def create_lmdb_entry(image_bytes: bytes, label: int, compression: str) -> bytes:\n",
    "    \"\"\"\n",
    "    Create an LMDB entry from image bytes and label.\n",
    "    \n",
    "    Args:\n",
    "        image_bytes: Raw image bytes (JPEG/PNG encoded)\n",
    "        label: Integer label\n",
    "        compression: Compression type\n",
    "    \n",
    "    Returns:\n",
    "        Serialized entry bytes\n",
    "    \"\"\"\n",
    "    # Compress image if requested\n",
    "    compressed_image = compress_data(image_bytes, compression)\n",
    "    \n",
    "    # Create entry dict\n",
    "    entry = {\n",
    "        'image': compressed_image,\n",
    "        'label': label,\n",
    "        'compression': compression,\n",
    "    }\n",
    "    \n",
    "    # Serialize with pickle\n",
    "    return pickle.dumps(entry)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## LMDB Builder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_lmdb_split(\n",
    "    dataset_name: str,\n",
    "    split: str,\n",
    "    raw_path: Path,\n",
    "    output_path: Path,\n",
    "    compression: str,\n",
    "    class_to_label: dict,\n",
    "    image_extensions: list\n",
    "):\n",
    "    \"\"\"\n",
    "    Build LMDB database for a single split.\n",
    "    \n",
    "    Args:\n",
    "        dataset_name: Name of dataset\n",
    "        split: Split name ('train' or 'val')\n",
    "        raw_path: Path to raw dataset split directory\n",
    "        output_path: Path to output directory\n",
    "        compression: Compression type ('none', 'zstd', 'lz4')\n",
    "        class_to_label: Mapping from class name to label index\n",
    "        image_extensions: List of image file extensions to include\n",
    "    \n",
    "    Returns:\n",
    "        Dictionary with build statistics\n",
    "    \"\"\"\n",
    "    output_path.mkdir(parents=True, exist_ok=True)\n",
    "    \n",
    "    # Find all image files\n",
    "    image_files = []\n",
    "    for ext in image_extensions:\n",
    "        image_files.extend(raw_path.rglob(f'*{ext}'))\n",
    "    \n",
    "    # Sort for reproducibility\n",
    "    image_files = sorted(image_files)\n",
    "    split_class_count = len({f.parent.name for f in image_files})\n",
    "    print(f\"    Found {len(image_files):,} images in {split_class_count} classes\")\n",
    "    \n",
    "    # Create LMDB database\n",
    "    lmdb_path = output_path / f\"{split}.lmdb\"\n",
    "    \n",
    "    # Estimate map size (generous estimate: 2x raw data size)\n",
    "    estimated_size = len(image_files) * 100 * 1024  # 100KB per image estimate\n",
    "    map_size = max(estimated_size * 2, 1024 * 1024 * 1024)  # At least 1GB\n",
    "    \n",
    "    # Open LMDB environment\n",
    "    env = lmdb.open(\n",
    "        str(lmdb_path),\n",
    "        map_size=map_size,\n",
    "        writemap=True,\n",
    "        map_async=True,\n",
    "        max_dbs=0\n",
    "    )\n",
    "    \n",
    "    # Write data\n",
    "    with env.begin(write=True) as txn:\n",
    "        for idx, img_path in enumerate(tqdm(image_files, desc=f\"    Writing {split}\")):\n",
    "            # Read image bytes\n",
    "            with open(img_path, 'rb') as f:\n",
    "                img_bytes = f.read()\n",
    "            \n",
    "            # Get label\n",
    "            class_name = img_path.parent.name\n",
    "            if class_name not in class_to_label:\n",
    "                raise KeyError(f\"Class {class_name} missing from mapping for split {split}\")\n",
    "            label = class_to_label[class_name]\n",
    "            \n",
    "            # Create entry\n",
    "            entry_bytes = create_lmdb_entry(img_bytes, label, compression)\n",
    "            \n",
    "            # Write to LMDB with index as key\n",
    "            key = f\"{idx:08d}\".encode('utf-8')\n",
    "            txn.put(key, entry_bytes)\n",
    "        \n",
    "        # Store metadata\n",
    "        metadata = {\n",
    "            'num_samples': len(image_files),\n",
    "            'num_classes': len(class_to_label),\n",
    "            'class_names': sorted(class_to_label, key=class_to_label.get),\n",
    "            'compression': compression,\n",
    "        }\n",
    "        txn.put(b'__metadata__', pickle.dumps(metadata))\n",
    "    \n",
    "    # Close environment\n",
    "    env.close()\n",
    "    \n",
    "    # Calculate statistics\n",
    "    lmdb_size = sum(f.stat().st_size for f in lmdb_path.iterdir())\n",
    "    \n",
    "    return {\n",
    "        'items': len(image_files),\n",
    "        'bytes_on_disk': lmdb_size,\n",
    "        'num_files': 1,  # LMDB is a single database (directory)\n",
    "        'avg_file_size': lmdb_size,\n",
    "    }\n",
    "\n",
    "\n",
    "def build_lmdb(\n",
    "    dataset_name: str,\n",
    "    raw_path: Path,\n",
    "    output_path: Path,\n",
    "    compression: str,\n",
    "    variant_name: str\n",
    "):\n",
    "    \"\"\"\n",
    "    Build LMDB for a dataset with specific configuration.\n",
    "    \n",
    "    Args:\n",
    "        dataset_name: Name of dataset\n",
    "        raw_path: Path to raw dataset directory\n",
    "        output_path: Path to output directory\n",
    "        compression: Compression type ('none', 'zstd', 'lz4')\n",
    "        variant_name: Variant identifier\n",
    "    \n",
    "    Returns:\n",
    "        Dictionary with build statistics\n",
    "    \"\"\"\n",
    "    print(f\"\\nBuilding LMDB for {dataset_name} ({variant_name})...\")\n",
    "    print(f\"  Source: {raw_path}\")\n",
    "    print(f\"  Output: {output_path}\")\n",
    "    print(f\"  Compression: {compression}\")\n",
    "    \n",
    "    start_time = time.time()\n",
    "    # FIXED: Use case-insensitive matching\n",
    "    image_extensions = {'.jpg', '.jpeg', '.png'}\n",
    "    all_class_names = set()\n",
    "    for split in ['train', 'val']:\n",
    "        split_dir = raw_path / split\n",
    "        if not split_dir.exists():\n",
    "            continue\n",
    "        for ext in image_extensions:\n",
    "            for img_path in split_dir.rglob(f'*{ext}'):\n",
    "                all_class_names.add(img_path.parent.name)\n",
    "    class_names = sorted(all_class_names)\n",
    "    class_to_label = {name: idx for idx, name in enumerate(class_names)}\n",
    "    print(f\"  Total classes across splits: {len(class_names)}\")\n",
    "    \n",
    "    # Process each split\n",
    "    for split in ['train', 'val']:\n",
    "        split_dir = raw_path / split\n",
    "        if not split_dir.exists():\n",
    "            print(f\"  ⚠ {split} split not found, skipping\")\n",
    "            continue\n",
    "        \n",
    "        print(f\"\\n  Processing {split} split...\")\n",
    "        \n",
    "        split_stats = build_lmdb_split(\n",
    "            dataset_name, split, split_dir, output_path,\n",
    "            compression, class_to_label, image_extensions\n",
    "        )\n",
    "        \n",
    "        print(f\"    ✓ {split}: {format_bytes(split_stats['bytes_on_disk'])}\")\n",
    "        \n",
    "        # Log to summary\n",
    "        build_time = time.time() - start_time\n",
    "        row = {\n",
    "            'stage': 'build',\n",
    "            'dataset': dataset_name,\n",
    "            'format': 'lmdb',\n",
    "            'variant': variant_name,\n",
    "            'split': split,\n",
    "            'items': split_stats['items'],\n",
    "            'bytes_on_disk': split_stats['bytes_on_disk'],\n",
    "            'num_files': split_stats['num_files'],\n",
    "            'avg_file_size': split_stats['avg_file_size'],\n",
    "            'build_wall_s': build_time,\n",
    "        }\n",
    "        append_to_summary(SUMMARY_CSV, row)\n",
    "    \n",
    "    build_time = time.time() - start_time\n",
    "    print(f\"\\n  ✓ Build completed in {build_time:.2f}s\")\n",
    "    \n",
    "    return {'dataset': dataset_name, 'variant': variant_name, 'build_time': build_time}\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Build LMDB for All Datasets and Variants"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 2 dataset(s): cifar10, imagenet-mini\n",
      "\n",
      "============================================================\n"
     ]
    }
   ],
   "source": [
    "# Find available datasets\n",
    "available_datasets = []\n",
    "for dataset_name in ['cifar10', 'imagenet-mini', 'tiny-imagenet-200']:\n",
    "    dataset_path = RAW_DIR / dataset_name\n",
    "    if dataset_path.exists() and (dataset_path / 'train').exists():\n",
    "        available_datasets.append(dataset_name)\n",
    "\n",
    "print(f\"Found {len(available_datasets)} dataset(s): {', '.join(available_datasets)}\")\n",
    "print(\"\\n\" + \"=\"*60)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Building LMDB for cifar10 (compress_none)...\n",
      "  Source: C:\\Users\\arjya\\Fall 2025\\Systems for ML\\Project 1\\SML\\format-matters\\data\\raw\\cifar10\n",
      "  Output: C:\\Users\\arjya\\Fall 2025\\Systems for ML\\Project 1\\SML\\format-matters\\data\\built\\cifar10\\lmdb\\compress_none\n",
      "  Compression: none\n",
      "  Total classes across splits: 10\n",
      "\n",
      "  Processing train split...\n",
      "    Found 50,000 images in 10 classes\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a1f6da5cbea5489581df380a5b82d4d2",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "    Writing train:   0%|          | 0/50000 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "    ✓ train: 9.5 GB\n",
      "\n",
      "  Processing val split...\n",
      "    Found 10,000 images in 10 classes\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "cbaf0ec8aaa44729981337243f036ccc",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "    Writing val:   0%|          | 0/10000 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "    ✓ val: 1.9 GB\n",
      "\n",
      "  ✓ Build completed in 39.96s\n",
      "============================================================\n",
      "\n",
      "Building LMDB for cifar10 (compress_zstd)...\n",
      "  Source: C:\\Users\\arjya\\Fall 2025\\Systems for ML\\Project 1\\SML\\format-matters\\data\\raw\\cifar10\n",
      "  Output: C:\\Users\\arjya\\Fall 2025\\Systems for ML\\Project 1\\SML\\format-matters\\data\\built\\cifar10\\lmdb\\compress_zstd\n",
      "  Compression: zstd\n",
      "  Total classes across splits: 10\n",
      "\n",
      "  Processing train split...\n",
      "    Found 50,000 images in 10 classes\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "47621a0aab024d82b39d16918389cc6f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "    Writing train:   0%|          | 0/50000 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "    ✓ train: 9.5 GB\n",
      "\n",
      "  Processing val split...\n",
      "    Found 10,000 images in 10 classes\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "0c6d6236a3f44f05ba86f06b34d57db3",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "    Writing val:   0%|          | 0/10000 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "    ✓ val: 1.9 GB\n",
      "\n",
      "  ✓ Build completed in 12.71s\n",
      "============================================================\n",
      "\n",
      "Building LMDB for cifar10 (compress_lz4)...\n",
      "  Source: C:\\Users\\arjya\\Fall 2025\\Systems for ML\\Project 1\\SML\\format-matters\\data\\raw\\cifar10\n",
      "  Output: C:\\Users\\arjya\\Fall 2025\\Systems for ML\\Project 1\\SML\\format-matters\\data\\built\\cifar10\\lmdb\\compress_lz4\n",
      "  Compression: lz4\n",
      "  Total classes across splits: 10\n",
      "\n",
      "  Processing train split...\n",
      "    Found 50,000 images in 10 classes\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "41e1c1a4b7674218bd9e21ba5b691090",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "    Writing train:   0%|          | 0/50000 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "    ✓ train: 9.5 GB\n",
      "\n",
      "  Processing val split...\n",
      "    Found 10,000 images in 10 classes\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b8ead6d90d214ddbac03d4e28ab4ef03",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "    Writing val:   0%|          | 0/10000 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "    ✓ val: 1.9 GB\n",
      "\n",
      "  ✓ Build completed in 10.98s\n",
      "============================================================\n",
      "\n",
      "Building LMDB for imagenet-mini (compress_none)...\n",
      "  Source: C:\\Users\\arjya\\Fall 2025\\Systems for ML\\Project 1\\SML\\format-matters\\data\\raw\\imagenet-mini\n",
      "  Output: C:\\Users\\arjya\\Fall 2025\\Systems for ML\\Project 1\\SML\\format-matters\\data\\built\\imagenet-mini\\lmdb\\compress_none\n",
      "  Compression: none\n",
      "  Total classes across splits: 1000\n",
      "\n",
      "  Processing train split...\n",
      "    Found 34,745 images in 1000 classes\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "256bf2a492b245d9bd943856e23e753b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "    Writing train:   0%|          | 0/34745 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "    ✓ train: 6.6 GB\n",
      "\n",
      "  Processing val split...\n",
      "    Found 3,923 images in 1000 classes\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "7ffe665b17c04bcfa19b3417acd6e99a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "    Writing val:   0%|          | 0/3923 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "    ✓ val: 1.0 GB\n",
      "\n",
      "  ✓ Build completed in 58.38s\n",
      "============================================================\n",
      "\n",
      "Building LMDB for imagenet-mini (compress_zstd)...\n",
      "  Source: C:\\Users\\arjya\\Fall 2025\\Systems for ML\\Project 1\\SML\\format-matters\\data\\raw\\imagenet-mini\n",
      "  Output: C:\\Users\\arjya\\Fall 2025\\Systems for ML\\Project 1\\SML\\format-matters\\data\\built\\imagenet-mini\\lmdb\\compress_zstd\n",
      "  Compression: zstd\n",
      "  Total classes across splits: 1000\n",
      "\n",
      "  Processing train split...\n",
      "    Found 34,745 images in 1000 classes\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "08bbb4c467d649bbb76378859dc14014",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "    Writing train:   0%|          | 0/34745 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "    ✓ train: 6.6 GB\n",
      "\n",
      "  Processing val split...\n",
      "    Found 3,923 images in 1000 classes\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e213e8693f554ad1a22974d4f37759c8",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "    Writing val:   0%|          | 0/3923 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "    ✓ val: 1.0 GB\n",
      "\n",
      "  ✓ Build completed in 72.92s\n",
      "============================================================\n",
      "\n",
      "Building LMDB for imagenet-mini (compress_lz4)...\n",
      "  Source: C:\\Users\\arjya\\Fall 2025\\Systems for ML\\Project 1\\SML\\format-matters\\data\\raw\\imagenet-mini\n",
      "  Output: C:\\Users\\arjya\\Fall 2025\\Systems for ML\\Project 1\\SML\\format-matters\\data\\built\\imagenet-mini\\lmdb\\compress_lz4\n",
      "  Compression: lz4\n",
      "  Total classes across splits: 1000\n",
      "\n",
      "  Processing train split...\n",
      "    Found 34,745 images in 1000 classes\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "4d7373c33a214c2f9c7d74d5ccc0ef2f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "    Writing train:   0%|          | 0/34745 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "    ✓ train: 6.6 GB\n",
      "\n",
      "  Processing val split...\n",
      "    Found 3,923 images in 1000 classes\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "3ece3957e27f42da9c99c36c89b264a2",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "    Writing val:   0%|          | 0/3923 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "    ✓ val: 1.0 GB\n",
      "\n",
      "  ✓ Build completed in 50.86s\n",
      "============================================================\n",
      "\n",
      "✓ Built 6 LMDB variant(s)\n"
     ]
    }
   ],
   "source": [
    "# Build all variants for all datasets\n",
    "build_results = []\n",
    "\n",
    "for dataset_name in available_datasets:\n",
    "    raw_path = RAW_DIR / dataset_name\n",
    "    \n",
    "    for variant in VARIANTS:\n",
    "        output_path = BUILT_DIR / dataset_name / 'lmdb' / variant['name']\n",
    "        \n",
    "        result = build_lmdb(\n",
    "            dataset_name=dataset_name,\n",
    "            raw_path=raw_path,\n",
    "            output_path=output_path,\n",
    "            compression=variant['compression'],\n",
    "            variant_name=variant['name']\n",
    "        )\n",
    "        build_results.append(result)\n",
    "        \n",
    "        print(\"=\"*60)\n",
    "\n",
    "print(f\"\\n✓ Built {len(build_results)} LMDB variant(s)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Verification"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Verifying LMDB databases...\n",
      "\n",
      "cifar10:\n",
      "  ✓ compress_none:\n",
      "      Train: 9.5 GB\n",
      "      Val: 1.9 GB\n",
      "  ✓ compress_zstd:\n",
      "      Train: 9.5 GB\n",
      "      Val: 1.9 GB\n",
      "  ✓ compress_lz4:\n",
      "      Train: 9.5 GB\n",
      "      Val: 1.9 GB\n",
      "\n",
      "imagenet-mini:\n",
      "  ✓ compress_none:\n",
      "      Train: 6.6 GB\n",
      "      Val: 1.0 GB\n",
      "  ✓ compress_zstd:\n",
      "      Train: 6.6 GB\n",
      "      Val: 1.0 GB\n",
      "  ✓ compress_lz4:\n",
      "      Train: 6.6 GB\n",
      "      Val: 1.0 GB\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(\"\\nVerifying LMDB databases...\\n\")\n",
    "\n",
    "for dataset_name in available_datasets:\n",
    "    print(f\"{dataset_name}:\")\n",
    "    \n",
    "    for variant in VARIANTS:\n",
    "        lmdb_dir = BUILT_DIR / dataset_name / 'lmdb' / variant['name']\n",
    "        \n",
    "        if not lmdb_dir.exists():\n",
    "            print(f\"  ✗ {variant['name']}: directory not found\")\n",
    "            continue\n",
    "        \n",
    "        # Check databases\n",
    "        train_db = lmdb_dir / 'train.lmdb'\n",
    "        val_db = lmdb_dir / 'val.lmdb'\n",
    "        \n",
    "        if not train_db.exists():\n",
    "            print(f\"  ✗ {variant['name']}: no train database found\")\n",
    "            continue\n",
    "        \n",
    "        # Calculate sizes\n",
    "        train_size = sum(f.stat().st_size for f in train_db.iterdir())\n",
    "        val_size = sum(f.stat().st_size for f in val_db.iterdir()) if val_db.exists() else 0\n",
    "        \n",
    "        print(f\"  ✓ {variant['name']}:\")\n",
    "        print(f\"      Train: {format_bytes(train_size)}\")\n",
    "        if val_db.exists():\n",
    "            print(f\"      Val: {format_bytes(val_size)}\")\n",
    "    \n",
    "    print()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Sample Data Inspection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Inspecting train database of cifar10 (compress_none):\n",
      "\n",
      "Database: train.lmdb\n",
      "Size: 9.5 GB\n",
      "\n",
      "Metadata:\n",
      "  Num samples: 50,000\n",
      "  Num classes: 10\n",
      "  Compression: none\n",
      "\n",
      "First 5 samples:\n",
      "\n",
      "Sample 0:\n",
      "  Image bytes: 2063\n",
      "  Label: 0\n",
      "  Compression: none\n",
      "\n",
      "Sample 1:\n",
      "  Image bytes: 2169\n",
      "  Label: 0\n",
      "  Compression: none\n",
      "\n",
      "Sample 2:\n",
      "  Image bytes: 2237\n",
      "  Label: 0\n",
      "  Compression: none\n",
      "\n",
      "Sample 3:\n",
      "  Image bytes: 2104\n",
      "  Label: 0\n",
      "  Compression: none\n",
      "\n",
      "Sample 4:\n",
      "  Image bytes: 2181\n",
      "  Label: 0\n",
      "  Compression: none\n"
     ]
    }
   ],
   "source": [
    "# Inspect first database of first dataset/variant\n",
    "if available_datasets and VARIANTS:\n",
    "    dataset_name = available_datasets[0]\n",
    "    variant = VARIANTS[0]\n",
    "    lmdb_dir = BUILT_DIR / dataset_name / 'lmdb' / variant['name']\n",
    "    \n",
    "    train_db = lmdb_dir / 'train.lmdb'\n",
    "    if train_db.exists():\n",
    "        print(f\"Inspecting train database of {dataset_name} ({variant['name']}):\\n\")\n",
    "        print(f\"Database: {train_db.name}\")\n",
    "        \n",
    "        train_size = sum(f.stat().st_size for f in train_db.iterdir())\n",
    "        print(f\"Size: {format_bytes(train_size)}\")\n",
    "        \n",
    "        # Open database and read metadata\n",
    "        env = lmdb.open(str(train_db), readonly=True, lock=False)\n",
    "        \n",
    "        with env.begin() as txn:\n",
    "            # Read metadata\n",
    "            metadata_bytes = txn.get(b'__metadata__')\n",
    "            if metadata_bytes:\n",
    "                metadata = pickle.loads(metadata_bytes)\n",
    "                print(f\"\\nMetadata:\")\n",
    "                print(f\"  Num samples: {metadata['num_samples']:,}\")\n",
    "                print(f\"  Num classes: {metadata['num_classes']}\")\n",
    "                print(f\"  Compression: {metadata['compression']}\")\n",
    "            \n",
    "            # Read first few samples\n",
    "            print(\"\\nFirst 5 samples:\")\n",
    "            for i in range(5):\n",
    "                key = f\"{i:08d}\".encode('utf-8')\n",
    "                entry_bytes = txn.get(key)\n",
    "                \n",
    "                if entry_bytes:\n",
    "                    entry = pickle.loads(entry_bytes)\n",
    "                    print(f\"\\nSample {i}:\")\n",
    "                    print(f\"  Image bytes: {len(entry['image'])}\")\n",
    "                    print(f\"  Label: {entry['label']}\")\n",
    "                    print(f\"  Compression: {entry['compression']}\")\n",
    "        \n",
    "        env.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Build Summary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Read and display summary\n",
    "# if SUMMARY_CSV.exists() and SUMMARY_CSV.stat().st_size > 0:\n",
    "#     import pandas as pd\n",
    "#     summary_df = pd.read_csv(SUMMARY_CSV)\n",
    "    \n",
    "#     print(\"\\nBuild Summary:\")\n",
    "#     print(\"=\"*80)\n",
    "    \n",
    "#     for dataset in summary_df['dataset'].unique():\n",
    "#         print(f\"\\n{dataset}:\")\n",
    "#         dataset_df = summary_df[summary_df['dataset'] == dataset]\n",
    "        \n",
    "#         for variant in dataset_df['variant'].unique():\n",
    "#             variant_df = dataset_df[dataset_df['variant'] == variant]\n",
    "#             print(f\"\\n  {variant}:\")\n",
    "            \n",
    "#             for _, row in variant_df.iterrows():\n",
    "#                 print(f\"    {row['split']}:\")\n",
    "#                 print(f\"      Items: {row['items']:,}\")\n",
    "#                 print(f\"      Size: {format_bytes(row['bytes_on_disk'])}\")\n",
    "    \n",
    "#     print(\"\\n\" + \"=\"*80)\n",
    "#     print(f\"\\nSummary saved to: {SUMMARY_CSV}\")\n",
    "# else:\n",
    "#     print(\"No summary data available\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "LMDB Build Summary:\n",
      "================================================================================\n",
      "\n",
      "cifar10:\n",
      "\n",
      "  compress_none:\n",
      "    train:\n",
      "      Items: 50,000\n",
      "      Size: 9.5 GB\n",
      "      Files: 1\n",
      "      Avg file: 9.5 GB\n",
      "      Build time: 34.01s\n",
      "    val:\n",
      "      Items: 10,000\n",
      "      Size: 1.9 GB\n",
      "      Files: 1\n",
      "      Avg file: 1.9 GB\n",
      "      Build time: 39.95s\n",
      "\n",
      "  compress_zstd:\n",
      "    train:\n",
      "      Items: 50,000\n",
      "      Size: 9.5 GB\n",
      "      Files: 1\n",
      "      Avg file: 9.5 GB\n",
      "      Build time: 10.74s\n",
      "    val:\n",
      "      Items: 10,000\n",
      "      Size: 1.9 GB\n",
      "      Files: 1\n",
      "      Avg file: 1.9 GB\n",
      "      Build time: 12.71s\n",
      "\n",
      "  compress_lz4:\n",
      "    train:\n",
      "      Items: 50,000\n",
      "      Size: 9.5 GB\n",
      "      Files: 1\n",
      "      Avg file: 9.5 GB\n",
      "      Build time: 9.27s\n",
      "    val:\n",
      "      Items: 10,000\n",
      "      Size: 1.9 GB\n",
      "      Files: 1\n",
      "      Avg file: 1.9 GB\n",
      "      Build time: 10.97s\n",
      "\n",
      "imagenet-mini:\n",
      "\n",
      "  compress_none:\n",
      "    train:\n",
      "      Items: 34,745\n",
      "      Size: 6.6 GB\n",
      "      Files: 1\n",
      "      Avg file: 6.6 GB\n",
      "      Build time: 53.34s\n",
      "    val:\n",
      "      Items: 3,923\n",
      "      Size: 1.0 GB\n",
      "      Files: 1\n",
      "      Avg file: 1.0 GB\n",
      "      Build time: 58.38s\n",
      "\n",
      "  compress_zstd:\n",
      "    train:\n",
      "      Items: 34,745\n",
      "      Size: 6.6 GB\n",
      "      Files: 1\n",
      "      Avg file: 6.6 GB\n",
      "      Build time: 66.26s\n",
      "    val:\n",
      "      Items: 3,923\n",
      "      Size: 1.0 GB\n",
      "      Files: 1\n",
      "      Avg file: 1.0 GB\n",
      "      Build time: 72.92s\n",
      "\n",
      "  compress_lz4:\n",
      "    train:\n",
      "      Items: 34,745\n",
      "      Size: 6.6 GB\n",
      "      Files: 1\n",
      "      Avg file: 6.6 GB\n",
      "      Build time: 45.65s\n",
      "    val:\n",
      "      Items: 3,923\n",
      "      Size: 1.0 GB\n",
      "      Files: 1\n",
      "      Avg file: 1.0 GB\n",
      "      Build time: 50.86s\n",
      "\n",
      "================================================================================\n",
      "\n",
      "Summary saved to: C:\\Users\\arjya\\Fall 2025\\Systems for ML\\Project 1\\SML\\format-matters\\runs\\20251127-141322\\builds\\summary.csv\n"
     ]
    }
   ],
   "source": [
    "# === Read and display LMDB build summary ===\n",
    "expected_cols = [\n",
    "    \"stage\", \"dataset\", \"format\", \"variant\", \"split\",\n",
    "    \"items\", \"bytes_on_disk\", \"num_files\", \"avg_file_size\",\n",
    "    \"build_wall_s\", \"timestamp\"\n",
    "]\n",
    "\n",
    "if SUMMARY_CSV.exists() and SUMMARY_CSV.stat().st_size > 0:\n",
    "    try:\n",
    "        summary_df = pd.read_csv(SUMMARY_CSV)\n",
    "        # If old file without headers, reload with defined column names\n",
    "        if not set(expected_cols).issubset(summary_df.columns):\n",
    "            summary_df = pd.read_csv(SUMMARY_CSV, names=expected_cols, header=None)\n",
    "            summary_df.to_csv(SUMMARY_CSV, index=False)\n",
    "    except pd.errors.ParserError:\n",
    "        summary_df = pd.read_csv(SUMMARY_CSV, names=expected_cols, header=None)\n",
    "        summary_df.to_csv(SUMMARY_CSV, index=False)\n",
    "else:\n",
    "    print(\"No summary data available\")\n",
    "    summary_df = None\n",
    "\n",
    "if summary_df is not None and not summary_df.empty:\n",
    "    # Filter only LMDB entries\n",
    "    lmdb_df = summary_df[summary_df['format'] == 'lmdb']\n",
    "    if lmdb_df.empty:\n",
    "        print(\"No LMDB builds found in summary.\")\n",
    "    else:\n",
    "        print(\"\\nLMDB Build Summary:\")\n",
    "        print(\"=\" * 80)\n",
    "\n",
    "        for dataset in lmdb_df['dataset'].unique():\n",
    "            print(f\"\\n{dataset}:\")\n",
    "            dataset_df = lmdb_df[lmdb_df['dataset'] == dataset]\n",
    "\n",
    "            for variant in dataset_df['variant'].unique():\n",
    "                variant_df = dataset_df[dataset_df['variant'] == variant]\n",
    "                print(f\"\\n  {variant}:\")\n",
    "\n",
    "                for _, row in variant_df.iterrows():\n",
    "                    print(f\"    {row['split']}:\")\n",
    "                    print(f\"      Items: {int(row['items']):,}\")\n",
    "                    print(f\"      Size: {format_bytes(row['bytes_on_disk'])}\")\n",
    "                    if 'num_files' in row and not pd.isna(row['num_files']):\n",
    "                        print(f\"      Files: {int(row['num_files'])}\")\n",
    "                    if 'avg_file_size' in row and not pd.isna(row['avg_file_size']):\n",
    "                        print(f\"      Avg file: {format_bytes(row['avg_file_size'])}\")\n",
    "                    if 'build_wall_s' in row and not pd.isna(row['build_wall_s']):\n",
    "                        print(f\"      Build time: {row['build_wall_s']:.2f}s\")\n",
    "\n",
    "        print(\"\\n\" + \"=\" * 80)\n",
    "        print(f\"\\nSummary saved to: {SUMMARY_CSV}\")\n",
    "else:\n",
    "    print(\"No summary data available\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ✅ LMDB Build Complete\n",
    "\n",
    "**What was created:**\n",
    "- LMDB databases (one per split)\n",
    "- Memory-mapped for fast random access\n",
    "- Multiple variants with different compression options\n",
    "- Excellent for random access patterns\n",
    "\n",
    "**Variants built:**\n",
    "- `compress_none`: No compression\n",
    "- `compress_zstd`: Zstandard compression (if available)\n",
    "- `compress_lz4`: LZ4 compression (if available)\n",
    "\n",
    "**Output locations:**\n",
    "- `data/built/<dataset>/lmdb/<variant>/<split>.lmdb/`\n",
    "\n",
    "**Next steps:**\n",
    "1. Run `14_loader_lmdb.ipynb` to create the dataloader\n",
    "2. Then run training experiments (20-21)\n",
    "3. Finally run analysis notebooks (30-31, 40)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
