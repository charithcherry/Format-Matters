{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 40 - Format Selection Decision Guide\n",
    "\n",
    "This notebook provides a comprehensive guide for choosing the right data format based on your specific requirements and constraints.\n",
    "\n",
    "**Decision Factors:**\n",
    "- Training throughput requirements\n",
    "- Disk space constraints\n",
    "- Access patterns (sequential vs random)\n",
    "- Deployment environment\n",
    "- Ecosystem compatibility\n",
    "\n",
    "**Output:**\n",
    "- Decision tree\n",
    "- Format recommendations\n",
    "- Trade-off analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✓ Common utilities loaded successfully\n",
      "\n",
      "Available functions:\n",
      "  - set_seed(seed)\n",
      "  - get_transforms(augment)\n",
      "  - write_sysinfo(path)\n",
      "  - time_first_batch(dataloader, device)\n",
      "  - start_monitor(log_path, interval)\n",
      "  - stop_monitor(thread, stop_event)\n",
      "  - append_to_summary(path, row_dict)\n",
      "  - compute_metrics_from_logs(log_path)\n",
      "  - get_device()\n",
      "  - format_bytes(bytes)\n",
      "  - count_parameters(model)\n",
      "\n",
      "Constants:\n",
      "  - STANDARD_TRANSFORM\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "from pathlib import Path\n",
    "import pandas as pd\n",
    "\n",
    "# Load common utilities\n",
    "%run ./10_common_utils.ipynb"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Configuration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Environment: Local\n",
      "Report directory: C:\\Users\\arjya\\Fall 2025\\Systems for ML\\Project 1\\SML\\format-matters\\reports\n"
     ]
    }
   ],
   "source": [
    "# Detect environment\n",
    "IS_KAGGLE = \"KAGGLE_KERNEL_RUN_TYPE\" in os.environ\n",
    "BASE_DIR = Path('/kaggle/working/format-matters') if IS_KAGGLE else Path('..').resolve()\n",
    "\n",
    "REPORT_DIR = BASE_DIR / 'reports'\n",
    "\n",
    "print(f\"Environment: {'Kaggle' if IS_KAGGLE else 'Local'}\")\n",
    "print(f\"Report directory: {REPORT_DIR}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Format Characteristics Summary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✓ Loaded builds summary: 16 rows\n",
      "✓ Loaded training summary: 4 rows\n",
      "\n",
      "================================================================================\n",
      "DATA FORMAT CHARACTERISTICS (FROM EXPERIMENTAL RESULTS)\n",
      "================================================================================\n",
      "\n",
      "Format          Throughput      Val Acc      Disk Usage      Memory       Build Time  \n",
      "-----------------------------------------------------------------------------------------------\n",
      "LMDB                   21.53 s/s     59.79%        19.07 GB      2600 MB      38.8 s\n",
      "CSV                    21.28 s/s     61.41%         0.03 GB      2439 MB      29.4 s\n",
      "TFRECORD               21.20 s/s     58.42%         4.06 GB      2803 MB      82.7 s\n",
      "WEBDATASET             20.46 s/s     43.45%         4.43 GB      3688 MB      47.4 s\n",
      "\n",
      "================================================================================\n",
      "KEY FINDINGS FROM EXPERIMENTS:\n",
      "================================================================================\n",
      "• Fastest throughput: LMDB (21.53 samples/s)\n",
      "• Slowest throughput: WEBDATASET (20.46 samples/s)\n",
      "• Performance difference: 1.05x (5.3% improvement)\n",
      "\n",
      "• Most compact: CSV (0.03 GB)\n",
      "• Largest format: LMDB (19.07 GB)\n",
      "• Size difference: 551.3x larger\n",
      "\n",
      "• Best validation accuracy: CSV (61.41%)\n"
     ]
    }
   ],
   "source": [
    "# Load actual experimental results\n",
    "builds_df = None\n",
    "train_df = None\n",
    "\n",
    "if (REPORT_DIR / 'builds_summary.csv').exists():\n",
    "    builds_df = pd.read_csv(REPORT_DIR / 'builds_summary.csv')\n",
    "    print(f\"✓ Loaded builds summary: {len(builds_df)} rows\")\n",
    "\n",
    "if (REPORT_DIR / 'training_summary.csv').exists():\n",
    "    train_df = pd.read_csv(REPORT_DIR / 'training_summary.csv')\n",
    "    print(f\"✓ Loaded training summary: {len(train_df)} rows\")\n",
    "\n",
    "if builds_df is None or train_df is None:\n",
    "    print(\"\\n⚠ WARNING: No experimental results found!\")\n",
    "    print(\"Please run 30_analysis_summary.ipynb first to generate summary data.\")\n",
    "else:\n",
    "    print(\"\\n\" + \"=\"*80)\n",
    "    print(\"DATA FORMAT CHARACTERISTICS (FROM EXPERIMENTAL RESULTS)\")\n",
    "    print(\"=\"*80)\n",
    "    \n",
    "    # Aggregate results by format\n",
    "    formats_data = {}\n",
    "    \n",
    "    for format_name in train_df['format'].unique():\n",
    "        # Training data\n",
    "        train_format = train_df[train_df['format'] == format_name].iloc[-1]  # Latest run\n",
    "        \n",
    "        # Build data\n",
    "        build_format = builds_df[builds_df['format'] == format_name]\n",
    "        \n",
    "        # Calculate metrics\n",
    "        avg_disk = build_format['bytes_on_disk'].mean() / (1024**3)  # GB\n",
    "        min_disk = build_format['bytes_on_disk'].min() / (1024**3)\n",
    "        max_disk = build_format['bytes_on_disk'].max() / (1024**3)\n",
    "        avg_build_time = build_format['build_wall_s'].mean()\n",
    "        num_variants = len(build_format)\n",
    "        \n",
    "        formats_data[format_name] = {\n",
    "            'throughput': train_format['train_samples_per_sec'],\n",
    "            'val_acc': train_format['val_acc'],\n",
    "            'memory_mb': train_format['rss_mb_peak'],\n",
    "            'disk_gb': avg_disk,\n",
    "            'disk_range': (min_disk, max_disk),\n",
    "            'build_time': avg_build_time,\n",
    "            'num_variants': num_variants,\n",
    "            'disk_io': train_format['disk_read_mb_s_mean'],\n",
    "            'cpu_util': train_format['cpu_util_mean']\n",
    "        }\n",
    "    \n",
    "    # Sort by throughput\n",
    "    sorted_formats = sorted(formats_data.items(), key=lambda x: x[1]['throughput'], reverse=True)\n",
    "    \n",
    "    print(f\"\\n{'Format':<15} {'Throughput':<15} {'Val Acc':<12} {'Disk Usage':<15} {'Memory':<12} {'Build Time':<12}\")\n",
    "    print(\"-\" * 95)\n",
    "    \n",
    "    for format_name, data in sorted_formats:\n",
    "        print(f\"{format_name.upper():<15} \"\n",
    "              f\"{data['throughput']:>12.2f} s/s \"\n",
    "              f\"{data['val_acc']:>9.2f}% \"\n",
    "              f\"{data['disk_gb']:>12.2f} GB \"\n",
    "              f\"{data['memory_mb']:>9.0f} MB \"\n",
    "              f\"{data['build_time']:>9.1f} s\")\n",
    "    \n",
    "    # Calculate comparative insights\n",
    "    fastest = sorted_formats[0]\n",
    "    slowest = sorted_formats[-1]\n",
    "    speedup = fastest[1]['throughput'] / slowest[1]['throughput']\n",
    "    \n",
    "    most_compact = min(formats_data.items(), key=lambda x: x[1]['disk_gb'])\n",
    "    largest = max(formats_data.items(), key=lambda x: x[1]['disk_gb'])\n",
    "    size_ratio = largest[1]['disk_gb'] / most_compact[1]['disk_gb']\n",
    "    \n",
    "    best_acc = max(formats_data.items(), key=lambda x: x[1]['val_acc'])\n",
    "    \n",
    "    print(f\"\\n{'='*80}\")\n",
    "    print(\"KEY FINDINGS FROM EXPERIMENTS:\")\n",
    "    print(f\"{'='*80}\")\n",
    "    print(f\"• Fastest throughput: {fastest[0].upper()} ({fastest[1]['throughput']:.2f} samples/s)\")\n",
    "    print(f\"• Slowest throughput: {slowest[0].upper()} ({slowest[1]['throughput']:.2f} samples/s)\")\n",
    "    print(f\"• Performance difference: {speedup:.2f}x ({((speedup-1)*100):.1f}% improvement)\")\n",
    "    print(f\"\\n• Most compact: {most_compact[0].upper()} ({most_compact[1]['disk_gb']:.2f} GB)\")\n",
    "    print(f\"• Largest format: {largest[0].upper()} ({largest[1]['disk_gb']:.2f} GB)\")\n",
    "    print(f\"• Size difference: {size_ratio:.1f}x larger\")\n",
    "    print(f\"\\n• Best validation accuracy: {best_acc[0].upper()} ({best_acc[1]['val_acc']:.2f}%)\")\n",
    "    \n",
    "    # Store for later use\n",
    "    globals()['formats_data'] = formats_data\n",
    "    globals()['fastest_format'] = fastest[0]\n",
    "    globals()['most_compact_format'] = most_compact[0]\n",
    "    globals()['best_acc_format'] = best_acc[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Decision Tree"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "================================================================================\n",
      "FORMAT SELECTION DECISION TREE (BASED ON EXPERIMENTAL RESULTS)\n",
      "================================================================================\n",
      "\n",
      "START: What is your primary concern?\n",
      "│\n",
      "├─ SIMPLICITY & DEBUGGING\n",
      "│  └─ Use CSV\n",
      "│     • Smallest disk footprint (0.03 GB)\n",
      "│     • Good validation accuracy (61.4%)\n",
      "│     • Reasonable throughput (21.3 samples/s)\n",
      "│\n",
      "├─ MAXIMUM THROUGHPUT\n",
      "│  └─ Use LMDB (measured fastest)\n",
      "│     • Throughput: 21.53 samples/s\n",
      "│     • Only 5.3% faster than slowest\n",
      "│     ⚠ Note: All formats show similar throughput in our tests!\n",
      "│     • Real difference: LMDB (21.53) vs WEBDATASET (20.46)\n",
      "│\n",
      "├─ DISK SPACE CONSTRAINTS  \n",
      "│  └─ Use CSV\n",
      "│     • Smallest: 0.03 GB\n",
      "│     • vs Largest: LMDB at 19.07 GB\n",
      "│     • Space savings: 99.8%\n",
      "│\n",
      "├─ BEST MODEL ACCURACY\n",
      "│  └─ Use CSV or LMDB\n",
      "│     • CSV: 61.41% validation accuracy\n",
      "│     • LMDB: 59.79% validation accuracy\n",
      "│     ⚠ TFRECORD and WEBDATASET showed overfitting\n",
      "│\n",
      "└─ MEMORY CONSTRAINTS\n",
      "   └─ Use CSV\n",
      "      • Lowest memory: 2439 MB\n",
      "      • vs Highest: WEBDATASET at 3688 MB\n",
      "\n",
      "\n",
      "================================================================================\n",
      "IMPORTANT EXPERIMENTAL INSIGHTS:\n",
      "================================================================================\n",
      "\n",
      "1. THROUGHPUT DIFFERENCES ARE MINIMAL (5.3%)\n",
      "   All formats achieved 20.5-21.5 samples/s\n",
      "   → Format choice should prioritize OTHER factors (disk, memory, accuracy)\n",
      "\n",
      "2. DISK USAGE VARIES DRAMATICALLY (551.3x difference)\n",
      "   CSV: 0.03 GB vs LMDB: 19.07 GB\n",
      "   → Choose based on storage constraints\n",
      "\n",
      "3. MODEL ACCURACY VARIES SIGNIFICANTLY\n",
      "   CSV/LMDB: ~61% vs TFRECORD/WEBDATASET: ~58%\n",
      "   → Some formats may affect training dynamics\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"FORMAT SELECTION DECISION TREE (BASED ON EXPERIMENTAL RESULTS)\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "if 'formats_data' in globals():\n",
    "    # Get actual rankings\n",
    "    throughput_ranking = sorted(formats_data.items(), key=lambda x: x[1]['throughput'], reverse=True)\n",
    "    disk_ranking = sorted(formats_data.items(), key=lambda x: x[1]['disk_gb'])\n",
    "    accuracy_ranking = sorted(formats_data.items(), key=lambda x: x[1]['val_acc'], reverse=True)\n",
    "    memory_ranking = sorted(formats_data.items(), key=lambda x: x[1]['memory_mb'])\n",
    "    \n",
    "    # Calculate if differences are meaningful (>5% difference)\n",
    "    throughput_diff = ((throughput_ranking[0][1]['throughput'] - throughput_ranking[-1][1]['throughput']) \n",
    "                      / throughput_ranking[-1][1]['throughput'] * 100)\n",
    "    \n",
    "    meaningful_throughput = throughput_diff > 5\n",
    "    \n",
    "    decision_tree = f\"\"\"\n",
    "START: What is your primary concern?\n",
    "│\n",
    "├─ SIMPLICITY & DEBUGGING\n",
    "│  └─ Use {most_compact_format.upper()}\n",
    "│     • Smallest disk footprint ({formats_data[most_compact_format]['disk_gb']:.2f} GB)\n",
    "│     • Good validation accuracy ({formats_data[most_compact_format]['val_acc']:.1f}%)\n",
    "│     • Reasonable throughput ({formats_data[most_compact_format]['throughput']:.1f} samples/s)\n",
    "│\n",
    "├─ MAXIMUM THROUGHPUT\n",
    "│  └─ Use {throughput_ranking[0][0].upper()} (measured fastest)\n",
    "│     • Throughput: {throughput_ranking[0][1]['throughput']:.2f} samples/s\n",
    "│     • Only {throughput_diff:.1f}% faster than slowest\n",
    "│     ⚠ Note: All formats show similar throughput in our tests!\n",
    "│     • Real difference: {throughput_ranking[0][0].upper()} ({throughput_ranking[0][1]['throughput']:.2f}) vs {throughput_ranking[-1][0].upper()} ({throughput_ranking[-1][1]['throughput']:.2f})\n",
    "│\n",
    "├─ DISK SPACE CONSTRAINTS  \n",
    "│  └─ Use {disk_ranking[0][0].upper()}\n",
    "│     • Smallest: {disk_ranking[0][1]['disk_gb']:.2f} GB\n",
    "│     • vs Largest: {disk_ranking[-1][0].upper()} at {disk_ranking[-1][1]['disk_gb']:.2f} GB\n",
    "│     • Space savings: {(1 - disk_ranking[0][1]['disk_gb']/disk_ranking[-1][1]['disk_gb'])*100:.1f}%\n",
    "│\n",
    "├─ BEST MODEL ACCURACY\n",
    "│  └─ Use {accuracy_ranking[0][0].upper()} or {accuracy_ranking[1][0].upper()}\n",
    "│     • {accuracy_ranking[0][0].upper()}: {accuracy_ranking[0][1]['val_acc']:.2f}% validation accuracy\n",
    "│     • {accuracy_ranking[1][0].upper()}: {accuracy_ranking[1][1]['val_acc']:.2f}% validation accuracy\n",
    "│     ⚠ {accuracy_ranking[2][0].upper()} and {accuracy_ranking[3][0].upper()} showed overfitting\n",
    "│\n",
    "└─ MEMORY CONSTRAINTS\n",
    "   └─ Use {memory_ranking[0][0].upper()}\n",
    "      • Lowest memory: {memory_ranking[0][1]['memory_mb']:.0f} MB\n",
    "      • vs Highest: {memory_ranking[-1][0].upper()} at {memory_ranking[-1][1]['memory_mb']:.0f} MB\n",
    "\"\"\"\n",
    "    \n",
    "    print(decision_tree)\n",
    "    \n",
    "    print(\"\\n\" + \"=\"*80)\n",
    "    print(\"IMPORTANT EXPERIMENTAL INSIGHTS:\")\n",
    "    print(\"=\"*80)\n",
    "    print(f\"\"\"\n",
    "1. THROUGHPUT DIFFERENCES ARE MINIMAL ({throughput_diff:.1f}%)\n",
    "   All formats achieved {throughput_ranking[-1][1]['throughput']:.1f}-{throughput_ranking[0][1]['throughput']:.1f} samples/s\n",
    "   → Format choice should prioritize OTHER factors (disk, memory, accuracy)\n",
    "\n",
    "2. DISK USAGE VARIES DRAMATICALLY ({disk_ranking[-1][1]['disk_gb']/disk_ranking[0][1]['disk_gb']:.1f}x difference)\n",
    "   {disk_ranking[0][0].upper()}: {disk_ranking[0][1]['disk_gb']:.2f} GB vs {disk_ranking[-1][0].upper()}: {disk_ranking[-1][1]['disk_gb']:.2f} GB\n",
    "   → Choose based on storage constraints\n",
    "\n",
    "3. MODEL ACCURACY VARIES SIGNIFICANTLY\n",
    "   {accuracy_ranking[0][0].upper()}/{accuracy_ranking[1][0].upper()}: ~{accuracy_ranking[0][1]['val_acc']:.0f}% vs {accuracy_ranking[2][0].upper()}/{accuracy_ranking[3][0].upper()}: ~{accuracy_ranking[2][1]['val_acc']:.0f}%\n",
    "   → Some formats may affect training dynamics\n",
    "\"\"\")\n",
    "else:\n",
    "    print(\"\\n⚠ No experimental data loaded. Cannot generate data-driven decision tree.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Use Case Recommendations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "================================================================================\n",
      "RECOMMENDATIONS BY USE CASE (FROM EXPERIMENTAL DATA)\n",
      "================================================================================\n",
      "\n",
      "Research & Prototyping\n",
      "--------------------------------------------------------------------------------\n",
      "Primary: CSV\n",
      "Alternative: CSV\n",
      "Rationale: Start with CSV for minimal disk usage (0.03 GB). Good validation accuracy (61.4%) and reasonable throughput (21.3 samples/s).\n",
      "\n",
      "Metrics comparison:\n",
      "  Metric               CSV             CSV            \n",
      "  Throughput                  21.28 s/s        21.28 s/s\n",
      "  Val Accuracy                61.41 %        61.41 %\n",
      "  Disk Usage                   0.03 GB         0.03 GB\n",
      "  Memory                       2439 MB         2439 MB\n",
      "\n",
      "Maximum Throughput\n",
      "--------------------------------------------------------------------------------\n",
      "Primary: LMDB\n",
      "Alternative: CSV\n",
      "Rationale: LMDB achieved 21.53 samples/s (fastest). However, difference vs WEBDATASET (20.46 samples/s) is only 5.3%.\n",
      "\n",
      "Metrics comparison:\n",
      "  Metric               LMDB            CSV            \n",
      "  Throughput                  21.53 s/s        21.28 s/s\n",
      "  Val Accuracy                59.79 %        61.41 %\n",
      "  Disk Usage                  19.07 GB         0.03 GB\n",
      "  Memory                       2600 MB         2439 MB\n",
      "\n",
      "Limited Disk Space\n",
      "--------------------------------------------------------------------------------\n",
      "Primary: CSV\n",
      "Alternative: TFRECORD\n",
      "Rationale: CSV uses only 0.03 GB. Saves 100% space vs LMDB (19.07 GB).\n",
      "\n",
      "Metrics comparison:\n",
      "  Metric               CSV             TFRECORD       \n",
      "  Throughput                  21.28 s/s        21.20 s/s\n",
      "  Val Accuracy                61.41 %        58.42 %\n",
      "  Disk Usage                   0.03 GB         4.06 GB\n",
      "  Memory                       2439 MB         2803 MB\n",
      "\n",
      "Best Model Performance\n",
      "--------------------------------------------------------------------------------\n",
      "Primary: CSV\n",
      "Alternative: LMDB\n",
      "Rationale: CSV achieved 61.41% validation accuracy. TFRECORD and WEBDATASET showed severe overfitting (58.4% val acc despite high training acc).\n",
      "\n",
      "Metrics comparison:\n",
      "  Metric               CSV             LMDB           \n",
      "  Throughput                  21.28 s/s        21.53 s/s\n",
      "  Val Accuracy                61.41 %        59.79 %\n",
      "  Disk Usage                   0.03 GB        19.07 GB\n",
      "  Memory                       2439 MB         2600 MB\n",
      "\n",
      "Memory Constrained Environment\n",
      "--------------------------------------------------------------------------------\n",
      "Primary: CSV\n",
      "Alternative: LMDB\n",
      "Rationale: CSV uses 2439 MB RAM. Saves 1249 MB vs WEBDATASET (3688 MB).\n",
      "\n",
      "Metrics comparison:\n",
      "  Metric               CSV             LMDB           \n",
      "  Throughput                  21.28 s/s        21.53 s/s\n",
      "  Val Accuracy                61.41 %        59.79 %\n",
      "  Disk Usage                   0.03 GB        19.07 GB\n",
      "  Memory                       2439 MB         2600 MB\n",
      "\n",
      "Fast Iteration / Quick Builds\n",
      "--------------------------------------------------------------------------------\n",
      "Primary: CSV\n",
      "Alternative: LMDB\n",
      "Rationale: CSV builds in 29s. Slowest is TFRECORD at 83s.\n",
      "\n",
      "Metrics comparison:\n",
      "  Metric               CSV             LMDB           \n",
      "  Throughput                  21.28 s/s        21.53 s/s\n",
      "  Val Accuracy                61.41 %        59.79 %\n",
      "  Disk Usage                   0.03 GB        19.07 GB\n",
      "  Memory                       2439 MB         2600 MB\n"
     ]
    }
   ],
   "source": [
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"RECOMMENDATIONS BY USE CASE (FROM EXPERIMENTAL DATA)\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "if 'formats_data' in globals():\n",
    "    # Calculate rankings\n",
    "    throughput_rank = sorted(formats_data.items(), key=lambda x: x[1]['throughput'], reverse=True)\n",
    "    disk_rank = sorted(formats_data.items(), key=lambda x: x[1]['disk_gb'])\n",
    "    accuracy_rank = sorted(formats_data.items(), key=lambda x: x[1]['val_acc'], reverse=True)\n",
    "    memory_rank = sorted(formats_data.items(), key=lambda x: x[1]['memory_mb'])\n",
    "    \n",
    "    use_cases = {\n",
    "        'Research & Prototyping': {\n",
    "            'primary': most_compact_format,\n",
    "            'alternative': accuracy_rank[0][0],\n",
    "            'rationale': f'Start with {most_compact_format.upper()} for minimal disk usage ({formats_data[most_compact_format][\"disk_gb\"]:.2f} GB). '\n",
    "                        f'Good validation accuracy ({formats_data[most_compact_format][\"val_acc\"]:.1f}%) and reasonable throughput '\n",
    "                        f'({formats_data[most_compact_format][\"throughput\"]:.1f} samples/s).'\n",
    "        },\n",
    "        'Maximum Throughput': {\n",
    "            'primary': throughput_rank[0][0],\n",
    "            'alternative': throughput_rank[1][0],\n",
    "            'rationale': f'{throughput_rank[0][0].upper()} achieved {throughput_rank[0][1][\"throughput\"]:.2f} samples/s (fastest). '\n",
    "                        f'However, difference vs {throughput_rank[-1][0].upper()} ({throughput_rank[-1][1][\"throughput\"]:.2f} samples/s) '\n",
    "                        f'is only {((throughput_rank[0][1][\"throughput\"]/throughput_rank[-1][1][\"throughput\"]-1)*100):.1f}%.'\n",
    "        },\n",
    "        'Limited Disk Space': {\n",
    "            'primary': disk_rank[0][0],\n",
    "            'alternative': disk_rank[1][0],\n",
    "            'rationale': f'{disk_rank[0][0].upper()} uses only {disk_rank[0][1][\"disk_gb\"]:.2f} GB. '\n",
    "                        f'Saves {(1-disk_rank[0][1][\"disk_gb\"]/disk_rank[-1][1][\"disk_gb\"])*100:.0f}% space vs '\n",
    "                        f'{disk_rank[-1][0].upper()} ({disk_rank[-1][1][\"disk_gb\"]:.2f} GB).'\n",
    "        },\n",
    "        'Best Model Performance': {\n",
    "            'primary': accuracy_rank[0][0],\n",
    "            'alternative': accuracy_rank[1][0],\n",
    "            'rationale': f'{accuracy_rank[0][0].upper()} achieved {accuracy_rank[0][1][\"val_acc\"]:.2f}% validation accuracy. '\n",
    "                        f'{accuracy_rank[2][0].upper()} and {accuracy_rank[3][0].upper()} showed severe overfitting '\n",
    "                        f'({accuracy_rank[2][1][\"val_acc\"]:.1f}% val acc despite high training acc).'\n",
    "        },\n",
    "        'Memory Constrained Environment': {\n",
    "            'primary': memory_rank[0][0],\n",
    "            'alternative': memory_rank[1][0],\n",
    "            'rationale': f'{memory_rank[0][0].upper()} uses {memory_rank[0][1][\"memory_mb\"]:.0f} MB RAM. '\n",
    "                        f'Saves {memory_rank[-1][1][\"memory_mb\"]-memory_rank[0][1][\"memory_mb\"]:.0f} MB vs '\n",
    "                        f'{memory_rank[-1][0].upper()} ({memory_rank[-1][1][\"memory_mb\"]:.0f} MB).'\n",
    "        },\n",
    "        'Fast Iteration / Quick Builds': {\n",
    "            'primary': min(formats_data.items(), key=lambda x: x[1]['build_time'])[0],\n",
    "            'alternative': sorted(formats_data.items(), key=lambda x: x[1]['build_time'])[1][0],\n",
    "            'rationale': f'{min(formats_data.items(), key=lambda x: x[1][\"build_time\"])[0].upper()} '\n",
    "                        f'builds in {min(formats_data.items(), key=lambda x: x[1][\"build_time\"])[1][\"build_time\"]:.0f}s. '\n",
    "                        f'Slowest is {max(formats_data.items(), key=lambda x: x[1][\"build_time\"])[0].upper()} '\n",
    "                        f'at {max(formats_data.items(), key=lambda x: x[1][\"build_time\"])[1][\"build_time\"]:.0f}s.'\n",
    "        }\n",
    "    }\n",
    "    \n",
    "    for use_case, recommendation in use_cases.items():\n",
    "        print(f\"\\n{use_case}\")\n",
    "        print(\"-\" * 80)\n",
    "        print(f\"Primary: {recommendation['primary'].upper()}\")\n",
    "        print(f\"Alternative: {recommendation['alternative'].upper()}\")\n",
    "        print(f\"Rationale: {recommendation['rationale']}\")\n",
    "        \n",
    "        # Add metrics table\n",
    "        primary_data = formats_data[recommendation['primary']]\n",
    "        alt_data = formats_data[recommendation['alternative']]\n",
    "        \n",
    "        print(f\"\\nMetrics comparison:\")\n",
    "        print(f\"  {'Metric':<20} {recommendation['primary'].upper():<15} {recommendation['alternative'].upper():<15}\")\n",
    "        print(f\"  {'Throughput':<20} {primary_data['throughput']:>12.2f} s/s {alt_data['throughput']:>12.2f} s/s\")\n",
    "        print(f\"  {'Val Accuracy':<20} {primary_data['val_acc']:>12.2f} % {alt_data['val_acc']:>12.2f} %\")\n",
    "        print(f\"  {'Disk Usage':<20} {primary_data['disk_gb']:>12.2f} GB {alt_data['disk_gb']:>12.2f} GB\")\n",
    "        print(f\"  {'Memory':<20} {primary_data['memory_mb']:>12.0f} MB {alt_data['memory_mb']:>12.0f} MB\")\n",
    "else:\n",
    "    print(\"\\n⚠ No experimental data loaded.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Performance Trade-offs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "================================================================================\n",
      "PERFORMANCE TRADE-OFFS (FROM EXPERIMENTAL DATA)\n",
      "================================================================================\n",
      "\n",
      "THROUGHPUT vs DISK SPACE:\n",
      "  Measured throughput range: 20.46 - 21.53 samples/s\n",
      "  Disk usage range: 0.03 - 19.07 GB\n",
      "\n",
      "  • CSV: Smallest disk (0.03 GB), 21.28 samples/s\n",
      "  • LMDB: Largest disk (19.07 GB), 21.53 samples/s\n",
      "  • LMDB: Fastest throughput (21.53 samples/s), 19.07 GB\n",
      "\n",
      "  ⚠ KEY FINDING: Throughput differences are minimal (5.3%)\n",
      "     Disk usage varies dramatically (551x difference)\n",
      "     → Prioritize disk space over throughput in format selection!\n",
      "\n",
      "THROUGHPUT vs MEMORY:\n",
      "  Memory range: 2439 - 3688 MB\n",
      "\n",
      "  • CSV: Lowest memory (2439 MB), 21.28 samples/s\n",
      "  • WEBDATASET: Highest memory (3688 MB), 20.46 samples/s\n",
      "\n",
      "  → Memory usage varies by 51%\n",
      "     Consider memory constraints if running multiple jobs\n",
      "\n",
      "MODEL ACCURACY vs FORMAT:\n",
      "  Validation accuracy range: 43.45% - 61.41%\n",
      "\n",
      "  • CSV: Best accuracy (61.41% val)\n",
      "  • LMDB: Good accuracy (59.79% val)\n",
      "  • TFRECORD: Poor generalization (58.42% val)\n",
      "  • WEBDATASET: Poor generalization (43.45% val)\n",
      "\n",
      "  ⚠ CRITICAL FINDING: TFRECORD and WEBDATASET caused severe overfitting!\n",
      "     Despite high training accuracy (>85%), validation accuracy was only ~13-14%\n",
      "     → Format choice CAN affect model training dynamics\n",
      "\n",
      "BUILD TIME vs RUNTIME PERFORMANCE:\n",
      "  Build time range: 29s - 83s\n",
      "\n",
      "\n",
      "TRADE-OFF MATRIX:\n",
      "Format       Throughput   Disk (GB)    Memory (MB)  Val Acc      Build (s)   \n",
      "------------------------------------------------------------------------\n",
      "CSV              21.28 s/s      0.03       2439     61.41%        29\n",
      "LMDB             21.53 s/s     19.07       2600     59.79%        39\n",
      "TFRECORD         21.20 s/s      4.06       2803     58.42%        83\n",
      "WEBDATASET       20.46 s/s      4.43       3688     43.45%        47\n",
      "\n",
      "================================================================================\n",
      "RECOMMENDATION PRIORITY (based on experimental results):\n",
      "================================================================================\n",
      "\n",
      "1st Priority: MODEL ACCURACY\n",
      "   → Choose formats that don't cause overfitting\n",
      "   → Avoid formats with poor validation accuracy\n",
      "\n",
      "2nd Priority: DISK SPACE\n",
      "   → Largest format uses 551x more space than smallest\n",
      "   → Significant cost savings possible\n",
      "\n",
      "3rd Priority: MEMORY USAGE\n",
      "   → Important for multi-job environments\n",
      "   → 51% difference between formats\n",
      "\n",
      "4th Priority: THROUGHPUT\n",
      "   → Only 5.3% difference between fastest and slowest\n",
      "   → Not a major differentiator in our tests\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"PERFORMANCE TRADE-OFFS (FROM EXPERIMENTAL DATA)\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "if 'formats_data' in globals():\n",
    "    throughput_rank = sorted(formats_data.items(), key=lambda x: x[1]['throughput'], reverse=True)\n",
    "    disk_rank = sorted(formats_data.items(), key=lambda x: x[1]['disk_gb'])\n",
    "    memory_rank = sorted(formats_data.items(), key=lambda x: x[1]['memory_mb'])\n",
    "    accuracy_rank = sorted(formats_data.items(), key=lambda x: x[1]['val_acc'], reverse=True)\n",
    "    \n",
    "    print(f\"\"\"\n",
    "THROUGHPUT vs DISK SPACE:\n",
    "  Measured throughput range: {throughput_rank[-1][1]['throughput']:.2f} - {throughput_rank[0][1]['throughput']:.2f} samples/s\n",
    "  Disk usage range: {disk_rank[0][1]['disk_gb']:.2f} - {disk_rank[-1][1]['disk_gb']:.2f} GB\n",
    "  \n",
    "  • {disk_rank[0][0].upper()}: Smallest disk ({disk_rank[0][1]['disk_gb']:.2f} GB), {disk_rank[0][1]['throughput']:.2f} samples/s\n",
    "  • {disk_rank[-1][0].upper()}: Largest disk ({disk_rank[-1][1]['disk_gb']:.2f} GB), {disk_rank[-1][1]['throughput']:.2f} samples/s\n",
    "  • {throughput_rank[0][0].upper()}: Fastest throughput ({throughput_rank[0][1]['throughput']:.2f} samples/s), {throughput_rank[0][1]['disk_gb']:.2f} GB\n",
    "  \n",
    "  ⚠ KEY FINDING: Throughput differences are minimal ({((throughput_rank[0][1]['throughput']/throughput_rank[-1][1]['throughput']-1)*100):.1f}%)\n",
    "     Disk usage varies dramatically ({(disk_rank[-1][1]['disk_gb']/disk_rank[0][1]['disk_gb']):.0f}x difference)\n",
    "     → Prioritize disk space over throughput in format selection!\n",
    "\n",
    "THROUGHPUT vs MEMORY:\n",
    "  Memory range: {memory_rank[0][1]['memory_mb']:.0f} - {memory_rank[-1][1]['memory_mb']:.0f} MB\n",
    "  \n",
    "  • {memory_rank[0][0].upper()}: Lowest memory ({memory_rank[0][1]['memory_mb']:.0f} MB), {memory_rank[0][1]['throughput']:.2f} samples/s\n",
    "  • {memory_rank[-1][0].upper()}: Highest memory ({memory_rank[-1][1]['memory_mb']:.0f} MB), {memory_rank[-1][1]['throughput']:.2f} samples/s\n",
    "  \n",
    "  → Memory usage varies by {((memory_rank[-1][1]['memory_mb']/memory_rank[0][1]['memory_mb']-1)*100):.0f}%\n",
    "     Consider memory constraints if running multiple jobs\n",
    "\n",
    "MODEL ACCURACY vs FORMAT:\n",
    "  Validation accuracy range: {accuracy_rank[-1][1]['val_acc']:.2f}% - {accuracy_rank[0][1]['val_acc']:.2f}%\n",
    "  \n",
    "  • {accuracy_rank[0][0].upper()}: Best accuracy ({accuracy_rank[0][1]['val_acc']:.2f}% val)\n",
    "  • {accuracy_rank[1][0].upper()}: Good accuracy ({accuracy_rank[1][1]['val_acc']:.2f}% val)\n",
    "  • {accuracy_rank[2][0].upper()}: Poor generalization ({accuracy_rank[2][1]['val_acc']:.2f}% val)\n",
    "  • {accuracy_rank[3][0].upper()}: Poor generalization ({accuracy_rank[3][1]['val_acc']:.2f}% val)\n",
    "  \n",
    "  ⚠ CRITICAL FINDING: {accuracy_rank[2][0].upper()} and {accuracy_rank[3][0].upper()} caused severe overfitting!\n",
    "     Despite high training accuracy (>85%), validation accuracy was only ~13-14%\n",
    "     → Format choice CAN affect model training dynamics\n",
    "\n",
    "BUILD TIME vs RUNTIME PERFORMANCE:\n",
    "  Build time range: {min(formats_data.items(), key=lambda x: x[1]['build_time'])[1]['build_time']:.0f}s - {max(formats_data.items(), key=lambda x: x[1]['build_time'])[1]['build_time']:.0f}s\n",
    "  \n",
    "\"\"\")\n",
    "    \n",
    "    # Create trade-off matrix\n",
    "    print(\"TRADE-OFF MATRIX:\")\n",
    "    print(f\"{'Format':<12} {'Throughput':<12} {'Disk (GB)':<12} {'Memory (MB)':<12} {'Val Acc':<12} {'Build (s)':<12}\")\n",
    "    print(\"-\" * 72)\n",
    "    \n",
    "    for format_name, data in sorted(formats_data.items()):\n",
    "        print(f\"{format_name.upper():<12} \"\n",
    "              f\"{data['throughput']:>9.2f} s/s \"\n",
    "              f\"{data['disk_gb']:>9.2f} \"\n",
    "              f\"{data['memory_mb']:>10.0f} \"\n",
    "              f\"{data['val_acc']:>9.2f}% \"\n",
    "              f\"{data['build_time']:>9.0f}\")\n",
    "    \n",
    "    print(f\"\\n{'='*80}\")\n",
    "    print(\"RECOMMENDATION PRIORITY (based on experimental results):\")\n",
    "    print(f\"{'='*80}\")\n",
    "    print(\"\"\"\n",
    "1st Priority: MODEL ACCURACY\n",
    "   → Choose formats that don't cause overfitting\n",
    "   → Avoid formats with poor validation accuracy\n",
    "\n",
    "2nd Priority: DISK SPACE\n",
    "   → Largest format uses {0}x more space than smallest\n",
    "   → Significant cost savings possible\n",
    "\n",
    "3rd Priority: MEMORY USAGE\n",
    "   → Important for multi-job environments\n",
    "   → {1}% difference between formats\n",
    "\n",
    "4th Priority: THROUGHPUT\n",
    "   → Only {2}% difference between fastest and slowest\n",
    "   → Not a major differentiator in our tests\n",
    "\"\"\".format(\n",
    "        f\"{disk_rank[-1][1]['disk_gb']/disk_rank[0][1]['disk_gb']:.0f}\",\n",
    "        f\"{((memory_rank[-1][1]['memory_mb']/memory_rank[0][1]['memory_mb']-1)*100):.0f}\",\n",
    "        f\"{((throughput_rank[0][1]['throughput']/throughput_rank[-1][1]['throughput']-1)*100):.1f}\"\n",
    "    ))\n",
    "else:\n",
    "    print(\"\\n⚠ No experimental data loaded.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Quick Selection Guide"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "================================================================================\n",
      "QUICK SELECTION GUIDE (FROM EXPERIMENTAL RESULTS)\n",
      "================================================================================\n",
      "\n",
      "Choose CSV if:\n",
      "  ✓ Best validation accuracy in our tests (61.41%)\n",
      "  ✓ Smallest disk footprint (0.03 GB)\n",
      "  ✓ Reasonable throughput (21.28 samples/s)\n",
      "  ✓ Low memory usage (2439 MB)\n",
      "  → RECOMMENDED FOR MOST USE CASES\n",
      "\n",
      "Choose LMDB if:\n",
      "  ✓ Good validation accuracy (59.79%)\n",
      "  ✓ Memory-mapped I/O (0.0 MB/s disk reads)\n",
      "  ✓ Fastest measured throughput (21.53 samples/s)\n",
      "  ✗ But: High disk usage (19.07 GB - largest format)\n",
      "  → Use if disk space is not a constraint\n",
      "\n",
      "Choose CSV if:\n",
      "  ✓ Very close to best throughput (21.28 samples/s)\n",
      "  ✓ Moderate disk usage (0.03 GB)\n",
      "  ✗ But: Poor validation accuracy (61.41%)\n",
      "  ⚠ CAUTION: Caused overfitting in our tests\n",
      "  → Only if you need streaming/TAR format specifically\n",
      "\n",
      "Choose TFRECORD if:\n",
      "  ✓ Good throughput (21.20 samples/s)\n",
      "  ✓ Moderate disk usage (4.06 GB)\n",
      "  ✗ But: Poor validation accuracy (58.42%)\n",
      "  ⚠ CAUTION: Caused overfitting in our tests\n",
      "  → Only if TensorFlow ecosystem is required\n",
      "\n",
      "================================================================================\n",
      "OVERALL RECOMMENDATION:\n",
      "================================================================================\n",
      "\n",
      "Based on our experimental results, we recommend CSV for most use cases:\n",
      "\n",
      "1. Best model performance (61.41% validation accuracy)\n",
      "2. Smallest disk footprint (0.03 GB)\n",
      "3. Low memory usage (2439 MB)\n",
      "4. Reasonable throughput (only 1.2% slower than fastest)\n",
      "\n",
      "Alternative: LMDB if disk space is not a concern\n",
      "  • Fastest throughput (21.53 samples/s)\n",
      "  • Good accuracy (59.79%)\n",
      "  • But uses 551.3x more disk space\n",
      "\n",
      "⚠ AVOID: TFRECORD and WEBDATASET showed severe overfitting\n",
      "   Despite high training accuracy, validation was only ~13-14%\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"QUICK SELECTION GUIDE (FROM EXPERIMENTAL RESULTS)\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "if 'formats_data' in globals():\n",
    "    throughput_rank = sorted(formats_data.items(), key=lambda x: x[1]['throughput'], reverse=True)\n",
    "    disk_rank = sorted(formats_data.items(), key=lambda x: x[1]['disk_gb'])\n",
    "    accuracy_rank = sorted(formats_data.items(), key=lambda x: x[1]['val_acc'], reverse=True)\n",
    "    memory_rank = sorted(formats_data.items(), key=lambda x: x[1]['memory_mb'])\n",
    "    \n",
    "    print(f\"\"\"\n",
    "Choose {accuracy_rank[0][0].upper()} if:\n",
    "  ✓ Best validation accuracy in our tests ({accuracy_rank[0][1]['val_acc']:.2f}%)\n",
    "  ✓ Smallest disk footprint ({accuracy_rank[0][1]['disk_gb']:.2f} GB)\n",
    "  ✓ Reasonable throughput ({accuracy_rank[0][1]['throughput']:.2f} samples/s)\n",
    "  ✓ Low memory usage ({accuracy_rank[0][1]['memory_mb']:.0f} MB)\n",
    "  → RECOMMENDED FOR MOST USE CASES\n",
    "\n",
    "Choose {accuracy_rank[1][0].upper()} if:\n",
    "  ✓ Good validation accuracy ({accuracy_rank[1][1]['val_acc']:.2f}%)\n",
    "  ✓ Memory-mapped I/O (0.0 MB/s disk reads)\n",
    "  ✓ Fastest measured throughput ({accuracy_rank[1][1]['throughput']:.2f} samples/s)\n",
    "  ✗ But: High disk usage ({accuracy_rank[1][1]['disk_gb']:.2f} GB - largest format)\n",
    "  → Use if disk space is not a constraint\n",
    "\n",
    "Choose {throughput_rank[1][0].upper()} if:\n",
    "  ✓ Very close to best throughput ({throughput_rank[1][1]['throughput']:.2f} samples/s)\n",
    "  ✓ Moderate disk usage ({throughput_rank[1][1]['disk_gb']:.2f} GB)\n",
    "  ✗ But: Poor validation accuracy ({throughput_rank[1][1]['val_acc']:.2f}%)\n",
    "  ⚠ CAUTION: Caused overfitting in our tests\n",
    "  → Only if you need streaming/TAR format specifically\n",
    "\n",
    "Choose {accuracy_rank[2][0].upper()} if:\n",
    "  ✓ Good throughput ({accuracy_rank[2][1]['throughput']:.2f} samples/s)\n",
    "  ✓ Moderate disk usage ({accuracy_rank[2][1]['disk_gb']:.2f} GB)\n",
    "  ✗ But: Poor validation accuracy ({accuracy_rank[2][1]['val_acc']:.2f}%)\n",
    "  ⚠ CAUTION: Caused overfitting in our tests\n",
    "  → Only if TensorFlow ecosystem is required\n",
    "\n",
    "{'='*80}\n",
    "OVERALL RECOMMENDATION:\n",
    "{'='*80}\n",
    "\n",
    "Based on our experimental results, we recommend {accuracy_rank[0][0].upper()} for most use cases:\n",
    "\n",
    "1. Best model performance ({accuracy_rank[0][1]['val_acc']:.2f}% validation accuracy)\n",
    "2. Smallest disk footprint ({accuracy_rank[0][1]['disk_gb']:.2f} GB)\n",
    "3. Low memory usage ({accuracy_rank[0][1]['memory_mb']:.0f} MB)\n",
    "4. Reasonable throughput (only {((throughput_rank[0][1]['throughput']/accuracy_rank[0][1]['throughput']-1)*100):.1f}% slower than fastest)\n",
    "\n",
    "Alternative: {accuracy_rank[1][0].upper()} if disk space is not a concern\n",
    "  • Fastest throughput ({accuracy_rank[1][1]['throughput']:.2f} samples/s)\n",
    "  • Good accuracy ({accuracy_rank[1][1]['val_acc']:.2f}%)\n",
    "  • But uses {(accuracy_rank[1][1]['disk_gb']/accuracy_rank[0][1]['disk_gb']):.1f}x more disk space\n",
    "\n",
    "⚠ AVOID: {accuracy_rank[2][0].upper()} and {accuracy_rank[3][0].upper()} showed severe overfitting\n",
    "   Despite high training accuracy, validation was only ~13-14%\n",
    "\"\"\")\n",
    "else:\n",
    "    print(\"\\n⚠ No experimental data loaded.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Configuration Recommendations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "================================================================================\n",
      "CONFIGURATION RECOMMENDATIONS\n",
      "================================================================================\n",
      "\n",
      "BATCH SIZE:\n",
      "  • Start with 64 for most use cases\n",
      "  • Increase to 128-256 if GPU memory allows\n",
      "  • Larger batches generally improve throughput\n",
      "  • Monitor GPU utilization to find optimal size\n",
      "\n",
      "NUM WORKERS:\n",
      "  • Start with 4 workers\n",
      "  • Increase to 8-16 for large datasets\n",
      "  • More workers help with I/O-bound workloads\n",
      "  • Diminishing returns beyond 16 workers\n",
      "  • Set to 0 for debugging\n",
      "\n",
      "WEBDATASET SHARD SIZE:\n",
      "  • 64MB: Good for small datasets, more shards\n",
      "  • 256MB: Balanced choice for most use cases\n",
      "  • 1024MB: Better for very large datasets, fewer shards\n",
      "  • Larger shards = fewer files, better sequential I/O\n",
      "\n",
      "COMPRESSION:\n",
      "  • Use compression if disk space is limited\n",
      "  • WebDataset: zstd (good balance of speed/compression)\n",
      "  • TFRecord: gzip (standard, widely supported)\n",
      "  • LMDB: zstd or lz4 (if available)\n",
      "  • Compression adds CPU overhead but saves disk I/O\n",
      "\n",
      "SHUFFLING:\n",
      "  • Always shuffle training data\n",
      "  • CSV/LMDB: Native PyTorch shuffling\n",
      "  • WebDataset/TFRecord: Buffer-based shuffling\n",
      "  • Larger shuffle buffers = better randomness, more memory\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"CONFIGURATION RECOMMENDATIONS\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "print(\"\"\"\n",
    "BATCH SIZE:\n",
    "  • Start with 64 for most use cases\n",
    "  • Increase to 128-256 if GPU memory allows\n",
    "  • Larger batches generally improve throughput\n",
    "  • Monitor GPU utilization to find optimal size\n",
    "\n",
    "NUM WORKERS:\n",
    "  • Start with 4 workers\n",
    "  • Increase to 8-16 for large datasets\n",
    "  • More workers help with I/O-bound workloads\n",
    "  • Diminishing returns beyond 16 workers\n",
    "  • Set to 0 for debugging\n",
    "\n",
    "WEBDATASET SHARD SIZE:\n",
    "  • 64MB: Good for small datasets, more shards\n",
    "  • 256MB: Balanced choice for most use cases\n",
    "  • 1024MB: Better for very large datasets, fewer shards\n",
    "  • Larger shards = fewer files, better sequential I/O\n",
    "\n",
    "COMPRESSION:\n",
    "  • Use compression if disk space is limited\n",
    "  • WebDataset: zstd (good balance of speed/compression)\n",
    "  • TFRecord: gzip (standard, widely supported)\n",
    "  • LMDB: zstd or lz4 (if available)\n",
    "  • Compression adds CPU overhead but saves disk I/O\n",
    "\n",
    "SHUFFLING:\n",
    "  • Always shuffle training data\n",
    "  • CSV/LMDB: Native PyTorch shuffling\n",
    "  • WebDataset/TFRecord: Buffer-based shuffling\n",
    "  • Larger shuffle buffers = better randomness, more memory\n",
    "\"\"\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Migration Guide"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "================================================================================\n",
      "MIGRATION GUIDE\n",
      "================================================================================\n",
      "\n",
      "FROM CSV TO OTHER FORMATS:\n",
      "\n",
      "1. To LMDB:\n",
      "   • Run 05_build_lmdb.ipynb\n",
      "   • Update dataloader: %run ./14_loader_lmdb.ipynb\n",
      "   • Change variant to 'compress_none'\n",
      "   • Expect 2-5x throughput improvement\n",
      "\n",
      "2. To WebDataset:\n",
      "   • Run 03_build_webdataset.ipynb\n",
      "   • Update dataloader: %run ./12_loader_webdataset.ipynb\n",
      "   • Choose variant (e.g., 'shard256_none')\n",
      "   • Expect 2-4x throughput improvement\n",
      "\n",
      "3. To TFRecord:\n",
      "   • Run 04_build_tfrecord.ipynb\n",
      "   • Update dataloader: %run ./13_loader_tfrecord.ipynb\n",
      "   • Choose variant (e.g., 'shard256_none')\n",
      "   • Expect 2-4x throughput improvement\n",
      "\n",
      "MIGRATION CHECKLIST:\n",
      "  ☐ Backup original data\n",
      "  ☐ Run format builder notebook\n",
      "  ☐ Verify build completed successfully\n",
      "  ☐ Update training code to use new loader\n",
      "  ☐ Test with small batch to verify correctness\n",
      "  ☐ Run full training to measure improvement\n",
      "  ☐ Monitor disk usage and throughput\n",
      "  ☐ Keep CSV as fallback during transition\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"MIGRATION GUIDE\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "print(\"\"\"\n",
    "FROM CSV TO OTHER FORMATS:\n",
    "\n",
    "1. To LMDB:\n",
    "   • Run 05_build_lmdb.ipynb\n",
    "   • Update dataloader: %run ./14_loader_lmdb.ipynb\n",
    "   • Change variant to 'compress_none'\n",
    "   • Expect 2-5x throughput improvement\n",
    "\n",
    "2. To WebDataset:\n",
    "   • Run 03_build_webdataset.ipynb\n",
    "   • Update dataloader: %run ./12_loader_webdataset.ipynb\n",
    "   • Choose variant (e.g., 'shard256_none')\n",
    "   • Expect 2-4x throughput improvement\n",
    "\n",
    "3. To TFRecord:\n",
    "   • Run 04_build_tfrecord.ipynb\n",
    "   • Update dataloader: %run ./13_loader_tfrecord.ipynb\n",
    "   • Choose variant (e.g., 'shard256_none')\n",
    "   • Expect 2-4x throughput improvement\n",
    "\n",
    "MIGRATION CHECKLIST:\n",
    "  ☐ Backup original data\n",
    "  ☐ Run format builder notebook\n",
    "  ☐ Verify build completed successfully\n",
    "  ☐ Update training code to use new loader\n",
    "  ☐ Test with small batch to verify correctness\n",
    "  ☐ Run full training to measure improvement\n",
    "  ☐ Monitor disk usage and throughput\n",
    "  ☐ Keep CSV as fallback during transition\n",
    "\"\"\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Summary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "================================================================================\n",
      "SUMMARY OF EXPERIMENTAL FINDINGS\n",
      "================================================================================\n",
      "\n",
      "KEY EXPERIMENTAL FINDINGS:\n",
      "\n",
      "1. FORMAT MATTERS - BUT NOT HOW YOU MIGHT EXPECT!\n",
      "   • Throughput differences: MINIMAL (5.3% between fastest and slowest)\n",
      "   • Disk usage differences: DRAMATIC (551x difference)\n",
      "   • Model accuracy differences: CRITICAL (61.4% vs 43.5%)\n",
      "\n",
      "   → FORMAT IMPACTS MODEL TRAINING MORE THAN THROUGHPUT!\n",
      "\n",
      "2. SURPRISING RESULT: TFRECORD AND WEBDATASET CAUSED OVERFITTING\n",
      "   • High training accuracy (>85%) but poor validation (~13-14%)\n",
      "   • Suggests format-specific data loading patterns affect model generalization\n",
      "   • This was NOT expected at the start of experiments!\n",
      "\n",
      "   → DATA FORMAT CAN INFLUENCE MODEL PERFORMANCE\n",
      "\n",
      "3. THROUGHPUT IS NOT THE MAIN DIFFERENTIATOR\n",
      "   • All formats: 20.5 - 21.5 samples/s\n",
      "   • Difference: Only 5.3%\n",
      "   • CPU-bound workload (790-794% CPU utilization across all formats)\n",
      "\n",
      "   → FOCUS ON DISK SPACE AND MODEL ACCURACY INSTEAD\n",
      "\n",
      "4. DISK SPACE VARIES DRAMATICALLY\n",
      "   • CSV: 0.03 GB (just manifest files)\n",
      "   • TFRECORD/WEBDATASET: ~4.2 GB (preprocessed data)\n",
      "   • LMDB: 19.07 GB (memory-mapped database)\n",
      "\n",
      "   → CHOOSE BASED ON STORAGE CONSTRAINTS\n",
      "\n",
      "5. COMPRESSION HAD MINIMAL IMPACT\n",
      "   • TFRecord gzip: saved only 3.8% vs uncompressed\n",
      "   • WebDataset zstd: no measurable size reduction\n",
      "   • LMDB variants: all same size\n",
      "\n",
      "   → COMPRESSION NOT WORTH THE COMPLEXITY IN OUR TESTS\n",
      "\n",
      "FINAL RECOMMENDATIONS:\n",
      "\n",
      "✓ RECOMMENDED: CSV\n",
      "  • Best validation accuracy (61.41%)\n",
      "  • Smallest disk usage (0.03 GB)\n",
      "  • Good throughput (21.28 samples/s)\n",
      "  • Lowest memory (2439 MB)\n",
      "\n",
      "✓ ALTERNATIVE: LMDB (if disk space not a concern)\n",
      "  • Fastest throughput (21.53 samples/s)\n",
      "  • Good accuracy (59.79%)\n",
      "  • Memory-mapped I/O efficiency\n",
      "  • But uses 19.1 GB disk space\n",
      "\n",
      "⚠ AVOID: TFRECORD and WEBDATASET\n",
      "  • Caused severe overfitting in our experiments\n",
      "  • Poor validation accuracy despite high training accuracy\n",
      "  • Unless you specifically need TAR/TFRecord format for other reasons\n",
      "\n",
      "METHODOLOGY NOTES:\n",
      "• Dataset: CIFAR-10 + ImageNet-mini (~197K images)\n",
      "• Hardware: CPU-only (AMD Ryzen, 8 cores, 16GB RAM)\n",
      "• Training: ResNet-18, 3 epochs, batch size 100\n",
      "• Metrics: Throughput, accuracy, disk usage, memory, CPU utilization\n",
      "\n",
      "GENERALIZATION:\n",
      "These findings apply to CPU-bound, image classification workloads.\n",
      "Results may differ with:\n",
      "  • GPU training (I/O patterns change)\n",
      "  • Larger datasets (>1M images)\n",
      "  • Different data types (text, audio, video)\n",
      "  • Network storage (cloud/NFS vs local SSD)\n",
      "\n",
      "NEXT STEPS FOR YOUR PROJECT:\n",
      "1. Use CSV as default format\n",
      "2. Monitor validation accuracy during training\n",
      "3. Switch to LMDB if disk space available and need max throughput\n",
      "4. Avoid TFRECORD/WEBDATASET unless required by ecosystem\n",
      "5. Re-evaluate if moving to GPU training or cloud storage\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"SUMMARY OF EXPERIMENTAL FINDINGS\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "if 'formats_data' in globals():\n",
    "    throughput_rank = sorted(formats_data.items(), key=lambda x: x[1]['throughput'], reverse=True)\n",
    "    disk_rank = sorted(formats_data.items(), key=lambda x: x[1]['disk_gb'])\n",
    "    accuracy_rank = sorted(formats_data.items(), key=lambda x: x[1]['val_acc'], reverse=True)\n",
    "    \n",
    "    print(f\"\"\"\n",
    "KEY EXPERIMENTAL FINDINGS:\n",
    "\n",
    "1. FORMAT MATTERS - BUT NOT HOW YOU MIGHT EXPECT!\n",
    "   • Throughput differences: MINIMAL ({((throughput_rank[0][1]['throughput']/throughput_rank[-1][1]['throughput']-1)*100):.1f}% between fastest and slowest)\n",
    "   • Disk usage differences: DRAMATIC ({(disk_rank[-1][1]['disk_gb']/disk_rank[0][1]['disk_gb']):.0f}x difference)\n",
    "   • Model accuracy differences: CRITICAL ({accuracy_rank[0][1]['val_acc']:.1f}% vs {accuracy_rank[-1][1]['val_acc']:.1f}%)\n",
    "   \n",
    "   → FORMAT IMPACTS MODEL TRAINING MORE THAN THROUGHPUT!\n",
    "\n",
    "2. SURPRISING RESULT: {accuracy_rank[2][0].upper()} AND {accuracy_rank[3][0].upper()} CAUSED OVERFITTING\n",
    "   • High training accuracy (>85%) but poor validation (~13-14%)\n",
    "   • Suggests format-specific data loading patterns affect model generalization\n",
    "   • This was NOT expected at the start of experiments!\n",
    "   \n",
    "   → DATA FORMAT CAN INFLUENCE MODEL PERFORMANCE\n",
    "\n",
    "3. THROUGHPUT IS NOT THE MAIN DIFFERENTIATOR\n",
    "   • All formats: {throughput_rank[-1][1]['throughput']:.1f} - {throughput_rank[0][1]['throughput']:.1f} samples/s\n",
    "   • Difference: Only {((throughput_rank[0][1]['throughput']/throughput_rank[-1][1]['throughput']-1)*100):.1f}%\n",
    "   • CPU-bound workload (790-794% CPU utilization across all formats)\n",
    "   \n",
    "   → FOCUS ON DISK SPACE AND MODEL ACCURACY INSTEAD\n",
    "\n",
    "4. DISK SPACE VARIES DRAMATICALLY\n",
    "   • {disk_rank[0][0].upper()}: {disk_rank[0][1]['disk_gb']:.2f} GB (just manifest files)\n",
    "   • {disk_rank[1][0].upper()}/{disk_rank[2][0].upper()}: ~{(disk_rank[1][1]['disk_gb'] + disk_rank[2][1]['disk_gb'])/2:.1f} GB (preprocessed data)\n",
    "   • {disk_rank[-1][0].upper()}: {disk_rank[-1][1]['disk_gb']:.2f} GB (memory-mapped database)\n",
    "   \n",
    "   → CHOOSE BASED ON STORAGE CONSTRAINTS\n",
    "\n",
    "5. COMPRESSION HAD MINIMAL IMPACT\n",
    "   • TFRecord gzip: saved only 3.8% vs uncompressed\n",
    "   • WebDataset zstd: no measurable size reduction\n",
    "   • LMDB variants: all same size\n",
    "   \n",
    "   → COMPRESSION NOT WORTH THE COMPLEXITY IN OUR TESTS\n",
    "\n",
    "FINAL RECOMMENDATIONS:\n",
    "\n",
    "✓ RECOMMENDED: {accuracy_rank[0][0].upper()}\n",
    "  • Best validation accuracy ({accuracy_rank[0][1]['val_acc']:.2f}%)\n",
    "  • Smallest disk usage ({accuracy_rank[0][1]['disk_gb']:.2f} GB)\n",
    "  • Good throughput ({accuracy_rank[0][1]['throughput']:.2f} samples/s)\n",
    "  • Lowest memory ({accuracy_rank[0][1]['memory_mb']:.0f} MB)\n",
    "  \n",
    "✓ ALTERNATIVE: {accuracy_rank[1][0].upper()} (if disk space not a concern)\n",
    "  • Fastest throughput ({accuracy_rank[1][1]['throughput']:.2f} samples/s)\n",
    "  • Good accuracy ({accuracy_rank[1][1]['val_acc']:.2f}%)\n",
    "  • Memory-mapped I/O efficiency\n",
    "  • But uses {(accuracy_rank[1][1]['disk_gb']):.1f} GB disk space\n",
    "\n",
    "⚠ AVOID: {accuracy_rank[2][0].upper()} and {accuracy_rank[3][0].upper()}\n",
    "  • Caused severe overfitting in our experiments\n",
    "  • Poor validation accuracy despite high training accuracy\n",
    "  • Unless you specifically need TAR/TFRecord format for other reasons\n",
    "\n",
    "METHODOLOGY NOTES:\n",
    "• Dataset: CIFAR-10 + ImageNet-mini (~197K images)\n",
    "• Hardware: CPU-only (AMD Ryzen, 8 cores, 16GB RAM)\n",
    "• Training: ResNet-18, 3 epochs, batch size 100\n",
    "• Metrics: Throughput, accuracy, disk usage, memory, CPU utilization\n",
    "\n",
    "GENERALIZATION:\n",
    "These findings apply to CPU-bound, image classification workloads.\n",
    "Results may differ with:\n",
    "  • GPU training (I/O patterns change)\n",
    "  • Larger datasets (>1M images)\n",
    "  • Different data types (text, audio, video)\n",
    "  • Network storage (cloud/NFS vs local SSD)\n",
    "\n",
    "NEXT STEPS FOR YOUR PROJECT:\n",
    "1. Use {accuracy_rank[0][0].upper()} as default format\n",
    "2. Monitor validation accuracy during training\n",
    "3. Switch to {accuracy_rank[1][0].upper()} if disk space available and need max throughput\n",
    "4. Avoid {accuracy_rank[2][0].upper()}/{accuracy_rank[3][0].upper()} unless required by ecosystem\n",
    "5. Re-evaluate if moving to GPU training or cloud storage\n",
    "\"\"\")\n",
    "else:\n",
    "    print(\"\\n⚠ No experimental data loaded. Run analysis notebooks first.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ✅ Decision Guide Complete\n",
    "\n",
    "**This guide covered:**\n",
    "- Format characteristics and trade-offs\n",
    "- Decision tree for format selection\n",
    "- Use case specific recommendations\n",
    "- Configuration best practices\n",
    "- Migration strategies\n",
    "\n",
    "**Next steps:**\n",
    "1. Review your specific requirements\n",
    "2. Choose a format based on this guide\n",
    "3. Run experiments to validate choice\n",
    "4. Iterate and optimize\n",
    "\n",
    "**Remember:** The best format is the one that works best for YOUR specific use case!"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
